[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Makine Öğrenmesinde Sınıflandırma Yöntemleri ve R Uygulamaları, Ersoy Öz & Selçuk Alp, Nobel Akademik Yayıncılık, 2020.\nVeri Madenciliği ve Makine Öğrenmesi İle Farklı Alanlarda Uygulamalar, Şenol Çelik, Nilay Köleoğlu, & Fatih Çemrek, Holistence Publications, 2022\nSağlık Bilimlerinde Yapay Zeka, Ahmet Rıza Şahin, Kamil Doğan, & Süleyman Sivri, Akademisyen Kitabevi A.Ş., 2020\nYapay Zeka, Atınç Yılmaz, Kodlab Yayın Dağıtım Yazılım Ltd. Şti., 2022\nMachine Learning Algorithms, Giuseppe Bonaccorso, Packt Publishing, 2017.\nOptimizasyon Modelleme ve Yapay Zeka Algoritmaları, Bayram Köse, Bahar Demirtürk, Efe Akademi Yayınları, 2023\n\n\n\n\nGrup2: 4 Temmuz Perşembe 8:30 - Grup1: 4 Temmuz Perşembe 18:00\nİçerik: İlk derste, ChatGPT ve benzeri büyük dil modelleri tanıtıldı. OpenAI Playground üzerinde “cümle tamamlama” kavramı ve “temparature” ayarı gösterildi. Daha iyi çıktı almak için Prompt ayarları anlatıldı. Dökümandan sorgu/chat yapmak için Gemini ve Claude örnekleri verildi. Yine Claude üzerinden kod yazdırma örneği işlendi. İngilizce Sunum linki, Türkçe Sunum linki\n\n\n\n\nGrup2: 5 Temmuz Cuma 8:30 - Grup1: 5 Temmuz Cuma 14:00\nİçerik: Veri türleri ve verilerin işlenmesine dair bilgi verildi. Sunum linki\n\n\n\nGrup2: 11 Temmuz Perşembe 8:30 - Grup1: 11 Temmuz Perşembe 18:00\nİçerik: Confusion matrix ve sınıflandırma/regresyon değerlendirmesi Sunum linki\n\n\n\nGrup2: 12 Temmuz Cuma 8:30 - Grup1: 12 Temmuz Cuma 14:00\nİçerik: Yapay sinir ağları Sunum linki. Derste kullanılan yapay sinir ağı demo sayfaları için link1 ve link2\n\n\n\nGrup2: 19 Temmuz Cuma 8:30 - Grup1: 19 Temmuz Cuma 14:00\nİçerik: Sınıflandırma metodları İngilizce Sunum linki, Türkçe Sunum Linki\n\n\n\nGrup2: 25 Temmuz Perşembe 8:30 - Grup1: 25 Temmuz Perşembe 18:00\nİçerik: Regresyon İngilizce Sunum linki, Türkçe Sunum linki\n\n\n\nGrup2: 26 Temmuz Cuma 8:30 - Grup1: 26 Temmuz Cuma 14:00\nİçerik: Kümeleme İngilizce Sunum linki, Türkçe Sunum Linki\n\n\n\nGrup2: 01 Ağustos Perşembe 8:30 - Grup1: 01 Ağustos Perşembe 18:00\nİçerik: Doğal Dil İşleme İngilizce Sunum linki, Türkçe Sunum linki"
  },
  {
    "objectID": "index.html#kaynaklar",
    "href": "index.html#kaynaklar",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Makine Öğrenmesinde Sınıflandırma Yöntemleri ve R Uygulamaları, Ersoy Öz & Selçuk Alp, Nobel Akademik Yayıncılık, 2020.\nVeri Madenciliği ve Makine Öğrenmesi İle Farklı Alanlarda Uygulamalar, Şenol Çelik, Nilay Köleoğlu, & Fatih Çemrek, Holistence Publications, 2022\nSağlık Bilimlerinde Yapay Zeka, Ahmet Rıza Şahin, Kamil Doğan, & Süleyman Sivri, Akademisyen Kitabevi A.Ş., 2020\nYapay Zeka, Atınç Yılmaz, Kodlab Yayın Dağıtım Yazılım Ltd. Şti., 2022\nMachine Learning Algorithms, Giuseppe Bonaccorso, Packt Publishing, 2017.\nOptimizasyon Modelleme ve Yapay Zeka Algoritmaları, Bayram Köse, Bahar Demirtürk, Efe Akademi Yayınları, 2023"
  },
  {
    "objectID": "index.html#ders1",
    "href": "index.html#ders1",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 4 Temmuz Perşembe 8:30 - Grup1: 4 Temmuz Perşembe 18:00\nİçerik: İlk derste, ChatGPT ve benzeri büyük dil modelleri tanıtıldı. OpenAI Playground üzerinde “cümle tamamlama” kavramı ve “temparature” ayarı gösterildi. Daha iyi çıktı almak için Prompt ayarları anlatıldı. Dökümandan sorgu/chat yapmak için Gemini ve Claude örnekleri verildi. Yine Claude üzerinden kod yazdırma örneği işlendi. İngilizce Sunum linki, Türkçe Sunum linki"
  },
  {
    "objectID": "index.html#ders2",
    "href": "index.html#ders2",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 5 Temmuz Cuma 8:30 - Grup1: 5 Temmuz Cuma 14:00\nİçerik: Veri türleri ve verilerin işlenmesine dair bilgi verildi. Sunum linki"
  },
  {
    "objectID": "index.html#ders3",
    "href": "index.html#ders3",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 11 Temmuz Perşembe 8:30 - Grup1: 11 Temmuz Perşembe 18:00\nİçerik: Confusion matrix ve sınıflandırma/regresyon değerlendirmesi Sunum linki"
  },
  {
    "objectID": "index.html#ders4",
    "href": "index.html#ders4",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 12 Temmuz Cuma 8:30 - Grup1: 12 Temmuz Cuma 14:00\nİçerik: Yapay sinir ağları Sunum linki. Derste kullanılan yapay sinir ağı demo sayfaları için link1 ve link2"
  },
  {
    "objectID": "index.html#ders5",
    "href": "index.html#ders5",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 19 Temmuz Cuma 8:30 - Grup1: 19 Temmuz Cuma 14:00\nİçerik: Sınıflandırma metodları İngilizce Sunum linki, Türkçe Sunum Linki"
  },
  {
    "objectID": "index.html#ders6",
    "href": "index.html#ders6",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 25 Temmuz Perşembe 8:30 - Grup1: 25 Temmuz Perşembe 18:00\nİçerik: Regresyon İngilizce Sunum linki, Türkçe Sunum linki"
  },
  {
    "objectID": "index.html#ders7",
    "href": "index.html#ders7",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 26 Temmuz Cuma 8:30 - Grup1: 26 Temmuz Cuma 14:00\nİçerik: Kümeleme İngilizce Sunum linki, Türkçe Sunum Linki"
  },
  {
    "objectID": "index.html#ders8",
    "href": "index.html#ders8",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Grup2: 01 Ağustos Perşembe 8:30 - Grup1: 01 Ağustos Perşembe 18:00\nİçerik: Doğal Dil İşleme İngilizce Sunum linki, Türkçe Sunum linki"
  },
  {
    "objectID": "id3-decision-tree-TR.html",
    "href": "id3-decision-tree-TR.html",
    "title": "Karar Ağacı Algoritması",
    "section": "",
    "text": "Karar Ağacı Algoritması\nKaynak: https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38\nEntropi\nMakine öğreniminde, entropi işlenen bilgideki rastgeleliğin bir ölçüsüdür. Entropi ne kadar yüksek olursa, o bilgiden herhangi bir sonuç çıkarmak o kadar zor olur.\n\nBilgi Kazancı\nBilgi kazancı, bir rastgele değişken veya sinyalle ilgili olarak başka bir rastgele değişkeni gözlemleyerek elde edilen bilgi miktarı olarak tanımlanabilir. Ebeveyn düğümün entropisi ile çocuk düğümlerin ağırlıklı ortalama entropisi arasındaki fark olarak düşünülebilir.\n\nGini Safsızlığı\nGini safsızlığı, kümeden rastgele seçilen bir elemanın, alt kümedeki etiketlerin dağılımına göre rastgele etiketlendiğinde ne sıklıkla yanlış etiketleneceğinin bir ölçüsüdür.\n\nGini safsızlığı 0 ile alt sınırlıdır ve veri kümesi yalnızca bir sınıf içeriyorsa 0 olur.\n\nKarar ağacı oluşturmak için birçok algoritma vardır. Bunlar:\n\nCART (Sınıflandırma ve Regresyon Ağaçları) — Bu, metrik olarak Gini safsızlığını kullanır.\nID3 (Yinelemeli Bölücü 3) — Bu, metrik olarak entropi ve bilgi kazancını kullanır.\n\nBu makalede ID3’ü ele alacağım. Bunu anladıktan sonra, aynısını CART kullanarak uygulamak kolay olacaktır.\n\n\nID3 algoritması kullanarak sınıflandırma\nFutbol oynayıp oynamama kararını belirleyeceğimiz bir veri setini düşünün.\n\nBurada bağımlı değişkeni belirlemek için dört bağımsız değişken vardır. Bağımsız değişkenler Hava Durumu, Sıcaklık, Nem ve Rüzgardır. Bağımlı değişken ise futbol oynayıp oynamama durumudur.\nİlk adım olarak, karar ağacımız için ebeveyn düğümü bulmalıyız. Bunun için şu adımları izleyin:\nSınıf değişkeninin entropisini bulun.\nE(S) = -[(9/14)log(9/14) + (5/14)log(5/14)] = 0.94\nnot: Burada genellikle 2 tabanında logaritma alırız. Burada toplam 14 evet/hayır var. Bunların 9’u evet ve 5’i hayır. Buna dayanarak yukarıdaki olasılığı hesapladık.\nYukarıdaki verilerden hava durumu için aşağıdaki tabloyu kolayca elde edebiliriz:\n\nŞimdi ağırlıklı ortalama entropiyi hesaplamamız gerekiyor. Yani, her özelliğin ağırlıklarının olasılıklarla çarpımının toplamını bulduk.\nE(S, hava durumu) = (5/14)E(3,2) + (4/14)E(4,0) + (5/14)*E(2,3) = (5/14)(-(3/5)log(3/5)-(2/5)log(2/5))+ (4/14)(0) + (5/14)((2/5)log(2/5)-(3/5)log(3/5)) = 0.693\nBir sonraki adım bilgi kazancını bulmaktır. Bu, ebeveyn entropisi ile yukarıda bulduğumuz ağırlıklı ortalama entropi arasındaki farktır.\nIG(S, hava durumu) = 0.94 - 0.693 = 0.247\nBenzer şekilde Sıcaklık, Nem ve Rüzgar için Bilgi Kazancını bulun.\nIG(S, Sıcaklık) = 0.940 - 0.911 = 0.029\nIG(S, Nem) = 0.940 - 0.788 = 0.152\nIG(S, Rüzgar) = 0.940 - 0.8932 = 0.048\nŞimdi en büyük entropi kazancına sahip özelliği seçin. Burada bu Hava Durumu’dur. Dolayısıyla karar ağacımızın ilk düğümünü (kök düğüm) oluşturur.\nŞimdi verilerimiz şu şekilde görünüyor:\n\n\n\nBulutlu sadece ‘Evet’ sınıfına ait örnekler içerdiğinden, bunu evet olarak ayarlayabiliriz. Yani hava durumu bulutlu ise futbol oynanacak demektir. Şimdi karar ağacımız şu şekilde görünüyor:\n\nBir sonraki adım, karar ağacımızdaki bir sonraki düğümü bulmaktır. Şimdi güneşli altında bir tane bulacağız. Sıcaklık, Nem veya Rüzgardan hangisinin daha yüksek bilgi kazancına sahip olduğunu belirlememiz gerekiyor.\n\nEbeveyn entropisini hesaplayın E(güneşli)\nE(güneşli) = (-(3/5)log(3/5)-(2/5)log(2/5)) = 0.971.\nŞimdi Sıcaklığın bilgi kazancını hesaplayın. IG(güneşli, Sıcaklık)\n\nE(güneşli, Sıcaklık) = (2/5)E(0,2) + (2/5)E(1,1) + (1/5)*E(1,0)=2/5=0.4\nŞimdi bilgi kazancını hesaplayın.\nIG(güneşli, Sıcaklık) = 0.971–0.4 =0.571\nBenzer şekilde şunları elde ederiz:\nIG(güneşli, Nem) = 0.971\nIG(güneşli, Rüzgar) = 0.020\nBurada IG(güneşli, Nem) en büyük değerdir. Bu nedenle Nem, güneşli altına gelen düğümdür.\n\nNem için yukarıdaki tablodan, nem normal ise oyunun gerçekleşeceğini ve yüksek ise gerçekleşmeyeceğini söyleyebiliriz. Benzer şekilde, yağmurlu altındaki düğümleri bulun.\nNot: Entropisi 0’dan büyük olan bir dal daha fazla bölünmeye ihtiyaç duyar.\nSonuç olarak, karar ağacımız aşağıdaki gibi görünecektir:\n\n\n\nCART algoritması kullanarak sınıflandırma\nCART kullanarak sınıflandırma buna benzerdir. Ancak entropi yerine Gini safsızlığını kullanırız.\nİlk adım olarak karar ağacımızın kök düğümünü bulacağız. Bunun için sınıf değişkeninin Gini indeksini hesaplayın\nGini(S) = 1 - [(9/14)² + (5/14)²] = 0.4591\nBir sonraki adım olarak, Gini kazancını hesaplayacağız. Bunun için önce Hava Durumu, Sıcaklık, Nem ve Rüzgarın ağırlıklı ortalama Gini safsızlığını bulacağız.\nİlk olarak Hava Durumu durumunu ele alalım\n\nGini(S, hava durumu) = (5/14)gini(3,2) + (4/14)gini(4,0)+ (5/14)gini(2,3) = (5/14)(1 - (3/5)² - (2/5)²) + (4/14)*0 + (5/14)(1 - (2/5)² - (3/5)²)= 0.171+0+0.171 = 0.342\nGini kazancı (S, hava durumu) = 0.459 - 0.342 = 0.117\nGini kazancı(S, Sıcaklık) = 0.459 - 0.4405 = 0.0185\nGini kazancı(S, Nem) = 0.459 - 0.3674 = 0.0916\nGini kazancı(S, rüzgar) = 0.459 - 0.4286 = 0.0304\nDaha yüksek Gini kazancına sahip olanı seçin. Hava durumu için Gini kazancı daha yüksektir. Bu yüzden onu kök düğümümüz olarak seçebiliriz."
  },
  {
    "objectID": "ders3.html#değerlendirme-ölçütleri-performance-metrics",
    "href": "ders3.html#değerlendirme-ölçütleri-performance-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Değerlendirme ölçütleri / Performance Metrics",
    "text": "Değerlendirme ölçütleri / Performance Metrics\n\nSınıflandırma değerlendirme (Classification)\nRegresyon değerlendirme (Regression)"
  },
  {
    "objectID": "ders3.html#sınıflandırma-ve-regresyon-classification-vs-regression",
    "href": "ders3.html#sınıflandırma-ve-regresyon-classification-vs-regression",
    "title": "Değerlendirme Ölçütleri",
    "section": "Sınıflandırma ve Regresyon (Classification vs regression)",
    "text": "Sınıflandırma ve Regresyon (Classification vs regression)\n\nSource"
  },
  {
    "objectID": "ders3.html#section",
    "href": "ders3.html#section",
    "title": "Değerlendirme Ölçütleri",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders3.html#sınıflandırma-ve-regresyon",
    "href": "ders3.html#sınıflandırma-ve-regresyon",
    "title": "Değerlendirme Ölçütleri",
    "section": "Sınıflandırma ve Regresyon",
    "text": "Sınıflandırma ve Regresyon\n\nSource"
  },
  {
    "objectID": "ders3.html#section-1",
    "href": "ders3.html#section-1",
    "title": "Değerlendirme Ölçütleri",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders3.html#fit",
    "href": "ders3.html#fit",
    "title": "Değerlendirme Ölçütleri",
    "section": "Fit",
    "text": "Fit\n\nSource"
  },
  {
    "objectID": "ders3.html#training",
    "href": "ders3.html#training",
    "title": "Değerlendirme Ölçütleri",
    "section": "Training",
    "text": "Training\n\nSource"
  },
  {
    "objectID": "ders3.html#training-test",
    "href": "ders3.html#training-test",
    "title": "Değerlendirme Ölçütleri",
    "section": "Training / Test",
    "text": "Training / Test\n\nSource"
  },
  {
    "objectID": "ders3.html#classification-performance-metrics",
    "href": "ders3.html#classification-performance-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Classification Performance Metrics",
    "text": "Classification Performance Metrics\nBir Sınıflandırma modeli oluşturulduktan sonra bu model ile yapılan tahminlerin ne kadar doğru olduğuna dair değerlendirme yapılması gereklidir.\nAşağıda verilen confusion matrix (karşılaştırma matrisi) bir sınıflandırma modeline dair gerçekleşen durumları ve bu durumlara dair tahminleri verilmiştir.\n\n\n\n\n\nActual (Gerçek)\n\n\n\n\n\n\n\nTrue (Doğru)\nFalse (Yanlış)\n\n\nPrediction (Tahmin)\nPozitif\nTP\nFP\n\n\n\nNegatif\nFN\nTN\n\n\n\nTP : True Positive, FP : False Positive, FN : False Negative, TN : True Negative\nTrue ve false değeri bu modele dair gerçek sonuçları, positive ve negative ise modele dair tahminleri göstermektedir."
  },
  {
    "objectID": "ders3.html#classification-performance-metrics-1",
    "href": "ders3.html#classification-performance-metrics-1",
    "title": "Değerlendirme Ölçütleri",
    "section": "Classification Performance Metrics",
    "text": "Classification Performance Metrics\nTümör ve Hasta örnekleri ele alırsak, Normal örnekler negatif olarak, Tümör örnekleri de pozitif olarak değerlendirilebilir.\n\nTP : Gerçekte Tümör olan hastayı (true/pozitif) Tümör olarak tahmin etmek (pozitif).\n\nFP : Gerçekte Normal olan örneği (false/negatif) Tümör olarak tahmin etmek (pozitif). — &gt; Type 1 Error\n\nFN : Gerçekte Tümor olan örneği (true/pozitif) Normal olarak tahmin etmek (negatif). — &gt; Type 2 Error\n\nTN : Gerçekte Normal olan örneği (false/negatif) Normal olarak tahmin etmek (negatif)."
  },
  {
    "objectID": "ders3.html#metrics",
    "href": "ders3.html#metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Metrics",
    "text": "Metrics\nAccuracy (Doğruluk) : Doğru tahminlerin toplam veri kümesine oranıdır.\nPrecision (Kesinlik): Pozitif olarak tahmin edilen verilerin kaçının gerçekten pozitif olduğunu gösterir.\nRecall or Sensitivity (Duyarlılık): Geliştirilen modelin pozitif olanların kaçını yakaladığını gösterir.\nF1 Score (F1 Skoru): F1 score, precision ve recall değerlerinin harmonik ortalamasıdır. Sınıf dağılımı benzer olduğunda accuracy kullanılabilirken, dengesiz veri setleri söz konusu olduğunda F1 skor daha iyi bir metriktir.\nROC Curve (ROC Eğrisi): Yanlış pozitif oranı ve gerçek pozitif oranı göz önünde bulundurarak x ekseninde ve y ekseninde 0’dan 100’e kadar olan değerlerin üzerinde bir eğri oluşturulur. Bu eğrinin altında kalan alana Area Under Curve (AUC) adı verilir. Bu alanın büyük olması modelin başarılı olduğunu gösterir. Grafikte yer alan mavi çizgi; ne kadar geniş bir alan kaplıyorsa modelin tahmin başarısı o kadar yüksek, ortadaki kesikli çizgiye ne kadar yakınsa modelin başarı oranı o kadar düşüktür."
  },
  {
    "objectID": "ders3.html#roc-curve",
    "href": "ders3.html#roc-curve",
    "title": "Değerlendirme Ölçütleri",
    "section": "ROC Curve",
    "text": "ROC Curve"
  },
  {
    "objectID": "ders3.html#why-too-many-metrics",
    "href": "ders3.html#why-too-many-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Why too many metrics?",
    "text": "Why too many metrics?\n\n\nNeden birden fazla metrik kullanılıyor, bir örnek ile görelim. Yandaki durum için, 8 Normal ve 2 Tümör olan bir durumda, her örnek için Normal diye tahminde bulunursak. True Negatif (TN) sayısı 8 ve False Negatif (FN) sayısı ise 2 olmaktadır.\nBu durumda, doğruluk, accuracy\nACC = (TP + TN) / (TP + FP + TN + FN)\nformülünden dolayı 8 / 10 = 0.8 yani %80 olarak hesaplanmaktadır.\nFakat, Duyarlılık/Sensitivity\nSENS = TP / (TP + FN)\nformülünden 0 çıkmaktadır.\n\n\n\n\nActual Label\nPrediction\n\n\n\n\nTumor\nNormal\n\n\nTumor\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal\n\n\nNormal\nNormal"
  },
  {
    "objectID": "ders3.html#example",
    "href": "ders3.html#example",
    "title": "Değerlendirme Ölçütleri",
    "section": "Example",
    "text": "Example\n100 kişilik bir çalışmada, 25 Tümör hastası ve 75 Normal birey bulunmaktadır. Normal (negatif) olan 70 kişi Normal olarak tahmin edilmştir (TN: True Negatif). 5 kişi Normal olduğu halde Tümör olarak tahmin edilmiştir (FP: False pozitif). Gerçekte Tümör olan 15 kişi Tümör olarak tahmin edilmiştir (TP: True pozitif). Son olarak, 10 kişi Tümör olduğu halde Normal olarak tahmin edilmiştir (FN: False negatif). Bu duruma göre Confusion Matrix aşağıdaki gibi hesaplanacaktır.\n\n\n\n\n\nActual\n\n\n\n\n\n\n\nTrue\nFalse\n\n\nTahmin\nPozitif\nTP (15)\nFP (5)\n\n\n\nNegatif\nFN (10)\nTN (70)"
  },
  {
    "objectID": "ders3.html#example-1",
    "href": "ders3.html#example-1",
    "title": "Değerlendirme Ölçütleri",
    "section": "Example",
    "text": "Example\nBu durumda,\nDoğruluk (Accuracy) ACC = (TP + TN) / (TP + TN + FN + FP) = 0.85\nDuyarlılık (Sensitivity) SENS = TP / (TP + FN) = 0.60\nKesinlik (Precision) PREC = TP / (TP + FP) = 0.75\nF1 Skor = 2 x (PREC x SENS) / (PREC + SENS) = 0.6667"
  },
  {
    "objectID": "ders3.html#summary",
    "href": "ders3.html#summary",
    "title": "Değerlendirme Ölçütleri",
    "section": "Summary",
    "text": "Summary\n\nSource"
  },
  {
    "objectID": "ders3.html#python-code",
    "href": "ders3.html#python-code",
    "title": "Değerlendirme Ölçütleri",
    "section": "Python code",
    "text": "Python code\n\nfrom sklearn.metrics import confusion_matrix\n\nactual    = [1, 0, 1, 1, 0, 0, 1, 0, 1, 1]\npredicted = [0, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n\nprint(\"\\nConfusion matrix\")\nconf_mat = confusion_matrix(actual, predicted) \n\nprint(conf_mat)\n\n\nConfusion matrix\n[[1 3]\n [2 4]]"
  },
  {
    "objectID": "ders3.html#regression-performance-metrics",
    "href": "ders3.html#regression-performance-metrics",
    "title": "Değerlendirme Ölçütleri",
    "section": "Regression Performance Metrics",
    "text": "Regression Performance Metrics\nFollowing metrics can be used to measure the performance of regression model output.\n\nMean Error (ME)\nMean Absolute Error (MAE)\nMean Squared Error (MSE)\nRoot Mean Squared Error (RMSE)\n\nThere are much more advanced metrics but we’ll learn only essental ones"
  },
  {
    "objectID": "ders3.html#mean-error-me",
    "href": "ders3.html#mean-error-me",
    "title": "Değerlendirme Ölçütleri",
    "section": "Mean Error (ME)",
    "text": "Mean Error (ME)\n\\[ \\text{ME} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i) \\]\n\\(y_i\\): Actual value, \\(\\hat{y}_i\\): predicted value, n: number of observations"
  },
  {
    "objectID": "ders3.html#mean-absolute-error-mae",
    "href": "ders3.html#mean-absolute-error-mae",
    "title": "Değerlendirme Ölçütleri",
    "section": "Mean Absolute Error (MAE)",
    "text": "Mean Absolute Error (MAE)\nAdding negative results is not right when using ME. Let’s take care of it.\n\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| \\]"
  },
  {
    "objectID": "ders3.html#mean-squared-error-mse",
    "href": "ders3.html#mean-squared-error-mse",
    "title": "Değerlendirme Ölçütleri",
    "section": "Mean Squared Error (MSE)",
    "text": "Mean Squared Error (MSE)\n\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\]\nPlease note that squaring the error with punish the model more. Also, squaring a difference will take care of negative sign."
  },
  {
    "objectID": "ders3.html#root-mean-squared-error-rmse",
    "href": "ders3.html#root-mean-squared-error-rmse",
    "title": "Değerlendirme Ölçütleri",
    "section": "Root Mean Squared Error (RMSE)",
    "text": "Root Mean Squared Error (RMSE)\n\\[ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} \\]"
  },
  {
    "objectID": "ders6-TR.html#içerik",
    "href": "ders6-TR.html#içerik",
    "title": "Regresyon Yöntemleri",
    "section": "İçerik",
    "text": "İçerik\n\nRegresyonun tanımı\nRegresyon türleri (doğrusal, polinom ve topluluk yöntemleri)\n\nDoğrusal Regresyon: Basit Doğrusal Regresyon, En Küçük Kareler yöntemi, Model performansının değerlendirilmesi (R-kare, RMSE)\nPolinom Regresyon: Ne zaman polinom regresyon kullanılır, aşırı uyum\nRastgele Orman ile Regresyon: Regresyon için Rastgele Orman algoritması, Avantajları (doğrusal olmayan ilişkileri ele alma, özellik önemi)\nDiğer regresyon türleri: Lojistik regresyon (ikili sonuçlar için), Ridge ve Lasso regresyonu (düzenlileştirme teknikleri)\n\nExcel’de örnekler\nR ve Python’da kod örnekleri"
  },
  {
    "objectID": "ders6-TR.html#regresyon",
    "href": "ders6-TR.html#regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Regresyon",
    "text": "Regresyon\nRegresyon, bir bağımlı değişken (genellikle hedef veya sonuç değişkeni olarak adlandırılır) ile bir veya daha fazla bağımsız değişken (ayrıca tahmin edici veya özellik olarak da bilinir) arasındaki ilişkiyi modellemek ve analiz etmek için veri analizinde ve makine öğrenmesinde kullanılan istatistiksel bir yöntemdir. Regresyonun temel amacı, bağımsız değişkenlerdeki değişikliklerin bağımlı değişkendeki değişikliklerle nasıl ilişkili olduğunu tahmin etmek ve çıkarım yapmaktır.\nRegresyon hakkında önemli noktalar:\n\nDeğişkenler arasındaki ilişkileri tanımlamaya ve ölçmeye yardımcı olur.\nHem tahmin hem de değişkenlerin etkisini anlamak için kullanılabilir.\nDoğrusal, polinom ve Rastgele Orman veya regresyon için Sinir Ağları gibi daha karmaşık yöntemler dahil olmak üzere çeşitli regresyon türleri vardır.\nRegresyon modelleri hem sürekli hem de kategorik değişkenleri ele alabilir.\nBilim, ekonomi ve sosyal bilimler dahil olmak üzere birçok alanda yaygın olarak kullanılır."
  },
  {
    "objectID": "ders6-TR.html#doğrusal-lineer-regresyon",
    "href": "ders6-TR.html#doğrusal-lineer-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Doğrusal (Lineer) Regresyon",
    "text": "Doğrusal (Lineer) Regresyon"
  },
  {
    "objectID": "ders6-TR.html#doğrusal-regresyon",
    "href": "ders6-TR.html#doğrusal-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Doğrusal Regresyon",
    "text": "Doğrusal Regresyon"
  },
  {
    "objectID": "ders6-TR.html#doğrusal-regresyon-1",
    "href": "ders6-TR.html#doğrusal-regresyon-1",
    "title": "Regresyon Yöntemleri",
    "section": "Doğrusal Regresyon",
    "text": "Doğrusal Regresyon\nEn iyi doğruyu bulmayı deneyin: https://www.geogebra.org/m/xC6zq7Zv"
  },
  {
    "objectID": "ders6-TR.html#en-küçük-kareler-ols-yöntemi",
    "href": "ders6-TR.html#en-küçük-kareler-ols-yöntemi",
    "title": "Regresyon Yöntemleri",
    "section": "En Küçük Kareler (OLS) Yöntemi",
    "text": "En Küçük Kareler (OLS) Yöntemi\nEn Küçük Kareler (Ordinary Least Squares - OLS), doğrusal bir regresyon modelinin parametrelerini tahmin etmek için kullanılan istatistiksel bir yöntemdir. Doğrusal regresyonda bir veri kümesine doğru yerleştirmek için en yaygın kullanılan tekniktir.\nOLS, gözlemlenen bağımlı değişken değerleri ile bağımsız değişken(ler)in doğrusal fonksiyonu tarafından tahmin edilen değerler arasındaki farkların karelerinin toplamını en aza indiren bir yöntemdir. Başka bir deyişle, kare artıkların toplamını en aza indirerek verilere en iyi uyan çizgiyi (veya çoklu boyutlarda hiperplane) bulur.\nTeknik detaylar için lütfen Wikipedia’daki OLS Yöntemi sayfasını ziyaret edin."
  },
  {
    "objectID": "ders6-TR.html#çok-değişkenli-doğrusal-regresyon",
    "href": "ders6-TR.html#çok-değişkenli-doğrusal-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Çok Değişkenli Doğrusal Regresyon",
    "text": "Çok Değişkenli Doğrusal Regresyon\nÇok Değişkenli Doğrusal Regresyon, aynı zamanda Çoklu Doğrusal Regresyon olarak da bilinen, birden fazla bağımsız değişken ile tek bir bağımlı değişken arasındaki ilişkiyi modellemek için kullanılan istatistiksel bir yöntemdir. Basit doğrusal regresyon kavramını birden fazla tahmin değişkeni içerecek şekilde genişletir.\nModelin genel formu şöyledir:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\varepsilon\n\\]\nBurada:\ny bağımlı değişken, x₁, x₂, …, xₙ bağımsız değişkenler, β₀ y-kesişimi (tüm x’ler sıfır olduğunda y’nin değeri), β₁, β₂, …, βₙ her bağımsız değişkenle ilişkili katsayılar, n bağımsız değişken sayısı, ε hata terimi"
  },
  {
    "objectID": "ders6-TR.html#polinom-regresyon",
    "href": "ders6-TR.html#polinom-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Polinom Regresyon",
    "text": "Polinom Regresyon\nPolinom Regresyon, bağımsız değişken x ile bağımlı değişken y arasındaki ilişkinin n’inci dereceden bir polinom olarak modellendiği bir regresyon analizi formudur. Değişkenler arasındaki ilişki doğrusal olmadığında ancak bir polinom fonksiyonuyla yaklaşık olarak ifade edilebileceği durumlarda kullanılır.\n\\[\ny = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\ldots + \\beta_nx^n + \\varepsilon\n\\]\nBurada:\ny bağımlı değişken, x bağımsız değişken, β₀, β₁, β₂, …, βₙ tahmin edilecek katsayılar, n polinomun derecesi ve ε hata terimi"
  },
  {
    "objectID": "ders6-TR.html#polinom-regresyon-1",
    "href": "ders6-TR.html#polinom-regresyon-1",
    "title": "Regresyon Yöntemleri",
    "section": "Polinom Regresyon",
    "text": "Polinom Regresyon"
  },
  {
    "objectID": "ders6-TR.html#excelde-doğrusal-regresyon",
    "href": "ders6-TR.html#excelde-doğrusal-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Excel’de Doğrusal Regresyon",
    "text": "Excel’de Doğrusal Regresyon\nMTCARS veri seti Excel dosyasını kullanın ve;\n\nwt (ağırlık) ve mpg (galon başına mil) sütunları arasında Eğilim Çizgisi çizin.\ndisp (silindir hacmi) ve wt (ağırlık) sütunları arasında Eğilim Çizgisi çizin.\n\nExcel’de nasıl Eğilim Çizgisi çizileceğini görmek için lütfen MyExcelOnline sayfasını ziyaret edin."
  },
  {
    "objectID": "ders6-TR.html#r-ile-doğrusal-regresyon",
    "href": "ders6-TR.html#r-ile-doğrusal-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "R ile Doğrusal Regresyon",
    "text": "R ile Doğrusal Regresyon\nBasit doğrusal ve polinom regresyon için lütfen R ile Doğrusal Regresyon sayfasına bakın."
  },
  {
    "objectID": "ders6-TR.html#rastgele-orman-random-forest-ile-regresyon",
    "href": "ders6-TR.html#rastgele-orman-random-forest-ile-regresyon",
    "title": "Regresyon Yöntemleri",
    "section": "Rastgele Orman (Random Forest) ile Regresyon",
    "text": "Rastgele Orman (Random Forest) ile Regresyon\nLütfen R ile rastgele orman regresyonu belgesine bakın."
  },
  {
    "objectID": "kmeans-r.html",
    "href": "kmeans-r.html",
    "title": "K-means clustering with R",
    "section": "",
    "text": "taken from Partitional Clustering in R: The Essentials"
  },
  {
    "objectID": "kmeans-r.html#the-data",
    "href": "kmeans-r.html#the-data",
    "title": "K-means clustering with R",
    "section": "The data",
    "text": "The data\n\nlibrary(factoextra)\ndata(\"USArrests\")\ndf &lt;- scale(USArrests)\n\n\nhead(df)\n\n               Murder   Assault   UrbanPop         Rape\nAlabama    1.24256408 0.7828393 -0.5209066 -0.003416473\nAlaska     0.50786248 1.1068225 -1.2117642  2.484202941\nArizona    0.07163341 1.4788032  0.9989801  1.042878388\nArkansas   0.23234938 0.2308680 -1.0735927 -0.184916602\nCalifornia 0.27826823 1.2628144  1.7589234  2.067820292\nColorado   0.02571456 0.3988593  0.8608085  1.864967207"
  },
  {
    "objectID": "kmeans-r.html#what-should-k-be",
    "href": "kmeans-r.html#what-should-k-be",
    "title": "K-means clustering with R",
    "section": "What should K be?",
    "text": "What should K be?\n\nfviz_nbclust(df, kmeans, \"wss\")"
  },
  {
    "objectID": "kmeans-r.html#clustering",
    "href": "kmeans-r.html#clustering",
    "title": "K-means clustering with R",
    "section": "Clustering",
    "text": "Clustering\n\nkm.res &lt;- kmeans(df, 4, nstart=25)\nprint(km.res)\n\nK-means clustering with 4 clusters of sizes 16, 8, 13, 13\n\nCluster means:\n      Murder    Assault   UrbanPop        Rape\n1 -0.4894375 -0.3826001  0.5758298 -0.26165379\n2  1.4118898  0.8743346 -0.8145211  0.01927104\n3 -0.9615407 -1.1066010 -0.9301069 -0.96676331\n4  0.6950701  1.0394414  0.7226370  1.27693964\n\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             2              4              4              2              4 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             4              1              1              4              2 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             1              3              4              1              3 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             1              3              2              3              4 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             1              4              3              2              4 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             3              3              4              3              1 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             4              4              2              3              1 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             1              1              1              1              2 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             3              2              4              1              3 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             1              1              3              3              1 \n\nWithin cluster sum of squares by cluster:\n[1] 16.212213  8.316061 11.952463 19.922437\n (between_SS / total_SS =  71.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\""
  },
  {
    "objectID": "kmeans-r.html#visualizing-the-results",
    "href": "kmeans-r.html#visualizing-the-results",
    "title": "K-means clustering with R",
    "section": "Visualizing the results",
    "text": "Visualizing the results\n\nfviz_cluster(km.res, data=df)"
  },
  {
    "objectID": "ders1.html",
    "href": "ders1.html",
    "title": "GPT models",
    "section": "",
    "text": "GPT1, GPT2 abd GPT3 were released couple years before ChatGPT and it was able to complete sentences. Please visit “How Does ChatGPT Work?” site for more information. After ChatGPT was released, large language models got “conversational”\nPlease visit OpenAI Playground completion site and select “gpt-3.5-turbo” as model. Type a incomplete sentence and then press Submit button.\nYou’ll see that the model will complete your sentence. If you turn on the “Show probabilities” option on right menu, you’ll get a glimpse of how GPT works.\nIf you complete the sentence and then hover over the words, you’ll see that the model had many words to chose from with certain probabilities. In this example, “my” is the 4th most probable word after Today is and “birthday” is most probable word after Today is my.\nOne of key settings of GPT models is temperature. If you decrease the temperature the model will choose the most probably words. If temperature is high, the model will be picking words with low probabalities, which will bring creative and diverse output.\nIf you decrease the temperature to zero, the output will be always same."
  },
  {
    "objectID": "ders1.html#chatgpt-writes-code-for-us",
    "href": "ders1.html#chatgpt-writes-code-for-us",
    "title": "GPT models",
    "section": "ChatGPT writes code for us",
    "text": "ChatGPT writes code for us\nLet’s ask ChatGPT for a fast prime number function in Python\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nDid you see what just happened! It provided code tailored for us and it also explained bits of the code. Have you noticed that the code contains the tricks we have learned during our lecture: * early stop: if a condition is met, return immediately True or False * instead of checking until N, we should check numbers until \\(\\sqrt{N}\\) (n  0.5** in Python)\nYou can continue the conversation. Let’s ask for memoization.\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nLet’s get the code and test it here:\n\ndef is_prime(n, memo={}):\n    if n in memo:\n        return memo[n]\n    if n in (2, 3):\n        result = True\n    elif n == 1 or n % 2 == 0:\n        result = False\n    else:\n        result = True\n        for i in range(3, int(n ** 0.5) + 1, 2):\n            if n % i == 0:\n                result = False\n                break\n    memo[n] = result\n    return result\n\n\n# is_prime(100000000003)\n\n\n&lt;i class=\"fas fa-fw fa-exclamation-circle mr-3 align-self-center\"&gt;&lt;/i&gt;\n&lt;b&gt;Warning:&lt;/b&gt; At the time of writing this note (January 2023) chatGPT is known to provide results which are not exactly true!&lt;br&gt;So, be aware and don't use code or information you got from chatGPT as is without checking or confirming."
  },
  {
    "objectID": "ders1.html#chatgpt-can-fix-or-modify-the-code",
    "href": "ders1.html#chatgpt-can-fix-or-modify-the-code",
    "title": "GPT models",
    "section": "ChatGPT can fix or modify the code",
    "text": "ChatGPT can fix or modify the code\nYou have a piece of code and it does not work as intended? Then you can ask ChatGPT to fix the code.\nLet’s provide a code with error and ask chatGPT to debug it:\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nSo, ChatGPT can fix code! By the way, did you know that I used ChatGPT to prepare that example ;)\n\n\n\nimage.png\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "ders1.html#is-it-the-end-of-programming",
    "href": "ders1.html#is-it-the-end-of-programming",
    "title": "GPT models",
    "section": "Is it the end of programming?",
    "text": "Is it the end of programming?\nYou might think, “we learned Python for a semester and but now there’s ChatGPT, no need to learn coding at all!”. You might find content online which are exploring similar concerns. But more sane approach is “AI tools will increase speed and efficiency of developers also will decresase barriers to entry for new languages.\nThe section below is taken from Github Copilot’s page:"
  },
  {
    "objectID": "ders1.html#thanks-for-showing-us-chatgpt-i-can-now-prepare-my-homeworks-easily",
    "href": "ders1.html#thanks-for-showing-us-chatgpt-i-can-now-prepare-my-homeworks-easily",
    "title": "GPT models",
    "section": "Thanks for showing us ChatGPT, I can now prepare my homeworks easily!",
    "text": "Thanks for showing us ChatGPT, I can now prepare my homeworks easily!\nSeeing the capabilities of ChatGPT might give some hint about misuse. First of all, a homework is for you to learn a concept (remember the playing the guitar analogy?) so being involved in AI-assisted plagiarism is still a plagiarism and a loss on your side. (please also check the file 00-academic-integrity.ipynb for adverse effects of plagiarism)\nSecond of all, OpenAI and others are working on tools and ways to include watermarks in ChatGPT output so that it can be spotted easily.\nSo, please be inspired by this tool, use it to augment your learning but do not use it for plagiarism."
  },
  {
    "objectID": "ders1.html#closed-and-open-models",
    "href": "ders1.html#closed-and-open-models",
    "title": "GPT models",
    "section": "Closed and Open models",
    "text": "Closed and Open models\nCurrently there are several closed-source large language models which are developed by major corparations each took hundreds of millions to train * GPT3.5 (ChatGPT) and GPT4 by OpenAI * PaLM and Bard by Google * Claude by Anthropics"
  },
  {
    "objectID": "ders1.html#claude-3.5-sonnet-by-antropic",
    "href": "ders1.html#claude-3.5-sonnet-by-antropic",
    "title": "GPT models",
    "section": "Claude 3.5 Sonnet by Antropic",
    "text": "Claude 3.5 Sonnet by Antropic\nClaude 3.5 is the latest version of Claude model. Antropic releases Claude in three sizes Haiku (smallest), Sonnet (medium) and Opus (largest). Currently, Claude 3.5 Sonnest is the best LLM. Generally Claude is good at creative writing but with the latest release it excels at coding as well. Also, Claude allows preview of the code (web-based) on right panel.\nBelow is as example where Claude 3.5 Sonnet was prompted “Please write game of snake in HTML, CSS and JS”. You can actually play the game!\n\nClaude also allows uploading documents (e.g. PDF) and asking questions about the document.\nYou can use Claude (with some limitations) at https://claude.ai/chat after registering an account for free."
  },
  {
    "objectID": "ders1.html#gemini-by-google",
    "href": "ders1.html#gemini-by-google",
    "title": "GPT models",
    "section": "Gemini by Google",
    "text": "Gemini by Google\nGemini is actually an LLM with online search capacity. Also, you can download documents (via Google Drive) or point to YouTube videos to ask questions. Finally, Gemini has the longest context window of 1 million tokens (will fit around 10 books at once)."
  },
  {
    "objectID": "ders1.html#running-models-locally",
    "href": "ders1.html#running-models-locally",
    "title": "GPT models",
    "section": "Running models locally",
    "text": "Running models locally\nSince developers share the model weights, it’s possible to download and run the models locally. There are various sizes of models. 7B (7 billion) models require around 4Gb memory, so you can run them in your laptop.\nYou can run the models by installing PyTorch and some other libraries for Python and then writing some Python code. Or you can install Ollama and then run any compatible model with it.\nAdvantages of running a model locally: 1. Less Censorship 2. Better Data Privacy 3. Offline Usage 4. Cost Savings 5. Better Customization\nDisadvantages of running a model locally: 1. Resource Intensive 2. Slower Responses and Inferior Performance 3. Complex Setup\n\nRunning a model locally using Jupyter notebook or Google Colab\nYou can actually run LLM models in Jupyter notebooks. However the process will require you to install lots of Python packages, downloading the model weights and then writing some Python code to ask questions to the model and then capturing the answer in a dictionary. Although there are some solutions for “chat-like” experience, the experince is more like “one-shot question and answer”.\nPlease check the Youtube video which describes the process in Google Colab, where Google provides GPU (or TPU) to run the model in a Jupyter notebook environment.\n\n\nRunning a model locally with Ollama\n\nPlease visit the list of models page to have an idea about specialized models. With Ollama, you can download and use any of those models.\nIn the terminal, let’s list available models\n$ ollama list\n\nNAME                ID              SIZE    MODIFIED\ndeepseek-coder:6.7b 72be2442d736    3.8 GB  5 weeks ago     \nneural-chat:latest  73940af9fe02    4.1 GB  5 weeks ago     \norca2:7b            ea98cc422de3    3.8 GB  5 weeks ago     \nphi:latest          c651b7a89d73    1.6 GB  14 hours ago    \nsolar:latest        059fdabbe6e6    6.1 GB  2 hours ago     \nstablelm-zephyr:3b  7c596e78b1fc    1.6 GB  3 weeks ago\nLet’s run Phi-2 by Microsoft. Here’s info about Phi-2:\n\na 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters. On complex benchmarks Phi-2 matches or outperforms models up to 25x larger, thanks to new innovations in model scaling and training data curation.\n\n$ ollama run phi:latest\n\n&gt;&gt;&gt; why sky is blue?\n\n The sky appears blue due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere,\nit collides with molecules and tiny particles in the air, such as oxygen and nitrogen atoms. These collisions cause\nthe shorter wavelengths of light (blue) to scatter more than the longer wavelengths (red, orange, yellow, green, and\nviolet). As a result, our eyes perceive the scattered blue light to be dominant, which is why the sky appears blue to us.\nAs you can see, a small model, which can answer questions, help coding can be run locally.\n\n\nRunning a model locally with user interface\n\nOllama Web UI\n\n\n\nLLM Studio\nYou can install LM Studio and then interact with local models with its user interface"
  },
  {
    "objectID": "ders1.html#github-copilot",
    "href": "ders1.html#github-copilot",
    "title": "GPT models",
    "section": "Github CoPilot",
    "text": "Github CoPilot\n\n%%HTML\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ZapdeEJ7xJw?si=3DYWvMa7uudDhUnj\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen&gt;&lt;/iframe&gt;"
  },
  {
    "objectID": "ders1.html#free-github-copilot-alternative",
    "href": "ders1.html#free-github-copilot-alternative",
    "title": "GPT models",
    "section": "Free Github CoPilot alternative",
    "text": "Free Github CoPilot alternative\nCody is Sourcegraph’s AI coding assistant, and it has a couple of features that no other assistant has to make you a 10x developer. You can check it out here: https://sourcegraph.com/cody"
  },
  {
    "objectID": "ders1.html#open-source-local-alternatives",
    "href": "ders1.html#open-source-local-alternatives",
    "title": "GPT models",
    "section": "Open source local alternatives",
    "text": "Open source local alternatives\nPlease visit https://tabby.tabbyml.com/ for more information"
  },
  {
    "objectID": "ders1.html#image-generation",
    "href": "ders1.html#image-generation",
    "title": "GPT models",
    "section": "Image generation",
    "text": "Image generation\n\nDALL-E 3 by OpenAI\nPrompt: “a dna strand with istanbul city as silhoutte in background”\nRevised prompt: “Imagine a detailed science meets architecture illustration. In the foreground, there’s a strand of DNA, magnified and glowing against a dark backdrop. This helix structure is precisely depicted, its familiar double-helix shape twisting majestically. Intertwining with this scientific image, in the background, there’s a silhouette representation of Istanbul city. The skyline shows off its marvelous historical architecture, including its domed mosques and towering minarets. The cityscape sits against a dusky eveningsky, the setting sun casting an orange glow on the horizon behind the buildings. The combination results in a fascinating juxtapositionof science and urban beauty.”\nResult:\n\n\n\nMidjourney v6\nMidjourney announced v6 in December 2023. It can generate photorealistic images. Normally, you need to use their Discord channel to generate images free. Also, you can generate images using getimg.ai. I tried generating images using the following prompts\n\nMolecular biology lab with lots of students.\nBeautiful sea side with white sand beach and tropical trees on shore. The sea looks greenish blue\n\nThe results are great:\n\n\n\nDisinformation\nBe aware of deepfake or AI-generated fake photos\nFake Trump Photo\n\nFake Pope Photo"
  },
  {
    "objectID": "ders1.html#video-generation",
    "href": "ders1.html#video-generation",
    "title": "GPT models",
    "section": "Video generation",
    "text": "Video generation\n\nPrompt to video\n\nRunwayML Gen-2\nPika\nStable Video Diffusion by Stable Diffusion\nGENMO\n\n\n\nImage to video\n\nGENMO"
  },
  {
    "objectID": "ders1.html#music-or-sound-generation",
    "href": "ders1.html#music-or-sound-generation",
    "title": "GPT models",
    "section": "Music or sound generation",
    "text": "Music or sound generation\n\nElevenlabs can convert text to speech online for free with our AI voice generator"
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html",
    "href": "An_Introduction_to_Hierarchical_Clustering.html",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "",
    "text": "Taken from the article “An Introduction to Hierarchical Clustering in Python” from DataCamp"
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#definition-of-hierarchical-clustering",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#definition-of-hierarchical-clustering",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "Definition of Hierarchical Clustering",
    "text": "Definition of Hierarchical Clustering\nA hierarchical clustering approach is based on the determination of successive clusters based on previously defined clusters. It’s a technique aimed more toward grouping data into a tree of clusters called dendrograms, which graphically represents the hierarchical relationship between the underlying clusters."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#comparison-of-hierarchical-clustering-to-other-clustering-techniques",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#comparison-of-hierarchical-clustering-to-other-clustering-techniques",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "Comparison of Hierarchical Clustering to Other Clustering Techniques",
    "text": "Comparison of Hierarchical Clustering to Other Clustering Techniques\nHierarchical clustering is a powerful algorithm, but it is not the only one out there, and each type of clustering comes with its set of advantages and drawbacks.\nLet’s understand how it compares to other types of clustering, such as K-means and model-based clustering. Many more techniques exist, but these two, in addition to hierarchical clustering, are widely used and provide a framework to help better understand the others.\nYou can learn more about clustering in machine learning in our separate article, covering five essential clustering algorithms.\n\nHierarchical clustering vs K Means clustering\nUnlike Hierarchical clustering, K-means clustering seeks to partition the original data points into “K” groups or clusters where the user specifies “K” in advance.\nThe general idea is to look for clusters that minimize the squared Euclidean distance of all the points from the centers over all attributes (variables or features) and merge those individuals in an iterative manner.\nOur K-means Clustering in Python with Scikit-learn tutorial will help you understand the inner workings of K-means clustering with an interesting case study.\n\nBenefits\n\nIt is computationally efficient compared to hierarchical clustering and can be used to analyze large data sets.\nK-means is easier to understand and implement.\n\n\n\nDrawbacks\n\nIt is less flexible than hierarchical clustering because it constrains the user to specify the number of clusters beforehand, which may not be obvious in some situations.\nThe result is not stable and changes from one iteration to another for the same data set.\nIt is more sensitive to outliers because the use of outliers in the data impacts the mean of the cluster.\nBoth k-means and hierarchical clustering are incapable of handling categorical data directly and may not work well with data that is not continuous or has very large variance.\n\nDespite its limitations, k-means clustering is still a popular method due to its ease of use and computational efficiency. It is frequently used as a reference point for comparing the performance of other clustering techniques.\n\n\n\nModel-based clustering\nBoth K-means and hierarchical clustering techniques use a distance matrix to represent the distances between all the points in the dataset. Model-based clustering, on the other hand, applies statistical techniques to identify clusters in the data. Below is the general process:\n\nDecide the statistical model to use and choose the number of clusters.\nFit the model on the data.\nIdentify the clusters based on the model parameters.\n\n\nBenefits\n\nModel-based clustering is more flexible than hierarchical clustering because it allows the use of different models to identify different types of clusters.\nIt works better on data with complex shapes or structures.\n\n\n\nDrawbacks\n\nIt’s computationally more expensive than hierarchical clustering, especially for large data.\nIt requires a better understanding of statistical modeling techniques since the choice of the model can affect the final result.\nSimilarly to K-means clustering, it requires the number of clusters to be specified beforehand."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#applications-of-hierarchical-clustering",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#applications-of-hierarchical-clustering",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "Applications of Hierarchical Clustering",
    "text": "Applications of Hierarchical Clustering\nHierarchical clustering has a variety of applications in our day-to-day life, including (but by no means limited to) biology, image processing, marketing, economics, and social network analysis.\n\nBiology\nThe clustering of DNA sequences is one of the biggest challenges in bioinformatics.\nBiologists can leverage hierarchical clustering to study genetic relationships between organisms to classify those organisms into taxonomic groups. This is beneficial for quick analysis and visualization of the underlying relationships.\n\n\nImage processing\nHierarchical clustering can be performed in image processing to group similar regions or pixels of an image in terms of color, intensity, or other features. This can be useful for further tasks such as image segmentation, image classification, and object recognition.\n\n\nMarketing\nMarketing specialists can use hierarchical clustering to draw a hierarchy between different types of customers based on their purchasing habits for better marketing strategies and product recommendations. For instance, different products in retails can be recommended to customers whether they are low, medium, or high spenders.\n\n\nSocial network analysis\nSocial networks are a great source of valuable information when exploited efficiently. Hierarchical clustering can be used to identify groups or communities and to understand their relationships to each other and the structure of the network as a whole."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering.html#the-hierarchical-clustering-algorithm",
    "href": "An_Introduction_to_Hierarchical_Clustering.html#the-hierarchical-clustering-algorithm",
    "title": "An Introduction to Hierarchical Clustering",
    "section": "The Hierarchical Clustering Algorithm",
    "text": "The Hierarchical Clustering Algorithm\nIn this section, we will look at three main concepts. The steps of the hierarchical algorithm, a highlight of the two types of hierarchical clustering (agglomerative and divisive), and finally, some techniques to choose the right distance measure.\n\nSteps involved in the hierarchical clustering algorithm\nThe hierarchical clustering algorithm employs the use of distance measures to generate clusters. This generation process involves the following main steps:\n\n\n\nCreating Hierarchical Clustering Algorithm\n\n\nPreprocess the data by removing missing data and applying any additional tasks that make the data as clean as possible. This step is a more general one for most of the machine learning tasks.\n\nCompute the distance matrix containing the distance between each pair of data points using a particular distance metric such as Euclidean distance, Manhattan distance, or cosine similarity. But the default distance metric is the Euclidean one.\n\n2.Merge the two clusters that are the closest in distance.\n\nUpdate the distance matrix with regard to the new clusters.\nRepeat steps 1, 2, and 3 until all the clusters are merged together to create a single cluster.\n\n\n\nHierarchical clustering examples\nWe can consider agglomerative and divisive clustering as mirrors of each other. Let’s have a better look at how each one operates, along with a hierarchical clustering example and graphical visualization.\n\nAgglomerative hierarchical clustering\nThis first scenario corresponds to the approach explained above. It starts by considering each observation as a singleton cluster (cluster with only one data point). Then iteratively merges clusters until only one cluster is obtained. This process is also known as the bottom-up approach.\nAs showing in the illustration below:\n\nWe start by considering each animal to be its unique cluster.\nThen we generate three different clusters from those unique animals based on their similarities:\n\nBirds: Eagle and Peacock\nMammals: Lion and Bear\nMore than three leg animals: Spider and Scorpion.\n\nWe repeat the merging process to create the vertebrate cluster by combining the two most similar clusters: Birds and Mammals.\nAfter this step, the remaining two clusters, Vertebrate and More than three legs, are merged to create a single Animals cluster.\n\n\n\n\nDendrogram of Agglomerative Clustering Approach\n\n\nDendrogram of Agglomerative Clustering Approach\n\n\nDivisive clustering\nOn the other hand, divisive clustering is top-down because it starts by considering all the data points as a unique cluster. Then it separates them until all the data points are unique.\nFrom this divisive approach graphic:\n\nWe notice that the whole animal dataset is considered as a single bloc.\nThen, we divide that block into two clusters: Vertebrate and More than 3 legs.\nThe division process is iteratively applied to the previously created clusters until we get unique animals.\n\n\n\n\nDendrogram of Divisive Clustering Approach\n\n\nDendrogram of Divisive Clustering Approach\n\n\n\nChoosing the right distance measure\nYour choice of distance measure is a critical step in clustering, and it depends on the problem you’re trying to solve. Considering the following scenario, we could cluster students based on any number of approaches such as their:\n\nCountry of origin\nGender, either male or female\nPrevious academic background.\n\nThese are all valid clusters but differ in meaning.\nEven though Euclidean distance is the most common distance used in most clustering software, other distance measures such as Manhattan distance, Canberra distance, Pearson or Spearman correlation, and Minkowski distance exist.\n\n\nHow to measure clusters before merging them\nThe previously mentioned distances are related to items. In this section, we will cover three standard ways (not exhaustive) to measure the nearest pair of clusters before merging them: (1) Single linkage, (2) Complete linkage, and (3) Average linkage.\n\nSingle linkage\nFrom all the pairwise distances between the items in the two clusters C1 and C2, the single linkage takes the distance between the clusters as the minimum distance.\nDistance (C1, C2) = Min { d(i, j), where item i is within C1, and item j is within C2}\nOut of all the pairs of items from the two clusters, the ones highlighted in green have the minimum distance.\n\n\n\nSingle linkage illustration\n\n\nSingle linkage illustration\n\n\nComplete linkage\nFrom all the pairwise distances between the items in the two clusters C1 and C2, the single linkage takes the distance between the clusters as the maximum distance.\nDistance (C1, C2) = Max { d(i, j), where item i is within C1, and item j is within C2}\nOut of all the pairs of items from the two clusters, the ones highlighted in green have the maximum distance.\n\n\n\nComplete linkage illustration\n\n\nComplete linkage illustration\n\n\nAverage linkage\nIn the average linkage clustering, the distance between two given clusters C1 and C2 corresponds to the average distances between all pairs of items in the two clusters.\nDistance (C1, C2) = Sum{ d(i, j) } / Total Number of distances\n\n\n\nAverage linkage illustration\n\n\nAverage linkage illustration\nThen the average linkage clustering is performed as follows\nd(a,j) + d(a,h) + d(a,n) + d(d,j) + d(d,h) + d(d,n)\n————————————————————–, where Total number of distances = 6\nTotal number of distances"
  },
  {
    "objectID": "ders8.html#contents",
    "href": "ders8.html#contents",
    "title": "NLP - Natural Language Processing",
    "section": "Contents",
    "text": "Contents\n\n\n\nDefinition and why we need NLP\nCommon NLP Applications\n\nText summarization\nSentiment analysis\nKeyword extraction and association\nDocument classification\nNamed entity recognition\nMachine translation\nQuestion answering systems\nChatbots and conversational AI\n\n\n\n\nNLP Approaches\n\nTerm-based approaches\nDictionary-based approaches\nCorpus-based approaches\nStatistical approaches\nSemantic approaches\nDeep learning approaches"
  },
  {
    "objectID": "ders8.html#contents-contd",
    "href": "ders8.html#contents-contd",
    "title": "NLP - Natural Language Processing",
    "section": "Contents (cont’d)",
    "text": "Contents (cont’d)\n\nPreprocessing Steps in NLP: Text cleaning and normalization, Tokenization, Stop word removal, Stemming and lemmatization, Part-of-speech (POS) tagging, Syntax analysis and parsing, Named entity recognition\nFeature Extraction and Representation: Bag of Words (BoW) model, Term Frequency-Inverse Document Frequency (TF-IDF), N-gram models, Word embeddings (Word2Vec, FastText), Contextual embeddings (e.g., BERT, GPT)"
  },
  {
    "objectID": "ders8.html#definition",
    "href": "ders8.html#definition",
    "title": "NLP - Natural Language Processing",
    "section": "Definition",
    "text": "Definition\nNatural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language. It combines computational linguistics, machine learning, and deep learning techniques to enable computers to understand, interpret, generate, and respond to human language in a valuable way. NLP aims to bridge the gap between human communication and computer understanding by processing and analyzing large amounts of natural language data. This field encompasses various tasks such as speech recognition, text analysis, language translation, sentiment analysis, and text generation, allowing machines to derive meaning from human languages and respond in a way that is both meaningful and contextually appropriate."
  },
  {
    "objectID": "ders8.html#why-nlp",
    "href": "ders8.html#why-nlp",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?\nNatural Language Processing (NLP) is crucial because text data is fundamentally different from the structured numerical and categorical data typically used in traditional machine learning tasks. Here’s why NLP is necessary:\n\nUnstructured nature of text: Unlike tabular data where each column represents a specific feature, text is inherently unstructured. A sentence or document doesn’t come with predefined features or categories. This unstructured nature makes it challenging for computers to directly process and analyze text data.\nHigh dimensionality: Text data can be extremely high-dimensional. In a bag-of-words model, for instance, each unique word in the corpus becomes a feature. This can lead to thousands or even millions of dimensions, far exceeding what’s typically seen in structured data problems.\nSemantic complexity: Words and phrases can have multiple meanings depending on context. For example, “bank” could refer to a financial institution or the side of a river. Traditional ML models aren’t equipped to handle this semantic complexity without NLP techniques."
  },
  {
    "objectID": "ders8.html#why-nlp-1",
    "href": "ders8.html#why-nlp-1",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?\n\nSyntactic variability: The same meaning can be expressed in many different ways in natural language. For example, “The cat sat on the mat” and “On the mat sat the cat” convey the same information but have different word orders. ML models need NLP techniques to understand these variations.\nDealing with noise: Text data often contains “noise” such as spelling errors, informal language, or irrelevant information. NLP provides methods to clean and preprocess this data, making it suitable for analysis.\nFeature extraction: Unlike numerical data where features are explicit, textual features need to be extracted. NLP provides techniques like TF-IDF, word embeddings, and topic modeling to convert text into meaningful numerical representations that ML models can work with.\nHandling sequence information: Many ML models assume independence between features, but in text, the order of words matters. NLP techniques like recurrent neural networks or transformers are designed to capture this sequential information."
  },
  {
    "objectID": "ders8.html#why-nlp-2",
    "href": "ders8.html#why-nlp-2",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?\n\nMultilingual challenges: Text data can come in multiple languages, each with its own rules and nuances. NLP provides tools for language detection, translation, and cross-lingual analysis.\nDealing with context: Understanding text often requires grasping context beyond the immediate words. NLP techniques help in capturing and utilizing this broader context.\n\nIn essence, NLP is needed because it provides the necessary tools and techniques to transform the rich, complex, and unstructured nature of human language into a form that computers can process effectively. It bridges the gap between the way humans communicate and the way machines process information, enabling a wide range of applications from sentiment analysis to machine translation, chatbots, and beyond."
  },
  {
    "objectID": "ders8.html#why-nlp-3",
    "href": "ders8.html#why-nlp-3",
    "title": "NLP - Natural Language Processing",
    "section": "Why NLP?",
    "text": "Why NLP?"
  },
  {
    "objectID": "ders8.html#section",
    "href": "ders8.html#section",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders8.html#section-1",
    "href": "ders8.html#section-1",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Also, recordings is a text input after Speech2Text processing\nimage source"
  },
  {
    "objectID": "ders8.html#section-2",
    "href": "ders8.html#section-2",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Common NLP Applications - Summarization\n\nText summarization is an NLP application that condenses large volumes of text into brief, coherent summaries. For humans, summarizing involves understanding context, identifying key points, and rephrasing ideas - tasks that come naturally to us. However, for computers, this process is remarkably complex. A computer must analyze the text’s structure, grasp semantic meanings, determine the relative importance of different sentences or concepts, and then generate or extract a coherent summary. This requires sophisticated algorithms to mimic human-like understanding of language, context, and relevance. The challenge lies in teaching machines to distinguish between crucial information and extraneous details, a task that involves not just processing words, but understanding their relationships and significance within the broader context of the document.\n\nimage source"
  },
  {
    "objectID": "ders8.html#section-3",
    "href": "ders8.html#section-3",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Common NLP Applications - Sentiment Analysis\nSentiment analysis is an NLP technique that aims to determine the emotional tone behind a piece of text. While humans can often intuitively sense the sentiment in a statement, for computers, this task is surprisingly complex. A computer must go beyond simple keyword matching to grasp the overall emotional context. For instance, the phrase “This movie is sick!” could be positive or negative depending on the context and speaker.\nPlease visit https://hedonometer.org which tracks happiness in US using Twitter/X"
  },
  {
    "objectID": "ders8.html#section-4",
    "href": "ders8.html#section-4",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Common NLP Applications - Named Entity Recognition\n\nis an NLP technique that seeks to locate and classify named entities in text into predefined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. While humans can easily identify and categorize these entities in text, for computers, this task presents great challenges.\nPlease visit Geeks4Geeks Named Entity Recognition for demonstration."
  },
  {
    "objectID": "ders8.html#nlp-approaches---term-based",
    "href": "ders8.html#nlp-approaches---term-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Term based",
    "text": "NLP Approaches - Term based\nTerm-based approaches in NLP represent one of the fundamental methods for processing and analyzing text data. These approaches treat individual words or terms as the basic units of analysis. The core idea is to represent documents as collections of terms, often disregarding grammar and word order but keeping multiplicity. A common implementation is the “bag-of-words” model, where each document is represented as a vector of term frequencies. While simple, this method can be surprisingly effective for many tasks like document classification or information retrieval. However, term-based approaches face challenges with synonymy (different words with the same meaning) and polysemy (same word with multiple meanings). They also struggle to capture context and relationships between words. Despite these limitations, term-based methods serve as a foundation for more advanced NLP techniques and remain relevant in many applications due to their computational efficiency and interpretability"
  },
  {
    "objectID": "ders8.html#section-5",
    "href": "ders8.html#section-5",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders8.html#nlp-approaches---dictionary-based",
    "href": "ders8.html#nlp-approaches---dictionary-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Dictionary based",
    "text": "NLP Approaches - Dictionary based\nDictionary-based approaches in NLP rely on pre-existing lexical resources, such as dictionaries, thesauri, or ontologies, to process and analyze text. These methods leverage human-curated knowledge bases to understand and interpret language. In essence, they attempt to mimic how humans use dictionaries to comprehend unfamiliar words or concepts. For instance, in sentiment analysis, a dictionary-based approach might use a lexicon of words labeled with their associated sentiments."
  },
  {
    "objectID": "ders8.html#section-6",
    "href": "ders8.html#section-6",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders8.html#nlp-approaches---corpus-based",
    "href": "ders8.html#nlp-approaches---corpus-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Corpus based",
    "text": "NLP Approaches - Corpus based\nCorpus-based approaches in NLP rely on large collections of text data, known as corpora, to derive linguistic knowledge and patterns. Unlike dictionary-based methods that depend on pre-existing knowledge bases, corpus-based approaches learn from real-world language usage. These methods use statistical techniques to analyze how words and phrases are used in context across vast amounts of text. For example, word embeddings like Word2Vec or GloVe use neural networks to learn vector representations of words based on their co-occurrence patterns in large corpora.\nCorpus-based methods excel at capturing nuances in language use, handling ambiguity, and adapting to different domains or languages. They’re particularly powerful for tasks like language modeling, machine translation, and text generation."
  },
  {
    "objectID": "ders8.html#nlp-approaches---semantic-based",
    "href": "ders8.html#nlp-approaches---semantic-based",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Approaches - Semantic based",
    "text": "NLP Approaches - Semantic based\nSemantic approaches in NLP focus on understanding the meaning of language, rather than just its structure or statistical patterns. These methods aim to capture and represent the underlying concepts and relationships in text, mirroring human comprehension. Semantic approaches often utilize knowledge graphs, ontologies, or semantic networks to model relationships between concepts. For example, in a biomedical context, a semantic approach might understand that “aspirin” is a type of “pain reliever” which “treats” “headaches”. These methods can handle complex queries, infer new knowledge, and understand context-dependent meanings."
  },
  {
    "objectID": "ders8.html#nlp-preprocessing-steps",
    "href": "ders8.html#nlp-preprocessing-steps",
    "title": "NLP - Natural Language Processing",
    "section": "NLP Preprocessing steps",
    "text": "NLP Preprocessing steps\nText cleaning and normalization:\nThis initial step involves preparing raw text data for analysis by removing or standardizing elements that could interfere with processing. It typically includes tasks like converting text to lowercase to ensure consistency, removing special characters or punctuation that aren’t relevant to the analysis, and handling formatting issues. For instance, in social media text, it might involve expanding contractions (e.g., “don’t” to “do not”), correcting common misspellings, or converting emojis to text descriptions. The goal is to create a standardized, “clean” version of the text that retains its essential meaning while reducing noise and variations that could complicate analysis."
  },
  {
    "objectID": "ders8.html#section-7",
    "href": "ders8.html#section-7",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Tokenization\nTokenization is the process of breaking down text into smaller units called tokens, typically words or subwords. While it might seem straightforward, tokenization can be complex due to language-specific challenges. For example, in English, we might split on whitespace and punctuation, but this approach fails for languages like Chinese that don’t use spaces between words. Tokenization also involves decisions about how to handle contractions, hyphenated words, or multi-word expressions. The choice of tokenization method can significantly impact downstream NLP tasks, as it defines the basic units of meaning that the model will work with."
  },
  {
    "objectID": "ders8.html#section-8",
    "href": "ders8.html#section-8",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Stop word removal\nStop words are common words (like “the”, “is”, “at”, “which”) that are usually filtered out from text processing because they typically don’t carry significant meaning on their own. Removing them can help reduce noise in the data and focus the analysis on the more meaningful content words. However, the definition of what constitutes a stop word can vary depending on the specific application or domain. For instance, in sentiment analysis, words like “not” that are often considered stop words can be crucial for understanding the meaning. Therefore, while stop word removal can improve efficiency and reduce noise, it requires careful consideration of the specific NLP task at hand."
  },
  {
    "objectID": "ders8.html#section-9",
    "href": "ders8.html#section-9",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Stemming and lemmatization\nThese techniques aim to reduce words to their base or dictionary form, which helps in treating different word forms as a single item. Stemming is a crude heuristic process that chops off the ends of words, often resulting in non-words (e.g., “running” becomes “run”, but “universe” becomes “univers”). Lemmatization, on the other hand, uses vocabulary and morphological analysis to return the dictionary form of a word (e.g., “running” becomes “run”, “am” becomes “be”). While stemming is faster and simpler, lemmatization provides more accurate results. The choice between them depends on the specific requirements of the NLP task, balancing between processing speed and linguistic accuracy."
  },
  {
    "objectID": "ders8.html#section-10",
    "href": "ders8.html#section-10",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders8.html#section-11",
    "href": "ders8.html#section-11",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders8.html#section-12",
    "href": "ders8.html#section-12",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Part-of-speech (POS) tagging\nPOS tagging involves labeling each word in a text with its appropriate part of speech (noun, verb, adjective, etc.), based on both its definition and context. This process is crucial for understanding the grammatical structure of sentences and can significantly aid in tasks like named entity recognition or syntactic parsing. Modern POS taggers often use machine learning models trained on large annotated corpora and can achieve high accuracy. However, they can still face challenges with ambiguous words (e.g., “record” can be a noun or a verb) or in handling domain-specific language where words might be used in unconventional ways."
  },
  {
    "objectID": "ders8.html#section-13",
    "href": "ders8.html#section-13",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "image source"
  },
  {
    "objectID": "ders8.html#section-14",
    "href": "ders8.html#section-14",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Part Of Speech\nTag\n\n\n\n\nNoun (Singular)\nNN\n\n\nNoun (Plural)\nNNS\n\n\nVerb\nVB\n\n\nDeterminer\nDT\n\n\nAdjective\nJJ\n\n\nAdverb\nRB"
  },
  {
    "objectID": "ders8.html#section-15",
    "href": "ders8.html#section-15",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Syntax analysis and parsing\nThis technique involves analyzing the grammatical structure of sentences to understand how words relate to each other. It typically produces a parse tree or dependency graph representing the sentence structure. There are two main approaches: constituency parsing, which breaks sentences into nested constituents (phrases), and dependency parsing, which identifies grammatical relationships between individual words. Parsing is crucial for tasks that require understanding sentence structure, like machine translation or question answering. However, it can be computationally intensive and faces challenges with ambiguous sentences or non-standard language use.\nPlease visit the following links: link1"
  },
  {
    "objectID": "ders8.html#feature-extraction-and-representation-techniques-in-nlp",
    "href": "ders8.html#feature-extraction-and-representation-techniques-in-nlp",
    "title": "NLP - Natural Language Processing",
    "section": "Feature Extraction and Representation Techniques in NLP",
    "text": "Feature Extraction and Representation Techniques in NLP"
  },
  {
    "objectID": "ders8.html#section-16",
    "href": "ders8.html#section-16",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Bag of Words (BoW) model:\nThe Bag of Words model is a simple yet effective method for representing text as numerical features. It creates a vocabulary of all unique words in the corpus and represents each document as a vector of word frequencies. The order of words is disregarded, hence the name “bag”. While BoW is computationally efficient and easy to understand, it loses all information about word order and context. It also struggles with out-of-vocabulary words and can result in very high-dimensional, sparse vectors for large vocabularies. Despite these limitations, BoW remains useful for many text classification tasks and serves as a foundation for more advanced techniques."
  },
  {
    "objectID": "ders8.html#section-17",
    "href": "ders8.html#section-17",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Term Frequency-Inverse Document Frequency (TF-IDF):\nTF-IDF is a statistical measure used to evaluate the importance of a word in a document within a collection or corpus. It combines two metrics: Term Frequency (how often a word appears in a document) and Inverse Document Frequency (how rare the word is across all documents). The idea is that words frequently occurring in a document but rare in the overall corpus are likely more important for that document. TF-IDF helps to reduce the impact of common words that appear frequently across all documents but carry little meaning. This method provides a more nuanced representation than simple word counts, making it valuable for tasks like information retrieval and document ranking.\nPlease visit this link for demonstration."
  },
  {
    "objectID": "ders8.html#section-18",
    "href": "ders8.html#section-18",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "N-gram models:\nN-gram models extend the idea of BoW by considering sequences of N adjacent words, rather than individual words in isolation. For example, a bigram (2-gram) model would consider pairs of adjacent words, while a trigram (3-gram) model would look at sequences of three words. N-grams capture some local context and word order information, which can be crucial for tasks like language modeling or machine translation. However, as N increases, the model’s complexity and data sparsity issues also increase. N-gram models strike a balance between the simplicity of BoW and the need to capture word relationships, making them useful in various NLP applications.\nPlease visit these links for visual explanation: link1 link2 link3"
  },
  {
    "objectID": "ders8.html#section-19",
    "href": "ders8.html#section-19",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Word embeddings (Word2Vec, GloVe, FastText):\nWord embeddings are dense vector representations of words in a continuous vector space, where semantically similar words are mapped to nearby points. Unlike BoW or TF-IDF, which produce sparse, high-dimensional vectors, word embeddings typically have a few hundred dimensions. Models like Word2Vec learn these representations by analyzing word co-occurrences in large corpora, capturing semantic relationships (e.g., “king” - “man” + “woman” ≈ “queen”). FastText extends this idea to subword units, allowing it to generate embeddings for out-of-vocabulary words. These dense representations significantly improve performance on various NLP tasks and allow for interesting word algebra. However, they struggle with polysemy (words with multiple meanings) as each word has a single, fixed representation regardless of context.\nPlease visit these links for visual explanations: link1 link2. Please visit Embedding Projector for interactive visualization.\nVisual explanation for FastText"
  },
  {
    "objectID": "ders8.html#section-20",
    "href": "ders8.html#section-20",
    "title": "NLP - Natural Language Processing",
    "section": "",
    "text": "Contextual embeddings (e.g., BERT, GPT):\nContextual embeddings represent a significant advancement over traditional word embeddings by generating different representations for a word based on its context in a sentence. Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) use deep learning architectures to process entire sentences or documents simultaneously, capturing complex, bidirectional relationships between words. These models are pre-trained on vast amounts of text data and can be fine-tuned for specific tasks. Contextual embeddings excel at capturing nuanced meanings, handling polysemy, and understanding long-range dependencies in text. They have set new state-of-the-art benchmarks on various NLP tasks. However, they are computationally intensive and require significant resources to train and deploy, presenting challenges for applications with limited computational power or real-time requirements."
  },
  {
    "objectID": "katilim.html",
    "href": "katilim.html",
    "title": "Attendance and Quizes / Katılım ve Quizler",
    "section": "",
    "text": "Zoom üzerinden süre olarak katılımınız alınarak, toplam ders süremize1 göre oran hesaplanmıştır. Dersin üçte birinden (1/3 ya da 0.3) az katılan öğrenci o derse katılmamış sayılacaktır.\nAşağıdaki tablolarda her bir ders günü için Zoom süresi ve katılım oranı verilmiştir. (ŞU ANDA SADECE 19 TEMMUZ GÜNÜ YAPILAN DERSE GÖRE HESAPLAMA YAPILMIŞTIR)\n\n\n\n\n\n\nClick for translation\n\n\n\n\n\nYour participation time has been recorded via Zoom, and a ratio has been calculated based on our total lesson duration. A student who participates less than one-third (1/3 or 0.3) of the lesson will be considered as not having attended that lesson.\nIn the tables below, the Zoom duration and participation ratio for each lesson day are provided. (THE DATA BELOW ONLY CONSIDERS JULY 19TH LECTURE)"
  },
  {
    "objectID": "katilim.html#group1-grup1-türkçe-1",
    "href": "katilim.html#group1-grup1-türkçe-1",
    "title": "Attendance and Quizes / Katılım ve Quizler",
    "section": "Group1 / Grup1 Türkçe",
    "text": "Group1 / Grup1 Türkçe"
  },
  {
    "objectID": "katilim.html#group2-grup2-ingilizce-1",
    "href": "katilim.html#group2-grup2-ingilizce-1",
    "title": "Attendance and Quizes / Katılım ve Quizler",
    "section": "Group2 / Grup2 İngilizce",
    "text": "Group2 / Grup2 İngilizce"
  },
  {
    "objectID": "katilim.html#footnotes",
    "href": "katilim.html#footnotes",
    "title": "Attendance and Quizes / Katılım ve Quizler",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBeklenen ders süresi (yaklaşık 3 saat) değil, işlenen süre. Yani maksimum süre, ders süresi olarak alınmıştır.↩︎"
  },
  {
    "objectID": "hyper-parameter-tuning-R.html",
    "href": "hyper-parameter-tuning-R.html",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "library(cluster) library(dbscan)\nperform_dbscan &lt;- function(minPts, eps, data, datadist) { dbscan_result &lt;- dbscan(data, minPts = minPts, eps = eps) n_clusters &lt;- length(unique(dbscan_result\\(cluster[dbscan_result\\)cluster != 0])) n_noise &lt;- sum(dbscan_result$cluster == 0)\n  silhouette_score &lt;- if (n_clusters &gt; 1) {\n    mean(silhouette(dbscan_result$cluster, datadist)[, 3])\n  } else {\n    NA_real_\n  }\n\n  list(\n    n_clusters = n_clusters,\n    n_noise = n_noise,\n    silhouette_score = silhouette_score\n  )\n}\nparam_grid &lt;- expand_grid( minPts = seq(2, 10, by = 1), eps = seq(0.05, 0.5, by = 0.05) )\ndata(moons) data &lt;- moons datadist &lt;- dist(data) plot(moons,pch = 16, cex = 2)\nresults_df &lt;- param_grid %&gt;% mutate(results = map2(minPts, eps, ~perform_dbscan(.x, .y, data, datadist))) %&gt;% unnest_wider(results)\nresults_df |&gt; filter(n_clusters &gt; 1) |&gt; arrange(n_clusters+n_noise, -silhouette_score) |&gt; print(n=40)\nbest_result &lt;- dbscan(data, minPts = 3, eps = 0.3) plot(data, col=best_result$cluster+1, pch = 16, cex = 2)"
  },
  {
    "objectID": "r-linear-regression.html",
    "href": "r-linear-regression.html",
    "title": "Linear Regression with R",
    "section": "",
    "text": "Here we have a data mtcars {datasets} that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). We will find a mathematical model between Weight (wt) and Displacement (disp) parameters. So let’s find the most suitable model. Please draw the plot, determine the degree of equation (first, second of third) and draw the curve.\n\nmtcars\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n [ reached 'max' / getOption(\"max.print\") -- omitted 23 rows ]\n\n\n\nplot(mtcars)\n\n\n\n\n\n\n\n\n\nplot(mtcars$disp, mtcars$wt)\n\n\n\n\n\n\n\n\n\nfirst_wd &lt;- lm(wt ~ disp, data=mtcars)\nsummary(first_wd)\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\n\nsecond_wd &lt;- lm(wt ~ disp + I(disp^2), data=mtcars)\nsummary(second_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88654 -0.29136 -0.00961  0.32389  0.67010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.631e+00  3.622e-01   4.502 0.000101 ***\ndisp        6.694e-03  3.325e-03   2.013 0.053448 .  \nI(disp^2)   6.206e-07  6.380e-06   0.097 0.923178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4652 on 29 degrees of freedom\nMultiple R-squared:  0.7886,    Adjusted R-squared:  0.774 \nF-statistic: 54.08 on 2 and 29 DF,  p-value: 1.639e-10\n\n\n\nthird_wd &lt;- lm(wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\nsummary(third_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63764 -0.22785  0.00307  0.24614  0.58333 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.111e+00  5.651e-01  -1.966   0.0593 .  \ndisp         4.945e-02  8.198e-03   6.032 1.68e-06 ***\nI(disp^2)   -1.807e-04  3.361e-05  -5.378 9.88e-06 ***\nI(disp^3)    2.243e-07  4.119e-08   5.446 8.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3299 on 28 degrees of freedom\nMultiple R-squared:  0.8973,    Adjusted R-squared:  0.8863 \nF-statistic: 81.57 on 3 and 28 DF,  p-value: 5.959e-14\n\n\n\nplot(mtcars$disp,mtcars$wt)\ncurve(2.243e-07*x^3 -1.807e-04*x^2 +4.945e-02*x -1.111e+00,add=TRUE)\n\n\n\n\n\n\n\n\n\nbest_model &lt;- lm(wt ~ poly(disp,15), data=mtcars)\nsummary(best_model)\n\n\nCall:\nlm(formula = wt ~ poly(disp, 15), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43800 -0.11202 -0.00364  0.16319  0.37783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.2172500  0.0509709  63.119  &lt; 2e-16 ***\npoly(disp, 15)1   4.8375552  0.2883350  16.778 1.41e-11 ***\npoly(disp, 15)2   0.0452475  0.2883350   0.157   0.8773    \npoly(disp, 15)3   1.7965723  0.2883350   6.231 1.20e-05 ***\npoly(disp, 15)4   0.4463962  0.2883350   1.548   0.1411    \npoly(disp, 15)5  -0.6866393  0.2883350  -2.381   0.0300 *  \npoly(disp, 15)6  -0.3384161  0.2883350  -1.174   0.2577    \npoly(disp, 15)7  -0.6134397  0.2883350  -2.128   0.0493 *  \npoly(disp, 15)8   0.0528245  0.2883350   0.183   0.8569    \npoly(disp, 15)9   0.0814084  0.2883350   0.282   0.7813    \npoly(disp, 15)10 -0.0654665  0.2883350  -0.227   0.8233    \npoly(disp, 15)11  0.2853853  0.2883350   0.990   0.3370    \npoly(disp, 15)12 -0.0007108  0.2883350  -0.002   0.9981    \npoly(disp, 15)13  0.4495723  0.2883350   1.559   0.1385    \npoly(disp, 15)14 -0.0699728  0.2883350  -0.243   0.8113    \npoly(disp, 15)15 -0.5031480  0.2883350  -1.745   0.1002    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2883 on 16 degrees of freedom\nMultiple R-squared:  0.9552,    Adjusted R-squared:  0.9132 \nF-statistic: 22.73 on 15 and 16 DF,  p-value: 5.551e-08\n\n\n\npred &lt;- predict(best_model, data.frame(disp=50:500))\nplot(mtcars$disp, mtcars$wt)\npoints(50:500, pred, type=\"l\")\n\n\n\n\n\n\n\n\n\npredict(third_wd, data.frame(disp=420))\n\n       1 \n4.397246 \n\n\n\npredict(best_model, data.frame(disp=420))\n\n       1 \n7.329183 \n\n\n\npredict(best_model, data.frame(disp=c(20,420, 500)))\n\n           1            2            3 \n 3865.329736     7.329183 -1513.538298 \n\n\n\nSo, the “best” model is not best at all.."
  },
  {
    "objectID": "r-linear-regression.html#a-polynomial-model",
    "href": "r-linear-regression.html#a-polynomial-model",
    "title": "Linear Regression with R",
    "section": "",
    "text": "Here we have a data mtcars {datasets} that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). We will find a mathematical model between Weight (wt) and Displacement (disp) parameters. So let’s find the most suitable model. Please draw the plot, determine the degree of equation (first, second of third) and draw the curve.\n\nmtcars\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n [ reached 'max' / getOption(\"max.print\") -- omitted 23 rows ]\n\n\n\nplot(mtcars)\n\n\n\n\n\n\n\n\n\nplot(mtcars$disp, mtcars$wt)\n\n\n\n\n\n\n\n\n\nfirst_wd &lt;- lm(wt ~ disp, data=mtcars)\nsummary(first_wd)\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\n\nsecond_wd &lt;- lm(wt ~ disp + I(disp^2), data=mtcars)\nsummary(second_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88654 -0.29136 -0.00961  0.32389  0.67010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.631e+00  3.622e-01   4.502 0.000101 ***\ndisp        6.694e-03  3.325e-03   2.013 0.053448 .  \nI(disp^2)   6.206e-07  6.380e-06   0.097 0.923178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4652 on 29 degrees of freedom\nMultiple R-squared:  0.7886,    Adjusted R-squared:  0.774 \nF-statistic: 54.08 on 2 and 29 DF,  p-value: 1.639e-10\n\n\n\nthird_wd &lt;- lm(wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\nsummary(third_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63764 -0.22785  0.00307  0.24614  0.58333 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.111e+00  5.651e-01  -1.966   0.0593 .  \ndisp         4.945e-02  8.198e-03   6.032 1.68e-06 ***\nI(disp^2)   -1.807e-04  3.361e-05  -5.378 9.88e-06 ***\nI(disp^3)    2.243e-07  4.119e-08   5.446 8.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3299 on 28 degrees of freedom\nMultiple R-squared:  0.8973,    Adjusted R-squared:  0.8863 \nF-statistic: 81.57 on 3 and 28 DF,  p-value: 5.959e-14\n\n\n\nplot(mtcars$disp,mtcars$wt)\ncurve(2.243e-07*x^3 -1.807e-04*x^2 +4.945e-02*x -1.111e+00,add=TRUE)\n\n\n\n\n\n\n\n\n\nbest_model &lt;- lm(wt ~ poly(disp,15), data=mtcars)\nsummary(best_model)\n\n\nCall:\nlm(formula = wt ~ poly(disp, 15), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43800 -0.11202 -0.00364  0.16319  0.37783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.2172500  0.0509709  63.119  &lt; 2e-16 ***\npoly(disp, 15)1   4.8375552  0.2883350  16.778 1.41e-11 ***\npoly(disp, 15)2   0.0452475  0.2883350   0.157   0.8773    \npoly(disp, 15)3   1.7965723  0.2883350   6.231 1.20e-05 ***\npoly(disp, 15)4   0.4463962  0.2883350   1.548   0.1411    \npoly(disp, 15)5  -0.6866393  0.2883350  -2.381   0.0300 *  \npoly(disp, 15)6  -0.3384161  0.2883350  -1.174   0.2577    \npoly(disp, 15)7  -0.6134397  0.2883350  -2.128   0.0493 *  \npoly(disp, 15)8   0.0528245  0.2883350   0.183   0.8569    \npoly(disp, 15)9   0.0814084  0.2883350   0.282   0.7813    \npoly(disp, 15)10 -0.0654665  0.2883350  -0.227   0.8233    \npoly(disp, 15)11  0.2853853  0.2883350   0.990   0.3370    \npoly(disp, 15)12 -0.0007108  0.2883350  -0.002   0.9981    \npoly(disp, 15)13  0.4495723  0.2883350   1.559   0.1385    \npoly(disp, 15)14 -0.0699728  0.2883350  -0.243   0.8113    \npoly(disp, 15)15 -0.5031480  0.2883350  -1.745   0.1002    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2883 on 16 degrees of freedom\nMultiple R-squared:  0.9552,    Adjusted R-squared:  0.9132 \nF-statistic: 22.73 on 15 and 16 DF,  p-value: 5.551e-08\n\n\n\npred &lt;- predict(best_model, data.frame(disp=50:500))\nplot(mtcars$disp, mtcars$wt)\npoints(50:500, pred, type=\"l\")\n\n\n\n\n\n\n\n\n\npredict(third_wd, data.frame(disp=420))\n\n       1 \n4.397246 \n\n\n\npredict(best_model, data.frame(disp=420))\n\n       1 \n7.329183 \n\n\n\npredict(best_model, data.frame(disp=c(20,420, 500)))\n\n           1            2            3 \n 3865.329736     7.329183 -1513.538298 \n\n\n\nSo, the “best” model is not best at all.."
  },
  {
    "objectID": "r-linear-regression.html#traintest-split",
    "href": "r-linear-regression.html#traintest-split",
    "title": "Linear Regression with R",
    "section": "Train/Test split",
    "text": "Train/Test split\nLet’s understand the topic of overfitting.\nSplit the data into train and test. Train the model with “training” data. Then predict with “test” data.\n\nsample(1:32, 5, replace=FALSE)\n\n[1] 31 12 16 17  5\n\n\n\nset.seed(2)\nidx &lt;- sample(1:32, 5, replace=FALSE)\nidx\n\n[1] 21 15  6 30  8\n\n\n\ntest &lt;- mtcars[idx,]\n\ntest\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona      21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nValiant            18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 240D          24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\n\n\ntrain &lt;- mtcars[-idx,]\ntrain\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n [ reached 'max' / getOption(\"max.print\") -- omitted 18 rows ]\n\n\n\nthird_deg &lt;- lm(wt ~ poly(disp,3), data = train)\nsummary(third_deg)$r.squared\n\n[1] 0.8879368\n\n\n\npredict(third_deg, test)\n\n     Toyota Corona Cadillac Fleetwood            Valiant       Ferrari Dino \n          2.633774           5.824101           3.418857           2.970499 \n         Merc 240D \n          2.989188 \n\n\n\ntest$third_pred &lt;- predict(third_deg, test)\ntest\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona      21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nValiant            18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 240D          24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n                   third_pred\nToyota Corona        2.633774\nCadillac Fleetwood   5.824101\nValiant              3.418857\nFerrari Dino         2.970499\nMerc 240D            2.989188\n\n\n\nfifteen_deg &lt;- lm(wt ~ poly(disp,15), data=train)\nsummary(fifteen_deg)$r.squared\n\n[1] 0.9550231\n\n\n\npredict(fifteen_deg, test)\n\n     Toyota Corona Cadillac Fleetwood            Valiant       Ferrari Dino \n          2.644378        -844.610091           2.784157           2.761685 \n         Merc 240D \n          2.745391 \n\n\n\ntest$fifteen &lt;- predict(fifteen_deg, test)\n\ntest[c(\"disp\",\"wt\",\"third_pred\",\"fifteen\")]\n\n                    disp    wt third_pred     fifteen\nToyota Corona      120.1 2.465   2.633774    2.644378\nCadillac Fleetwood 472.0 5.250   5.824101 -844.610091\nValiant            225.0 3.460   3.418857    2.784157\nFerrari Dino       145.0 2.770   2.970499    2.761685\nMerc 240D          146.7 3.190   2.989188    2.745391\n\n\nAs you can see, “fifteenth degree” model memorized (i.e overfitted) the data and predicts horribly."
  },
  {
    "objectID": "midterm-grades.html",
    "href": "midterm-grades.html",
    "title": "Midterm Results / Vize Sonuçları",
    "section": "",
    "text": "Sınava katılan öğrenci sayısı: 189 ve vize notlarının ortalaması: 59.6. Aşağıda, vize notlarının dağılımı verilmiştir.\n\n\n\n\n\n\n\n\n\nAşağıdaki tabloda, sayfaları gezinerek veya arama kutusunda öğrenci numaranızı arayarak notlarınızı öğrenebilirsiniz."
  },
  {
    "objectID": "midterm-grades.html#grup-1-türkçe-sonuçları",
    "href": "midterm-grades.html#grup-1-türkçe-sonuçları",
    "title": "Midterm Results / Vize Sonuçları",
    "section": "",
    "text": "Sınava katılan öğrenci sayısı: 189 ve vize notlarının ortalaması: 59.6. Aşağıda, vize notlarının dağılımı verilmiştir.\n\n\n\n\n\n\n\n\n\nAşağıdaki tabloda, sayfaları gezinerek veya arama kutusunda öğrenci numaranızı arayarak notlarınızı öğrenebilirsiniz."
  },
  {
    "objectID": "midterm-grades.html#group-2-en-results",
    "href": "midterm-grades.html#group-2-en-results",
    "title": "Midterm Results / Vize Sonuçları",
    "section": "Group 2 (EN) results",
    "text": "Group 2 (EN) results\nTotal number of students took the exam: 70 and average score is: 60.5. Below is the distribution of scores:\n\n\n\n\n\n\n\n\n\nAnd here’s the scores. You can scroll pages or search for your student number in search box."
  },
  {
    "objectID": "ders7-TR.html#içerik",
    "href": "ders7-TR.html#içerik",
    "title": "Kümeleme Yöntemleri",
    "section": "İçerik",
    "text": "İçerik\n\n\n\nKümelemeye Giriş\n\nTanım ve amaç, Denetimsiz öğrenme kavramı, Bilimsel alanlardaki gerçek dünya uygulamaları\n\nKümeleme Algoritmaları Türleri\n\nBölümleme yöntemleri (örn. K-means)\nHiyerarşik yöntemler\nYoğunluk tabanlı yöntemler (örn. DBSCAN)\n\nK-means Kümeleme\n\nAlgoritma açıklaması\nKüme sayısını seçme (dirsek yöntemi)\nAvantajlar ve sınırlamalar\n\n\n\n\nHiyerarşik Kümeleme\n\nBirleştirici ve bölücü yaklaşımlar\nDendrogram yorumlama\nBağlantı yöntemleri (tek, tam, ortalama)\n\nYoğunluk Tabanlı Mekansal Gürültülü Uygulama Kümelemesi (DBSCAN)\n\nTemel kavramlar: çekirdek noktalar, sınır noktalar, gürültü\nAlgoritma açıklaması\nAvantajlar\n\nKümeleme için Değerlendirme Metrikleri\n\nSilüet skoru"
  },
  {
    "objectID": "ders7-TR.html#tanım",
    "href": "ders7-TR.html#tanım",
    "title": "Kümeleme Yöntemleri",
    "section": "Tanım",
    "text": "Tanım\nKümeleme, makine öğrenmesi ve veri analizinde bir dizi nesneyi veya veri noktasını, aynı gruptaki (küme olarak adlandırılır) nesnelerin diğer gruplardakilere göre birbirine daha benzer olacağı şekilde gruplandırmayı içeren bir tekniktir. Bu, birçok veri analizi ve örüntü tanıma probleminde temel bir görevdir."
  },
  {
    "objectID": "ders7-TR.html#amaç",
    "href": "ders7-TR.html#amaç",
    "title": "Kümeleme Yöntemleri",
    "section": "Amaç",
    "text": "Amaç\nKümelemenin amacı çok yönlüdür:\nÖrüntü Keşfi: Kümeleme, veriler içindeki hemen belli olmayan doğal örüntüleri veya yapıları tanımlamaya yardımcı olur.\nVeri Özetleme: Büyük veri setlerini, birçok veri noktasını daha az küme merkeziyle temsil ederek sıkıştırmak için kullanılabilir.\nAnomali Tespiti: Benzer veri noktalarının gruplarını tanımlayarak, herhangi bir kümeye uymayan aykırı değerleri veya anormallikleri tespit etmeye yardımcı olabilir.\nSegmentasyon: Pazarlama gibi alanlarda, müşterileri benzer davranışlara veya özelliklere sahip gruplara ayırmak için kullanılır.\nDiğer Algoritmalar için Ön İşleme: Kümeleme, verinin karmaşıklığını azaltmak için diğer algoritmalar için bir ön işleme adımı olarak kullanılabilir.\nDoğal Sınıflandırma: Bilimsel alanlarda, kümeleme verilerdeki doğal gruplandırmaları ortaya çıkarabilir, örneğin biyolojide benzer türleri gruplandırmak veya astronomide yıldız türlerini kategorilere ayırmak için."
  },
  {
    "objectID": "ders7-TR.html#kümeleme-yöntemleri",
    "href": "ders7-TR.html#kümeleme-yöntemleri",
    "title": "Kümeleme Yöntemleri",
    "section": "Kümeleme Yöntemleri",
    "text": "Kümeleme Yöntemleri\nBölümleme yöntemleri: Veriyi örtüşmeyen alt kümelere böler, her veri noktası tam olarak bir kümeye ait olur, genellikle belirli bir kriteri optimize eder.\nHiyerarşik Yöntemler: Küçük kümeleri daha büyük olanlarla birleştirerek veya büyük kümeleri daha küçük olanlara bölerek kümelerin ağaç benzeri bir yapısını oluşturur.\nYoğunluk tabanlı Yöntemler: Yüksek veri noktası yoğunluğu olan alanlarda kümeler oluşturur, düşük yoğunluklu bölgelerle ayrılır, keyfi şekilli kümelerin keşfine olanak tanır."
  },
  {
    "objectID": "ders7-TR.html#uygulamalar",
    "href": "ders7-TR.html#uygulamalar",
    "title": "Kümeleme Yöntemleri",
    "section": "Uygulamalar",
    "text": "Uygulamalar\n\nBiyoloji\n\n\nGen İfadesi Analizi: Farklı koşullar veya zaman noktaları arasında benzer ifade kalıplarına sahip genleri gruplandırma.\nTür Sınıflandırması: Organizmaları genetik veya morfolojik özelliklere göre kümeleme.\nProtein Yapısı Analizi: Proteinlerde yapısal motifleri tanımlama.\n\n\nKimya\n\n\nMoleküler Dinamikler: Simülasyonlar sırasında moleküllerin konformasyonlarını analiz etme.\nSpektroskopi: Bileşik tanımlama için benzer spektrumları kümeleme.\nİlaç Keşfi: Benzer özelliklere veya etkilere sahip kimyasal bileşikleri gruplandırma."
  },
  {
    "objectID": "ders7-TR.html#section",
    "href": "ders7-TR.html#section",
    "title": "Kümeleme Yöntemleri",
    "section": "",
    "text": "Fizik\n\n\nParçacık Fiziği: Yüksek enerjili fizik deneylerinde parçacık çarpışma olaylarını sınıflandırma.\nAstrofizik: Yıldızları veya galaksileri özelliklerine göre (örn. parlaklık, sıcaklık) kümeleme.\nMalzeme Bilimi: Malzemelerdeki kristal yapıları ve kusurları analiz etme."
  },
  {
    "objectID": "ders7-TR.html#bölümleme",
    "href": "ders7-TR.html#bölümleme",
    "title": "Kümeleme Yöntemleri",
    "section": "Bölümleme",
    "text": "Bölümleme\n\nGörsel kaynağı"
  },
  {
    "objectID": "ders7-TR.html#k-means-kümeleme",
    "href": "ders7-TR.html#k-means-kümeleme",
    "title": "Kümeleme Yöntemleri",
    "section": "K-means kümeleme",
    "text": "K-means kümeleme\nK-means kümeleme, benzer veri noktalarını kümelere gruplandırmak için kullanılan denetimsiz bir makine öğrenimi algoritmasıdır. Veri noktalarını, küme merkezlerine (merkezoidlere) olan benzerliklerine göre önceden tanımlanmış k kümeye iteratif olarak atayarak çalışır. Algoritma, özellik uzayında k merkezoidi rastgele başlatarak başlar. Daha sonra tekrarlı olarak iki adım gerçekleştirir:\n\nher veri noktasını en yakın merkezoide atar, ve\nmerkezoidleri her kümedeki tüm noktaların ortalaması olarak yeniden hesaplar.\n\nBu süreç, merkezoidler stabilize olana veya maksimum iterasyon sayısına ulaşılana kadar devam eder. K-means, küme içi kareler toplamını minimize etmeyi amaçlar, bu da kompakt ve belirgin kümeler oluşturur."
  },
  {
    "objectID": "ders7-TR.html#k-means-kümeleme---interaktif-demolar",
    "href": "ders7-TR.html#k-means-kümeleme---interaktif-demolar",
    "title": "Kümeleme Yöntemleri",
    "section": "K-means kümeleme - interaktif demolar",
    "text": "K-means kümeleme - interaktif demolar\nLütfen aşağıdaki siteleri ziyaret edin:\n\nK-means kümeleme algoritmasının iki boyutlu görselleştirmesi\nK-means Kümeleme - İnteraktif Gösterim\nK-Means Kümelemesini Görselleştirme"
  },
  {
    "objectID": "ders7-TR.html#r-ile-k-means-kümeleme",
    "href": "ders7-TR.html#r-ile-k-means-kümeleme",
    "title": "Kümeleme Yöntemleri",
    "section": "R ile K-means kümeleme",
    "text": "R ile K-means kümeleme\nKod için lütfen R ile K-means sayfasını ziyaret edin."
  },
  {
    "objectID": "ders7-TR.html#k-means-kümelemenin-avantajları",
    "href": "ders7-TR.html#k-means-kümelemenin-avantajları",
    "title": "Kümeleme Yöntemleri",
    "section": "K-means kümelemenin avantajları",
    "text": "K-means kümelemenin avantajları\n\nBasitlik: Anlaşılması ve uygulanması kolay.\nVerimlilik: Genellikle hızlı ve hesaplama açısından verimli, özellikle büyük veri setleri için.\nÖlçeklenebilirlik: Yüksek boyutlu verilerle iyi çalışır.\nEsneklik: Uygun uzaklık metrikleriyle farklı veri türlerine uyarlanabilir.\nYorumlanabilirlik: Sonuçta elde edilen kümeler genellikle yorumlanması kolaydır."
  },
  {
    "objectID": "ders7-TR.html#k-means-kümelemenin-dezavantajları",
    "href": "ders7-TR.html#k-means-kümelemenin-dezavantajları",
    "title": "Kümeleme Yöntemleri",
    "section": "K-means kümelemenin dezavantajları",
    "text": "K-means kümelemenin dezavantajları\n\nBaşlangıç merkezoidlerine duyarlılık: Sonuçlar, merkezoidlerin başlangıçtaki rastgele yerleşimine bağlı olarak değişebilir.\nÖnceden tanımlanmış k gerektirir: Küme sayısı önceden belirtilmelidir, bu her zaman bilinmeyebilir.\nKüresel kümeler varsayar: Küresel olmayan küme şekilleriyle zayıf performans gösterir.\nAykırı değerlere duyarlılık: Aykırı değerler, merkezoid hesaplamasını ve küme atamasını önemli ölçüde etkileyebilir.\nDeğişen küme boyutlarıyla mücadele eder: Benzer mekansal büyüklükte kümeler oluşturma eğilimindedir.\nYerel optima: Global optimum yerine yerel bir minimuma yakınsayabilir.\nKategorik veriyi doğrudan işleyememe: Sayısal olmayan veriler için ön işleme gerektirir."
  },
  {
    "objectID": "ders7-TR.html#hiyerarşik-kümeleme",
    "href": "ders7-TR.html#hiyerarşik-kümeleme",
    "title": "Kümeleme Yöntemleri",
    "section": "Hiyerarşik kümeleme",
    "text": "Hiyerarşik kümeleme\nHiyerarşik kümeleme, benzer veri noktalarını kümelere gruplandırmak için kullanılan denetimsiz bir makine öğrenimi tekniğidir. Önceden tanımlanmış bir küme sayısı gerektiren diğer kümeleme yöntemlerinin aksine, hiyerarşik kümeleme dendrogram adı verilen ağaç benzeri bir küme yapısı oluşturur. Bu yöntem, ya her veri noktasını kendi kümesi olarak başlatıp en yakın kümeleri iteratif olarak birleştirerek (birleştirici yaklaşım), ya da tüm veri noktalarını tek bir kümede başlatıp tekrarlı olarak bölerek (bölücü yaklaşım) çalışır. Bu süreç, istenen küme sayısı veya kümeler arası bir eşik mesafesi gibi bir durma kriteri karşılanana kadar devam eder.\nHiyerarşik kümeleme, verilerin doğal yapısını farklı detay seviyelerinde keşfetmek ve küme sayısının önceden bilinmediği veri setleri için özellikle kullanışlıdır."
  },
  {
    "objectID": "ders7-TR.html#nasıl-çalışır",
    "href": "ders7-TR.html#nasıl-çalışır",
    "title": "Kümeleme Yöntemleri",
    "section": "Nasıl çalışır",
    "text": "Nasıl çalışır\nHiyerarşik kümeleme, seçilen bir uzaklık metriği (örn. Öklid uzaklığı) kullanarak tüm veri noktası çiftleri arasındaki uzaklıkları hesaplayarak başlar. Daha yaygın olan birleştirici yaklaşımda, her veri noktası kendi kümesi olarak başlar. Algoritma daha sonra bir bağlantı kriterine (örn. tek bağlantı, tam bağlantı veya ortalama bağlantı) göre en yakın iki kümeyi iteratif olarak birleştirir. Bu süreç, tüm veri noktaları tek bir kümede toplanana veya istenen küme sayısına ulaşılana kadar tekrarlanır, daha büyük kümeler oluşturur. Sonuç, bir dendrogram olarak görselleştirilebilen hiyerarşik bir yapıdır ve kullanıcıların ihtiyaçlarına en uygun kümeleme seviyesini seçmelerine olanak tanır.\nLütfen Veri Madenciliğinde Hiyerarşik Kümeleme ve Hiyerarşik Kümelemeye Giriş sayfalarını ziyaret edin."
  },
  {
    "objectID": "ders7-TR.html#section-1",
    "href": "ders7-TR.html#section-1",
    "title": "Kümeleme Yöntemleri",
    "section": "",
    "text": "Demolar için lütfen aşağıdaki siteyi ziyaret edin:\n\nyFiles’da Kümeleme Demosu"
  },
  {
    "objectID": "ders7-TR.html#hiyerarşik-kümelemenin-avantajları",
    "href": "ders7-TR.html#hiyerarşik-kümelemenin-avantajları",
    "title": "Kümeleme Yöntemleri",
    "section": "Hiyerarşik kümelemenin avantajları",
    "text": "Hiyerarşik kümelemenin avantajları\n\nÖnceden tanımlanmış küme sayısı gerekmez: K-means’in aksine, hiyerarşik kümeleme önceden küme sayısının belirtilmesini gerektirmez.\nHiyerarşik temsil: Farklı seviyelerde veri yapısına dair içgörüler sunan bir dendrogram sağlar.\nKüme detay seviyesinde esneklik: Kullanıcılar, dendrogramı farklı seviyelerden keserek ihtiyaçlarına en uygun kümeleme seviyesini seçebilirler.\nÇeşitli veri türlerine uygulanabilirlik: Benzerlik veya uzaklık ölçüsü tanımlanabilen her veri türüne uygulanabilir.\nYorumlanabilirlik: Dendrogram, kümeleme sürecinin görsel olarak sezgisel bir temsilini sunar.\nFarklı küme şekillerini işleyebilme: Sadece küresel olanlar değil, çeşitli şekil ve boyutlardaki kümeleri tanımlayabilir."
  },
  {
    "objectID": "ders7-TR.html#dezavantajları",
    "href": "ders7-TR.html#dezavantajları",
    "title": "Kümeleme Yöntemleri",
    "section": "Dezavantajları",
    "text": "Dezavantajları\n\nHesaplama karmaşıklığı: Birçok uygulamada O(n^2) alan karmaşıklığı ve O(n^3) zaman karmaşıklığı, çok büyük veri setleri için daha az uygun hale getirir.\nGürültü ve aykırı değerlere duyarlılık: Aykırı değerler dendogramın şeklini önemli ölçüde etkileyebilir.\nYüksek boyutlu verileri ele almada zorluk: Boyut sayısı arttıkça performans ve yorumlanabilirlik azalabilir.\nGeri alma eksikliği: Bir birleştirme veya bölme yapıldığında geri alınamaz, bu da optimal olmayan sonuçlara yol açabilir.\nBellek yoğun: Büyük veri setleri için uzaklık matrisini saklamak bellek yoğun olabilir.\nUzaklık metriği ve bağlantı yöntemi seçimi: Sonuçlar, seçilen uzaklık metriği ve bağlantı yöntemine göre önemli ölçüde değişebilir, dikkatli düşünme gerektirir.\nÖlçeklenebilirlik sorunları: Çok büyük veri setleri için diğer bazı kümeleme yöntemlerine göre daha az ölçeklenebilir."
  },
  {
    "objectID": "ders7-TR.html#dbscan-kümeleme",
    "href": "ders7-TR.html#dbscan-kümeleme",
    "title": "Kümeleme Yöntemleri",
    "section": "DBSCAN kümeleme",
    "text": "DBSCAN kümeleme\nDBSCAN (Gürültülü Uygulamalarla Yoğunluk Tabanlı Mekansal Kümeleme), uzayda yakın paketlenmiş noktaları gruplandıran ve düşük yoğunluklu bölgelerde yalnız duran noktaları aykırı değer olarak işaretleyen popüler bir kümeleme algoritmasıdır.\nRastgele bir ziyaret edilmemiş nokta seçerek ve belirli bir yarıçap (ε) içindeki komşuluğunu keşfederek çalışır. Bu komşuluk minimum sayıda nokta (MinPts) içeriyorsa, bir küme oluşturulur. Algoritma daha sonra yeni eklenen noktaların komşuluklarını keşfederek kümeyi yinelemeli olarak genişletir. Bir nokta küme oluşturmak için yeterli komşuya sahip değilse, gürültü olarak etiketlenir. Bu süreç tüm noktalar ziyaret edilene kadar devam eder ve çeşitli şekil ve boyutlarda yoğun kümeler seti ile birlikte tanımlanmış gürültü noktaları ortaya çıkar."
  },
  {
    "objectID": "ders7-TR.html#interaktif-demolar",
    "href": "ders7-TR.html#interaktif-demolar",
    "title": "Kümeleme Yöntemleri",
    "section": "İnteraktif demolar",
    "text": "İnteraktif demolar\nLütfen temel kavramlar hakkında DBSCAN — Görselleştirilmiş ve detaylı bir giriş sayfasını ziyaret edin.\nDBSCAN algoritması için interaktif demo için lütfen DBSCAN Kümelemesini Görselleştirme sayfasını ziyaret edin."
  },
  {
    "objectID": "ders7-TR.html#dbscan-algoritması",
    "href": "ders7-TR.html#dbscan-algoritması",
    "title": "Kümeleme Yöntemleri",
    "section": "DBSCAN Algoritması",
    "text": "DBSCAN Algoritması"
  },
  {
    "objectID": "ders7-TR.html#dbscanin-avantajları",
    "href": "ders7-TR.html#dbscanin-avantajları",
    "title": "Kümeleme Yöntemleri",
    "section": "DBSCAN’in Avantajları",
    "text": "DBSCAN’in Avantajları\n\nŞekil esnekliği: Sadece dairesel olanlar değil, keyfi şekilli kümeleri bulabilir.\nAykırı değer tespiti: Otomatik olarak gürültü noktalarını ve aykırı değerleri tanımlar.\nÖnceden tanımlanmış küme sayısı yok: K-means’in aksine, önceden küme sayısını belirtmeyi gerektirmez.\nYoğunluk tabanlı: Farklı yoğunluktaki kümeleri ayırmada etkilidir.\nSağlamlık: Merkez tabanlı algoritmalara göre aykırı değerlere daha az duyarlıdır."
  },
  {
    "objectID": "ders7-TR.html#dezavantajları-1",
    "href": "ders7-TR.html#dezavantajları-1",
    "title": "Kümeleme Yöntemleri",
    "section": "Dezavantajları",
    "text": "Dezavantajları\n\nParametre hassasiyeti: Sonuçlar, ε ve MinPts parametrelerinin seçimine bağlı olarak önemli ölçüde değişebilir.\nDeğişen yoğunluklarla mücadele: Kümeler çok farklı yoğunluklara sahip olduğunda zorluk yaşayabilir.\nYüksek boyutlu veriler: “Boyutsallık laneti” nedeniyle yüksek boyutlu uzaylarda performans ve etkinlik azalabilir.\nBellek gereksinimleri: Büyük veri setleri için bellek yoğun olabilir.\nÖlçeklenebilirlik: Bazı diğer algoritmalara göre çok büyük veri setleri için o kadar verimli değildir.\nBağlantı tabanlı: Kümelerin yakın ama seyrek alanlarla ayrıldığı veri setlerinde zorlanabilir."
  },
  {
    "objectID": "ders7-TR.html#kümelemeyi-değerlendirme",
    "href": "ders7-TR.html#kümelemeyi-değerlendirme",
    "title": "Kümeleme Yöntemleri",
    "section": "Kümelemeyi değerlendirme",
    "text": "Kümelemeyi değerlendirme\nTanım: Silhouette skoru -1 ile 1 arasında değişir, burada:\n\n1’e yakın bir skor, veri noktasının kendi kümesine iyi uyduğunu ve komşu kümelere kötü uyduğunu gösterir.\n0 civarında bir skor, veri noktasının iki komşu küme arasındaki karar sınırında veya çok yakınında olduğunu önerir.\nNegatif bir skor, veri noktasının yanlış kümeye atanmış olabileceğini gösterir.\n\nHesaplama: Her veri noktası i için, silhouette skoru s(i) şöyle hesaplanır:\ns(i) = (b(i) - a(i)) / max(a(i), b(i))\nBurada: a(i), i ile aynı kümedeki diğer tüm noktalar arasındaki ortalama mesafedir b(i), i ile i’nin parçası olmadığı en yakın kümedeki tüm noktalar arasındaki ortalama mesafedir\nYorumlama:\n\nDaha yüksek silhouette skorları daha iyi tanımlanmış kümeleri gösterir.\nTüm noktalar üzerinden ortalama silhouette skoru, farklı kümeleme konfigürasyonlarını karşılaştırmak için kullanılabilir."
  },
  {
    "objectID": "ders7-TR.html#sınıflandırma-vs.-kümeleme",
    "href": "ders7-TR.html#sınıflandırma-vs.-kümeleme",
    "title": "Kümeleme Yöntemleri",
    "section": "Sınıflandırma vs. Kümeleme",
    "text": "Sınıflandırma vs. Kümeleme\nlütfen bu iki kavramı karşılaştırın"
  },
  {
    "objectID": "quiz-clustering.html",
    "href": "quiz-clustering.html",
    "title": "FEF3001 Yapay Zekaya Giriş",
    "section": "",
    "text": "Question 1. The goal of clustering a set of data is to\nA* divide them into groups of data that are near each other B choose the best data from the set C determine the nearest neighbors of each of the data D predict the class of data\nQuestion 2. The choice of k, the number of clusters to partition a set of data into,\nA is a personal choice that shouldn’t be discussed in public B* depends on why you are clustering the data C should always be as large as your computer system can handle D has maximum 10\nQuestion 3. Which of the following statement about the K-means algorithm is correct?\nA* The K-means algorithm is sensitive to outliers. B For different initializations, the K-means algorithm will definitely give the same clustering results. C There’s no need to define K beforehand D The K-means algorithm is ideal for non-spherical shaped data\nQuestion 4. Considering the K-median algorithm, if points (0, 3), (2, 1), and (-2, 2) are the only points which are assigned to the first cluster now, what is the new centroid for this cluster?\nA* (0,2) B (2,1) C (2,0) D (1,2)\nQuestion 5. The Iris dataset contains information about Iris setosa and versicolor. What is the Euclidean distance between these two objects?\nA* 2.8 B 4.6 C 22.6 D -3.6\nQuestion 6. What are the two types of Hierarchical Clustering\nA* Top-Down Clustering (Divisive) B* Bottom-Top Clustering (Agglomerative) C Dendrogram D K-means\nQuestion 7. The most commonly used measure of similarity is the _____ or its square.\nA* euclidean distance B city-block distance C Chebychev’s distance D Manhattan distance\nQuestion 8. Which of the following is required by K-means clustering?\nA defined distance metric B number of clusters C initial guess as to cluster centroids D* all answers are correct\nQuestion 9. In the figure above, if you draw a horizontal line on y-axis for y=2. What will be the number of clusters formed?\nA* 2 B 3 C 4 D 5\nQuestion 10. For which of the following tasks might clustering be a suitable approach? A Given sales data from a large number of products in a supermarket, estimate future sales for each of these products. B* Given a database of information about your users, automatically group them into different market segments. C* From the user’s usage patterns on a website, identify different user groups. D Given historical weather records, predict if tomorrow’s weather will be sunny or rainy."
  },
  {
    "objectID": "ders5-TR.html#içerik",
    "href": "ders5-TR.html#içerik",
    "title": "Sınıflandırma Yöntemleri",
    "section": "İçerik",
    "text": "İçerik\n\n\n\nDenetimli öğrenmeye giriş\nSınıflandırmanın tanımı ve uygulamaları\nVeri hazırlama\n\nÖzellik seçimi ve ön işleme ✅\n\nYöntemler\n\nKarar Ağaçları\nRastgele Orman\nDestek Vektör Makineleri (SVM)\nLojistik Regresyon\nK-en yakın komşu\nNaive Bayes\nYapay Sinir Ağları ✅\n\n\n\n\nEnsemble yöntemleri\nDeğerlendirme ✅\n\nKarmaşıklık Matrisi ✅\nDoğruluk, hassasiyet, geri çağırma, F1-skoru ✅\nROC eğrileri ✅\n\nAşırı öğrenme (overfitting) ve yetersiz öğrenme (underfitting) ✅\nÇapraz doğrulama (Cross Validation)"
  },
  {
    "objectID": "ders5-TR.html#section",
    "href": "ders5-TR.html#section",
    "title": "Sınıflandırma Yöntemleri",
    "section": "",
    "text": "Denetimli Öğrenme\n\nKaynak"
  },
  {
    "objectID": "ders5-TR.html#section-1",
    "href": "ders5-TR.html#section-1",
    "title": "Sınıflandırma Yöntemleri",
    "section": "",
    "text": "Denetimsiz Öğrenme\n\nKaynak"
  },
  {
    "objectID": "ders5-TR.html#sınıflandırmanın-tanımı",
    "href": "ders5-TR.html#sınıflandırmanın-tanımı",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Sınıflandırmanın tanımı",
    "text": "Sınıflandırmanın tanımı\n\nDenetimli öğrenmenin bir türü\nAmaç: Giriş verilerini önceden tanımlanmış sınıflara veya kategorilere ayırmak\nModel, sınıflar arasında karar sınırları çizmeyi öğrenir\nÇıktı, ayrık bir sınıf etiketidir (sürekli değerler tahmin eden regresyondan farklı olarak)"
  },
  {
    "objectID": "ders5-TR.html#uygulamalar",
    "href": "ders5-TR.html#uygulamalar",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Uygulamalar",
    "text": "Uygulamalar\n\n\n\nMetin Sınıflandırma\n\nE-postalarda spam tespiti\nÜrün yorumlarının duygu analizi\nHaber makalelerinin kategorize edilmesi\n\nGörüntü Sınıflandırma\n\nHastalık tespiti için tıbbi görüntüleme\nYüz tanıma sistemleri\nBitki veya hayvan türlerinin tanımlanması\n\nFinansal Uygulamalar\n\nKredi puanlaması (kredi başvurularını onaylama/reddetme)\nİşlemlerde dolandırıcılık tespiti\n\n\n\n\nSağlık Hizmetleri\n\nBelirtiler ve test sonuçlarına dayalı hastalık teşhisi\nHasta yeniden yatış riskini tahmin etme\n\nÇevre Bilimi\n\nİklim deseni sınıflandırması\nTür habitat tahmini\n\nEdebiyat ve Dilbilim\n\nYazarlık atfı\nMetinlerin tür sınıflandırması\nDil tanımlama"
  },
  {
    "objectID": "ders5-TR.html#sıra-sizde",
    "href": "ders5-TR.html#sıra-sizde",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Sıra Sizde",
    "text": "Sıra Sizde\nZoom sohbet penceresinde lütfen bölümünüzü ve alanınızla ilgili bir sınıflandırma görevi örneği yazın"
  },
  {
    "objectID": "ders5-TR.html#sıra-sizde-1",
    "href": "ders5-TR.html#sıra-sizde-1",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Sıra Sizde",
    "text": "Sıra Sizde\nBir örnek seçin ve veriler hakkında tartışın\nKaggle’ı ziyaret edin ve ilgili veri setini bulun"
  },
  {
    "objectID": "ders5-TR.html#karar-ağaçları",
    "href": "ders5-TR.html#karar-ağaçları",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Karar Ağaçları",
    "text": "Karar Ağaçları\nKarar Ağaçları, kararların ve olası sonuçlarının ağaç benzeri bir modelini kullanan bir sınıflandırma yöntemidir. Algoritma, veriyi özellik değerlerine göre bölen bir dizi eğer-o zaman-değilse karar kuralını öğrenir ve bir akış şemasına benzeyen bir yapı oluşturur. Her iç düğüm bir özellik üzerinde bir “test”i temsil eder, her dal testin sonucunu temsil eder ve her yaprak düğüm bir sınıf etiketi veya kararı temsil eder.\ndal, test, yaprak"
  },
  {
    "objectID": "ders5-TR.html#örnek",
    "href": "ders5-TR.html#örnek",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Örnek",
    "text": "Örnek\n\n\n\n\n\n\n\n\n\n\n\n\nÇalışılan saat\nÖnceki Puan\nUygulamaya Katıldı\nGeçti?\n\n\n\n\n3\n60\nHayır\n?\n\n\n4\n75\nHayır\n?\n\n\n7\n80\nEvet\n?"
  },
  {
    "objectID": "ders5-TR.html#karar-ağacı-oluşturma-için-temel-metrikler",
    "href": "ders5-TR.html#karar-ağacı-oluşturma-için-temel-metrikler",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Karar Ağacı Oluşturma için Temel Metrikler",
    "text": "Karar Ağacı Oluşturma için Temel Metrikler\nSorular: Hangi özellik ilk dal olur? Hangi değerde bir dal oluştururuz (5 saat, 70 puan, vb.)\n\n\nEntropi\nBilgi Kazancı\nGini Safsızlığı"
  },
  {
    "objectID": "ders5-TR.html#entropi",
    "href": "ders5-TR.html#entropi",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Entropi",
    "text": "Entropi\n\nEntropi, bir örnek kümesindeki safsızlık veya belirsizlik ölçüsüdür. Karar ağaçları bağlamında, bir veri kümesindeki sınıf etiketlerinin düzensizliğini ölçer.\n\nFormül: \\(H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\nBurada \\(S\\) veri kümesi, \\(c\\) sınıf sayısı ve \\(p_i\\), \\(i\\) sınıfına ait örneklerin oranıdır.\n\n0 (tamamen saf, tüm örnekler bir sınıfa ait) ile \\(\\log_2(c)\\) (tamamen saf olmayan, tüm sınıflara eşit dağılım) arasında değişir.\nBilgi kazancını hesaplamak için kullanılır."
  },
  {
    "objectID": "ders5-TR.html#bilgi-kazancı",
    "href": "ders5-TR.html#bilgi-kazancı",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Bilgi Kazancı",
    "text": "Bilgi Kazancı\n\nBilgi kazancı, veriyi belirli bir özelliğe göre bölerek elde edilen entropi azalmasını ölçer. Karar ağacının her düğümünde hangi özelliğin bölünmesi gerektiğini belirlemeye yardımcı olur.\n\nFormül: \\(IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\\)\nBurada \\(S\\) veri kümesi, \\(A\\) bölünme için düşünülen özellik, \\(Values(A)\\) \\(A\\) özelliğinin olası değerleri ve \\(S_v\\), \\(A\\) özelliğinin \\(v\\) değerine sahip olduğu \\(S\\)’nin alt kümesidir.\n\nDaha yüksek bilgi kazancı, sınıflandırma için daha faydalı bir özelliği gösterir.\nGenellikle her düğümde bölünme için en yüksek bilgi kazancına sahip özellik seçilir."
  },
  {
    "objectID": "ders5-TR.html#gini-safsızlığı",
    "href": "ders5-TR.html#gini-safsızlığı",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Gini Safsızlığı",
    "text": "Gini Safsızlığı\n\nGini safsızlığı, bir örnek kümesinin safsızlığını ölçmek için entropiye bir alternatiftir. Rastgele seçilen bir elemanın, alt kümedeki etiket dağılımına göre rastgele etiketlendirilirse yanlış sınıflandırılma olasılığını temsil eder.\n\nFormül: \\(Gini(S) = 1 - \\sum_{i=1}^{c} (p_i)^2\\)\nBurada \\(S\\) veri kümesi, \\(c\\) sınıf sayısı ve \\(p_i\\), \\(i\\) sınıfına ait örneklerin oranıdır.\n\n0 (tamamen saf) ile \\(1 - \\frac{1}{c}\\) (tamamen saf olmayan) arasında değişir.\nGenellikle CART (Sınıflandırma ve Regresyon Ağaçları) gibi algoritmalarda kullanılır.\n\nEntropi (bilgi kazancı ile) veya Gini safsızlığı kullanma seçimi genellikle karar ağacı algoritmasının belirli uygulamasına bağlıdır. Pratikte, genellikle benzer sonuçlar verirler."
  },
  {
    "objectID": "ders5-TR.html#karar-ağacı-oluşturma-algoritmaları",
    "href": "ders5-TR.html#karar-ağacı-oluşturma-algoritmaları",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Karar ağacı oluşturma algoritmaları",
    "text": "Karar ağacı oluşturma algoritmaları\n\nID3\nCART\n\nAlgoritmalar hakkında detaylar için lütfen bu bağlantıyı ziyaret edin"
  },
  {
    "objectID": "ders5-TR.html#r-ile-bir-örnek",
    "href": "ders5-TR.html#r-ile-bir-örnek",
    "title": "Sınıflandırma Yöntemleri",
    "section": "R ile bir örnek",
    "text": "R ile bir örnek\nhttps://www.dataspoof.info/post/decision-tree-classification-in-r/\nhttps://forum.posit.co/t/decision-tree-in-r/5561/5"
  },
  {
    "objectID": "ders5-TR.html#section-2",
    "href": "ders5-TR.html#section-2",
    "title": "Sınıflandırma Yöntemleri",
    "section": "",
    "text": "Karar Ağaçlarının Avantajları:\n\nYorumlanabilirlik: Uzman olmayanlar için bile anlaşılması ve açıklanması kolaydır. Karar verme süreci görsel olarak temsil edilebilir.\nAz veya hiç veri ön işleme gerektirmez: Normalizasyon veya ölçeklendirme ihtiyacı olmadan hem sayısal hem de kategorik verileri işleyebilir.\nHesaplama açısından verimli: Özellikle küçük ve orta ölçekli veri setleriyle eğitim ve tahmin yapmak genellikle hızlıdır.\n\nDezavantajları:\n\nAşırı öğrenme: Özellikle derin ağaçlarda aşırı öğrenmeye eğilimlidir, bu da yeni verilerde zayıf genellemeye yol açar.\nİstikrarsızlık: Verideki küçük değişiklikler tamamen farklı bir ağacın oluşturulmasına neden olabilir.\nYüksek boyutlu verilerle zorluk: Birçok özellikle hesaplama açısından pahalı hale gelebilir ve aşırı öğrenmeye eğilimli olabilir."
  },
  {
    "objectID": "ders5-TR.html#quiz-zamanı",
    "href": "ders5-TR.html#quiz-zamanı",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Quiz zamanı",
    "text": "Quiz zamanı"
  },
  {
    "objectID": "ders5-TR.html#rastgele-orman-random-forest",
    "href": "ders5-TR.html#rastgele-orman-random-forest",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Rastgele Orman (Random Forest)",
    "text": "Rastgele Orman (Random Forest)\nRastgele orman, Leo Breiman ve Adele Cutler tarafından tescillenen, birden çok karar ağacının çıktısını birleştirerek tek bir sonuca ulaşan yaygın olarak kullanılan bir makine öğrenimi algoritmasıdır.\nRastgele ormanlar, eğitim sırasında çok sayıda karar ağacı oluşturarak çalışan ve sınıflandırma, regresyon ve diğer görevler için kullanılan bir topluluk öğrenme yöntemidir. Sınıflandırma görevleri için rastgele ormanın çıktısı, çoğu ağaç tarafından seçilen sınıftır."
  },
  {
    "objectID": "ders5-TR.html#kaggle-örneği",
    "href": "ders5-TR.html#kaggle-örneği",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Kaggle örneği",
    "text": "Kaggle örneği\nLütfen şu adresi ziyaret edin: https://www.kaggle.com/code/lara311/diabetes-prediction-using-machine-learning"
  },
  {
    "objectID": "ders5-TR.html#destek-vektör-makineleri",
    "href": "ders5-TR.html#destek-vektör-makineleri",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Destek Vektör Makineleri",
    "text": "Destek Vektör Makineleri\nTemel Fikir\nFarklı türdeki nesneleri, örneğin elmaları ve portakalları, renk, şekil ve boyut gibi özelliklerine göre ayırmaya çalıştığınızı hayal edin. İki tür nesneyi mümkün olduğunca doğru bir şekilde ayıran bir çizgi (veya daha yüksek boyutlarda bir hiper düzlem) çizmek istiyorsunuz.\nDestek Vektör Makineleri = Support Vector Machines (SVM)"
  },
  {
    "objectID": "ders5-TR.html#svm-yöntemi",
    "href": "ders5-TR.html#svm-yöntemi",
    "title": "Sınıflandırma Yöntemleri",
    "section": "SVM Yöntemi",
    "text": "SVM Yöntemi\nDestek Vektör Makinesi, veriyi farklı sınıflara ayıran en iyi hiper düzlemi bulmayı amaçlayan bir tür denetimli öğrenme algoritmasıdır. İşte nasıl çalıştığı:\n\nVeri Hazırlama: İlgili özellikleri (öznitelikleri) ve etiketleri (örneğin, “elma” veya “portakal”) olan nesnelerin (örneğin, elmalar ve portakallar) bir veri setini toplayın.\nVerileri Çizme: Veri noktalarını bir özellik uzayında çizin, burada her eksen bir özelliği (örneğin, renk, şekil, boyut) temsil eder.\nHiperdüzlemi Bulma: Amaç, veri noktalarını farklı sınıflara ayıran bir hiperdüzlem bulmaktır. Hiperdüzlem, özellik uzayını iki bölgeye ayıran bir çizgi (2B’de) veya düzlem (3B’de) dir.\nMarjini Maksimize Etme: SVM algoritması, iki sınıf arasındaki marjini maksimize eden hiperdüzlemi bulmaya çalışır. Marj, hiperdüzlem ile hiperdüzlemin her iki tarafındaki en yakın veri noktaları (destek vektörleri olarak adlandırılır) arasındaki mesafedir.\nDestek Vektörleri: Destek vektörleri, hiperdüzleme en yakın olan ve konumu üzerinde en çok etkiye sahip olan veri noktalarıdır. Bunlar, hiperdüzlemi tanımlamaya yardımcı olan “destekler”dir."
  },
  {
    "objectID": "ders5-TR.html#destek-vektör-makineleri-1",
    "href": "ders5-TR.html#destek-vektör-makineleri-1",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Destek Vektör Makineleri",
    "text": "Destek Vektör Makineleri\nTemel Kavramlar\n\nHiperdüzlem: Verileri farklı sınıflara ayıran bir çizgi (2B’de) veya düzlem (3B’de).\nMarj: Hiperdüzlem ile hiperdüzlemin her iki tarafındaki en yakın veri noktaları (destek vektörleri) arasındaki mesafe.\nDestek Vektörleri: Hiperdüzleme en yakın olan ve konumu üzerinde en çok etkiye sahip olan veri noktaları."
  },
  {
    "objectID": "ders5-TR.html#svmler-neden-faydalıdır",
    "href": "ders5-TR.html#svmler-neden-faydalıdır",
    "title": "Sınıflandırma Yöntemleri",
    "section": "SVM’ler Neden Faydalıdır",
    "text": "SVM’ler Neden Faydalıdır\nSVM’ler güçlüdür çünkü:\n\nYüksek boyutlu verileri işleyebilirler\nGürültüye ve aykırı değerlere karşı dayanıklıdırlar\nHem sınıflandırma hem de regresyon görevleri için kullanılabilirler\nKarar sınırının net bir geometrik yorumunu sağlarlar"
  },
  {
    "objectID": "ders5-TR.html#section-3",
    "href": "ders5-TR.html#section-3",
    "title": "Sınıflandırma Yöntemleri",
    "section": "",
    "text": "H1 sınıfları ayırmaz. H2 ayırır, ancak sadece küçük bir marjla. H3 onları maksimum marjla ayırır. Kaynak"
  },
  {
    "objectID": "ders5-TR.html#section-4",
    "href": "ders5-TR.html#section-4",
    "title": "Sınıflandırma Yöntemleri",
    "section": "",
    "text": "İki sınıftan örneklerle eğitilmiş bir SVM için maksimum marjlı hiperdüzlem ve marjlar. Marj üzerindeki örneklere destek vektörleri denir. Kaynak"
  },
  {
    "objectID": "ders5-TR.html#section-5",
    "href": "ders5-TR.html#section-5",
    "title": "Sınıflandırma Yöntemleri",
    "section": "",
    "text": "SVM için çevrimiçi interaktif bir demo için lütfen SVM demo sitesini ziyaret edin."
  },
  {
    "objectID": "ders5-TR.html#lojistik-regresyon",
    "href": "ders5-TR.html#lojistik-regresyon",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Lojistik Regresyon",
    "text": "Lojistik Regresyon\nTemel Fikir\nLojistik regresyon, bir sonucun, olayın veya gözlemin olasılığını tahmin ederek ikili sınıflandırma görevlerini gerçekleştiren denetimli bir makine öğrenimi algoritmasıdır. Model, iki olası sonuçla sınırlı ikili bir sonuç sunar: evet/hayır, 0/1 veya doğru/yanlış."
  },
  {
    "objectID": "ders5-TR.html#lojistik-regresyon-yöntemi",
    "href": "ders5-TR.html#lojistik-regresyon-yöntemi",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Lojistik Regresyon Yöntemi",
    "text": "Lojistik Regresyon Yöntemi\nLojistik Regresyon, bir dizi girdi değişkenine (örneğin, puanlar) dayalı olarak bir olayın gerçekleşme olasılığını (örneğin, sınavı geçme) modelleyen bir tür denetimli öğrenme algoritmasıdır. İşte nasıl çalıştığı:\n\nVeri Hazırlama: Girdi değişkenleri (örneğin, puanlar) ve çıktı değişkenleri (örneğin, geçti/kaldı) için bir veri seti toplayın.\nLojistik Fonksiyon: Lojistik fonksiyon, aynı zamanda sigmoid fonksiyonu olarak da bilinen, olayın gerçekleşme olasılığını modellemek için kullanılır. Girdi değişkenlerini 0 ile 1 arasında bir olasılığa eşler.\nLog-Odds: Lojistik fonksiyon, olayın gerçekleşme olasılığının log-odds’una dayanır, bu da olayın gerçekleşme olasılığının olayın gerçekleşmeme olasılığına oranının logaritmasıdır.\nKatsayılar: Algoritma, her girdi değişkeni için katsayıları (ağırlıkları) öğrenir, bu da her değişkenin çıktıyı tahmin etmedeki önemini belirler.\nKarar Sınırı: Algoritma, katsayıları ve lojistik fonksiyonu kullanarak bir karar sınırı oluşturur, bu da girdi uzayını iki bölgeye ayırır: her sınıf için bir tane (örneğin, geçti ve kaldı).\nTahmin: Yeni bir girdi için, algoritma lojistik fonksiyonu ve öğrenilen katsayıları kullanarak olayın gerçekleşme olasılığını hesaplar. Eğer olasılık belirli bir eşiğin üzerindeyse (örneğin, 0.5), algoritma olayın gerçekleşeceğini tahmin eder (örneğin, öğrenci geçecek)."
  },
  {
    "objectID": "ders5-TR.html#temel-kavramlar",
    "href": "ders5-TR.html#temel-kavramlar",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Temel Kavramlar",
    "text": "Temel Kavramlar\n\nLojistik Fonksiyon: Girdi değişkenlerini 0 ile 1 arasında bir olasılığa eşleyen matematiksel bir fonksiyon.\nLog-Odds: Olayın gerçekleşme olasılığının olayın gerçekleşmeme olasılığına oranının logaritması.\nKatsayılar: Algoritmanın her girdi değişkeni için öğrendiği ağırlıklar, bunlar çıktıyı tahmin etmedeki önemlerini belirler.\nKarar Sınırı: Girdi uzayını iki bölgeye ayıran sınır, her sınıf için bir tane."
  },
  {
    "objectID": "ders5-TR.html#lojistik-regresyon-neden-faydalıdır",
    "href": "ders5-TR.html#lojistik-regresyon-neden-faydalıdır",
    "title": "Sınıflandırma Yöntemleri",
    "section": "Lojistik Regresyon Neden Faydalıdır",
    "text": "Lojistik Regresyon Neden Faydalıdır\nLojistik Regresyon popüler bir algoritmadır çünkü:\n\nUygulaması ve yorumlanması kolaydır\nBirden çok girdi değişkenini işleyebilir\nHer tahmin için bir olasılık tahmini sağlar\nTıp, finans ve pazarlama gibi birçok alanda yaygın olarak kullanılır"
  },
  {
    "objectID": "rf-regression-R.html",
    "href": "rf-regression-R.html",
    "title": "Random Forest Regression with R",
    "section": "",
    "text": "# Load required libraries\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n# Set seed for reproducibility\nset.seed(123)\n\ndata &lt;- mtcars\n\n\nSplit data into training and testing sets\n\ntrain_indices &lt;- sample(1:nrow(data), 0.7 * nrow(data))\ntrain_data &lt;- data[train_indices, ]\ntest_data &lt;- data[-train_indices, ]\n\n\n\nTrain Random Forest model and make predictions\n\nrf_model &lt;- randomForest(mpg ~ ., data = train_data, ntree = 500)\n\npredictions &lt;- predict(rf_model, newdata = test_data)\n\n\n\nCalculate RMSE\n\nrmse &lt;- sqrt(mean((test_data$mpg - predictions)^2))\ncat(\"Root Mean Square Error:\", rmse, \"\\n\")\n\nRoot Mean Square Error: 2.00452 \n\n\n\n\nPlot actual vs predicted values\n\nggplot(data.frame(actual = test_data$mpg, predicted = predictions), aes(x = actual, y = predicted)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Actual Values\", y = \"Predicted Values\", title = \"Random Forest Regression: Actual vs Predicted\")\n\n\n\n\n\n\n\n\n\n\nPrint feature importance\n\nimportance &lt;- importance(rf_model)\nprint(importance)\n\n     IncNodePurity\ncyl      152.64821\ndisp     224.33387\nhp       158.42647\ndrat      43.37372\nwt       178.44424\nqsec      34.05620\nvs        25.67666\nam        19.73485\ngear      10.54314\ncarb      21.62506\n\n\n\n\nPlot feature importance\n\nimportance_df &lt;- data.frame(feature = rownames(importance), importance = importance[, 1])\nggplot(importance_df, aes(x = reorder(feature, importance), y = importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(x = \"Features\", y = \"Importance\", title = \"Random Forest: Feature Importance\")\n\n\n\n\n\n\n\n\nalternative at https://hackernoon.com/random-forest-regression-in-r-code-and-interpretation"
  },
  {
    "objectID": "ders8-TR.html#içerik",
    "href": "ders8-TR.html#içerik",
    "title": "NLP - Doğal Dil İşleme",
    "section": "İçerik",
    "text": "İçerik\n\n\n\nNLP’nin tanımı ve neden ihtiyaç duyulduğu\nYaygın NLP Uygulamaları\n\nMetin özetleme\nDuygu analizi\nAnahtar kelime çıkarımı ve ilişkilendirme\nBelge sınıflandırma\nVarlık ismi tanıma\nMakine çevirisi\nSoru cevaplama sistemleri\nSohbet robotları\n\n\n\n\nNLP Yaklaşımları\n\nTerim tabanlı yaklaşımlar\nSözlük tabanlı yaklaşımlar\nDerlem (Corpus) tabanlı yaklaşımlar\nİstatistiksel yaklaşımlar\nAnlamsal yaklaşımlar\nDerin öğrenme yaklaşımları"
  },
  {
    "objectID": "ders8-TR.html#içerik-devam",
    "href": "ders8-TR.html#içerik-devam",
    "title": "NLP - Doğal Dil İşleme",
    "section": "İçerik (devam)",
    "text": "İçerik (devam)\n\nNLP’de Ön İşleme Adımları: Metin temizleme ve normalleştirme, Belirteç çıkarma (Tokenization), Durak kelime (Stop words) çıkarma, Kök bulma (Stemming) ve lemmatizasyon, Kelime türü (POS) etiketleme, Sözdizimi analizi ve ayrıştırma, Varlık ismi tanıma\nÖzellik Çıkarma ve Temsil: Torba içinde Kelimeler (BoW) modeli, Terim Frekansı-Ters Belge Frekansı (TF-IDF), N-gram modelleri, Kelime gömmeleri (Word2Vec, FastText), Bağlamsal gömmeler (örn. BERT, GPT)"
  },
  {
    "objectID": "ders8-TR.html#tanım",
    "href": "ders8-TR.html#tanım",
    "title": "NLP - Doğal Dil İşleme",
    "section": "Tanım",
    "text": "Tanım\nDoğal Dil İşleme (NLP), bilgisayarlar ve insan dili arasındaki etkileşime odaklanan bir yapay zeka dalıdır. Bilgisayarların insan dilini anlamlı bir şekilde anlamasını, yorumlamasını, üretmesini ve yanıtlamasını sağlamak için hesaplamalı dilbilim, makine öğrenimi ve derin öğrenme tekniklerini birleştirir. NLP, büyük miktarda doğal dil verisini işleyerek ve analiz ederek insan iletişimi ile bilgisayar anlayışı arasındaki boşluğu kapatmayı amaçlar. Bu alan, konuşma tanıma, metin analizi, dil çevirisi, duygu analizi ve metin üretimi gibi çeşitli görevleri kapsar ve makinelerin insan dillerinden anlam çıkarmasına ve hem anlamlı hem de bağlamsal olarak uygun bir şekilde yanıt vermesine olanak tanır."
  },
  {
    "objectID": "ders8-TR.html#neden-nlp",
    "href": "ders8-TR.html#neden-nlp",
    "title": "NLP - Doğal Dil İşleme",
    "section": "Neden NLP?",
    "text": "Neden NLP?\nDoğal Dil İşleme (NLP), metin verisinin geleneksel makine öğrenimi görevlerinde tipik olarak kullanılan yapılandırılmış sayısal ve kategorik verilerden temelde farklı olması nedeniyle çok önemlidir. İşte NLP’nin neden gerekli olduğuna dair nedenler:\n\nMetnin yapılandırılmamış doğası: Her sütunun belirli bir özelliği temsil ettiği tablo verilerinin aksine, metin doğası gereği yapılandırılmamıştır. Bir cümle veya belge önceden tanımlanmış özellikler veya kategorilerle gelmez. Bu yapılandırılmamış doğa, bilgisayarların metin verisini doğrudan işlemesini ve analiz etmesini zorlaştırır.\nYüksek boyutluluk: Metin verisi son derece yüksek boyutlu olabilir. Örneğin, bir kelime çantası modelinde, derlemdeki her benzersiz kelime bir özellik haline gelir. Bu, yapılandırılmış veri problemlerinde tipik olarak görülenin çok üzerinde binlerce hatta milyonlarca boyuta yol açabilir.\nAnlamsal karmaşıklık: Kelimeler ve ifadeler bağlama bağlı olarak birden fazla anlama sahip olabilir. Örneğin, “banka” bir finansal kurumu veya bir nehrin kenarını ifade edebilir. Geleneksel ML modelleri, NLP teknikleri olmadan bu anlamsal karmaşıklığı ele almak için donanımlı değildir."
  },
  {
    "objectID": "ders8-TR.html#neden-nlp-1",
    "href": "ders8-TR.html#neden-nlp-1",
    "title": "NLP - Doğal Dil İşleme",
    "section": "Neden NLP?",
    "text": "Neden NLP?\n\nSözdizimsel değişkenlik: Doğal dilde aynı anlam birçok farklı şekilde ifade edilebilir. Örneğin, “Kedi halının üstünde oturdu” ve “Halının üstünde kedi oturdu” aynı bilgiyi iletir ancak farklı kelime sıralamasına sahiptir. ML modellerinin bu varyasyonları anlaması için NLP tekniklerine ihtiyacı vardır.\nGürültüyle başa çıkma: Metin verisi genellikle yazım hataları, resmi olmayan dil veya ilgisiz bilgiler gibi “gürültü” içerir. NLP, bu veriyi temizlemek ve ön işlemden geçirmek için yöntemler sağlayarak analiz için uygun hale getirir.\nÖzellik çıkarma: Sayısal verilerde özellikler açık olduğu halde, metinsel özelliklerin çıkarılması gerekir. NLP, TF-IDF, kelime gömmeleri ve konu modellemesi gibi metni ML modellerinin çalışabileceği anlamlı sayısal temsillere dönüştürmek için teknikler sağlar.\nSıra bilgisini ele alma: Birçok ML modeli özellikler arasında bağımsızlık varsayar, ancak metinde kelimelerin sırası önemlidir. Tekrarlayan sinir ağları veya transformatörler gibi NLP teknikleri bu sıralı bilgiyi yakalamak için tasarlanmıştır."
  },
  {
    "objectID": "ders8-TR.html#neden-nlp-2",
    "href": "ders8-TR.html#neden-nlp-2",
    "title": "NLP - Doğal Dil İşleme",
    "section": "Neden NLP?",
    "text": "Neden NLP?\n\nÇok dilli zorluklar: Metin verisi birden fazla dilde gelebilir ve her birinin kendi kuralları ve nüansları vardır. NLP, dil algılama, çeviri ve diller arası analiz için araçlar sağlar.\nBağlamla başa çıkma: Metni anlamak genellikle doğrudan kelimelerin ötesinde bir bağlamı kavramayı gerektirir. NLP teknikleri bu daha geniş bağlamı yakalamaya ve kullanmaya yardımcı olur.\n\nÖzünde, NLP, insan dilinin zengin, karmaşık ve yapılandırılmamış doğasını bilgisayarların etkili bir şekilde işleyebileceği bir forma dönüştürmek için gerekli araç ve teknikleri sağladığı için gereklidir. İnsanların iletişim kurma şekli ile makinelerin bilgiyi işleme şekli arasındaki boşluğu kapatarak duygu analizinden makine çevirisine, sohbet robotlarına ve daha ötesine kadar geniş bir uygulama yelpazesini mümkün kılar."
  },
  {
    "objectID": "ders8-TR.html#neden-nlp-3",
    "href": "ders8-TR.html#neden-nlp-3",
    "title": "NLP - Doğal Dil İşleme",
    "section": "Neden NLP?",
    "text": "Neden NLP?"
  },
  {
    "objectID": "ders8-TR.html#section",
    "href": "ders8-TR.html#section",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "görsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#section-1",
    "href": "ders8-TR.html#section-1",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Ayrıca, ses kayıtları da Konuşmadan 2Metin işleminden sonra bir metin girişidir\ngörsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#section-2",
    "href": "ders8-TR.html#section-2",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Yaygın NLP Uygulamaları - Özetleme\n\nMetin özetleme, büyük miktardaki metni kısa ve tutarlı özetlere yoğunlaştıran bir NLP uygulamasıdır. İnsanlar için özetleme, bağlamı anlamayı, ana noktaları belirlemeyi ve fikirleri yeniden ifade etmeyi içerir - bunlar bizim için doğal gelen görevlerdir. Ancak bilgisayarlar için bu süreç oldukça karmaşıktır. Bir bilgisayar metnin yapısını analiz etmeli, anlamsal anlamları kavramalı, farklı cümlelerin veya kavramların göreceli önemini belirlemeliı ve ardından tutarlı bir özet üretmeli veya çıkarmalıdır. Bu, dil, bağlam ve ilgililik konusunda insan benzeri bir anlayışı taklit etmek için sofistike algoritmalar gerektirir. Zorluk, makinelere sadece kelimeleri işlemekle kalmayıp, daha geniş belge bağlamı içindeki ilişkilerini ve önemlerini anlamayı öğretmektedir.\n\ngörsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#section-3",
    "href": "ders8-TR.html#section-3",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Yaygın NLP Uygulamaları - Duygu Analizi\nDuygu analizi, bir metin parçasının arkasındaki duygusal tonu belirlemeyi amaçlayan bir NLP tekniğidir. İnsanlar genellikle bir ifadedeki duyguyu sezgisel olarak algılayabilirken, bilgisayarlar için bu görev şaşırtıcı derecede karmaşıktır. Bir bilgisayar, genel duygusal bağlamı kavramak için basit anahtar kelime eşleştirmesinin ötesine geçmelidir. Örneğin, “Bu film müthiş!” ifadesi, bağlama ve konuşmacıya bağlı olarak olumlu veya olumsuz olabilir.\nLütfen Twitter/X kullanarak ABD’deki mutluluğu takip eden https://hedonometer.org adresini ziyaret edin."
  },
  {
    "objectID": "ders8-TR.html#section-4",
    "href": "ders8-TR.html#section-4",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Yaygın NLP Uygulamaları - Varlık İsmi Tanıma\n\nMetindeki adlandırılmış varlıkları bulmayı ve bunları kişi adları, kuruluşlar, konumlar, tıbbi kodlar, zaman ifadeleri, miktarlar, parasal değerler, yüzdeler vb. önceden tanımlanmış kategorilere sınıflandırmayı amaçlayan bir NLP tekniğidir. İnsanlar metindeki bu varlıkları kolayca tanımlayıp kategorize edebilirken, bilgisayarlar için bu görev büyük zorluklar sunar.\nGösterim için lütfen Geeks4Geeks Varlık İsmi Tanıma adresini ziyaret edin."
  },
  {
    "objectID": "ders8-TR.html#nlp-yaklaşımları---terim-tabanlı",
    "href": "ders8-TR.html#nlp-yaklaşımları---terim-tabanlı",
    "title": "NLP - Doğal Dil İşleme",
    "section": "NLP Yaklaşımları - Terim tabanlı",
    "text": "NLP Yaklaşımları - Terim tabanlı\nNLP’deki terim tabanlı yaklaşımlar, metin verisini işlemek ve analiz etmek için temel yöntemlerden birini temsil eder. Bu yaklaşımlar, bireysel kelimeleri veya terimleri analizin temel birimleri olarak ele alır. Temel fikir, belgeleri terimler koleksiyonu olarak temsil etmektir, genellikle dilbilgisi ve kelime sırasını göz ardı ederek ancak çokluğu koruyarak. Yaygın bir uygulama, her belgenin terim frekansları vektörü olarak temsil edildiği “kelime çantası” modelidir. Basit olmasına rağmen, bu yöntem belge sınıflandırma veya bilgi alma gibi birçok görev için şaşırtıcı derecede etkili olabilir. Ancak, terim tabanlı yaklaşımlar eşanlamlılık (aynı anlamlı farklı kelimeler) ve çok anlamlılık (birden çok anlamı olan aynı kelime) ile zorluklarla karşılaşır. Ayrıca bağlamı ve kelimeler arasındaki ilişkileri yakalamakta zorlanırlar. Bu sınırlamalara rağmen, terim tabanlı yöntemler daha gelişmiş NLP teknikleri için bir temel oluşturur ve hesaplama verimliliği ve yorumlanabilirliği nedeniyle birçok uygulamada ilgili kalmaya devam eder."
  },
  {
    "objectID": "ders8-TR.html#section-5",
    "href": "ders8-TR.html#section-5",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "görsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#nlp-yaklaşımları---sözlük-tabanlı",
    "href": "ders8-TR.html#nlp-yaklaşımları---sözlük-tabanlı",
    "title": "NLP - Doğal Dil İşleme",
    "section": "NLP Yaklaşımları - Sözlük Tabanlı",
    "text": "NLP Yaklaşımları - Sözlük Tabanlı\nNLP’de sözlük tabanlı yaklaşımlar, metni işlemek ve analiz etmek için sözlükler, eşanlamlılar sözlüğü veya ontolojiler gibi önceden var olan sözcüksel kaynakları kullanır. Bu yöntemler, dili anlamak ve yorumlamak için insan tarafından oluşturulmuş bilgi tabanlarından yararlanır. Özünde, insanların bilinmeyen kelimeleri veya kavramları anlamak için sözlükleri nasıl kullandığını taklit etmeye çalışırlar. Örneğin, duygu analizinde sözlük tabanlı bir yaklaşım, ilişkili duyguları ile etiketlenmiş kelimelerin bir sözlüğünü kullanabilir."
  },
  {
    "objectID": "ders8-TR.html#section-6",
    "href": "ders8-TR.html#section-6",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "görsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#nlp-yaklaşımları---derlemcorpus-tabanlı",
    "href": "ders8-TR.html#nlp-yaklaşımları---derlemcorpus-tabanlı",
    "title": "NLP - Doğal Dil İşleme",
    "section": "NLP Yaklaşımları - Derlem/Corpus Tabanlı",
    "text": "NLP Yaklaşımları - Derlem/Corpus Tabanlı\nNLP’de derlem tabanlı yaklaşımlar, dilbilimsel bilgi ve kalıpları türetmek için derlem olarak bilinen büyük metin veri koleksiyonlarına dayanır. Önceden var olan bilgi tabanlarına bağlı olan sözlük tabanlı yöntemlerin aksine, derlem tabanlı yaklaşımlar gerçek dünya dil kullanımından öğrenir. Bu yöntemler, kelimelerin ve ifadelerin geniş miktardaki metin içinde bağlam içinde nasıl kullanıldığını analiz etmek için istatistiksel teknikler kullanır. Örneğin, Word2Vec veya GloVe gibi kelime gömmeleri, büyük derlemlerdeki eş-oluşum kalıplarına dayalı olarak kelimelerin vektör temsillerini öğrenmek için sinir ağlarını kullanır.\nDerlem tabanlı yöntemler, dil kullanımındaki nüansları yakalamada, belirsizliği ele almada ve farklı alanlara veya dillere uyum sağlamada öne çıkar. Dil modelleme, makine çevirisi ve metin üretimi gibi görevler için özellikle güçlüdürler."
  },
  {
    "objectID": "ders8-TR.html#nlp-yaklaşımları---anlamsal-semantic-yaklaşım",
    "href": "ders8-TR.html#nlp-yaklaşımları---anlamsal-semantic-yaklaşım",
    "title": "NLP - Doğal Dil İşleme",
    "section": "NLP Yaklaşımları - Anlamsal (Semantic) Yaklaşım",
    "text": "NLP Yaklaşımları - Anlamsal (Semantic) Yaklaşım\nNLP’de anlambilim yaklaşımları, dilin sadece yapısını veya istatistiksel kalıplarını değil, anlamını anlamaya odaklanır. Bu yöntemler, insan kavrayışını yansıtarak metindeki altta yatan kavramları ve ilişkileri yakalamayı ve temsil etmeyi amaçlar. Anlambilim yaklaşımları genellikle kavramlar arasındaki ilişkileri modellemek için bilgi grafikleri, ontolojiler veya anlamsal ağları kullanır. Örneğin, biyomedikal bir bağlamda, anlambilim yaklaşımı “aspirinin” bir tür “ağrı kesici” olduğunu ve “baş ağrılarını” “tedavi ettiğini” anlayabilir. Bu yöntemler karmaşık sorguları ele alabilir, yeni bilgiler çıkarabilir ve bağlama bağlı anlamları anlayabilir."
  },
  {
    "objectID": "ders8-TR.html#nlp-ön-işleme-adımları",
    "href": "ders8-TR.html#nlp-ön-işleme-adımları",
    "title": "NLP - Doğal Dil İşleme",
    "section": "NLP Ön İşleme Adımları",
    "text": "NLP Ön İşleme Adımları\nMetin temizleme ve normalleştirme:\nBu ilk adım, işlemeyi engelleyebilecek öğeleri kaldırarak veya standartlaştırarak ham metin verisini analize hazırlamayı içerir. Genellikle tutarlılığı sağlamak için metni küçük harfe dönüştürme, analizle ilgisi olmayan özel karakterleri veya noktalama işaretlerini kaldırma ve biçimlendirme sorunlarını ele alma gibi görevleri içerir. Örneğin, sosyal medya metninde, kısaltmaları genişletme (örn. “değil” yerine “değil”), yaygın yazım hatalarını düzeltme veya emojileri metin açıklamalarına dönüştürme işlemlerini içerebilir. Amaç, temel anlamını korurken analizi karmaşıklaştırabilecek gürültüyü ve varyasyonları azaltan standartlaştırılmış, “temiz” bir metin versiyonu oluşturmaktır."
  },
  {
    "objectID": "ders8-TR.html#section-7",
    "href": "ders8-TR.html#section-7",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Belirteçlere Ayırma (Tokenization)\nBelirteçlere ayırma, metni genellikle kelimeler veya alt kelimeler olarak adlandırılan daha küçük birimlere (belirteçlere) ayırma işlemidir. Basit görünse de, dile özgü zorluklar nedeniyle karmaşık olabilir. Örneğin, İngilizcede boşluk ve noktalama işaretlerinden ayırabiliriz, ancak bu yaklaşım kelimeler arasında boşluk kullanmayan Çince gibi diller için başarısız olur. Belirteçlere ayırma ayrıca kısaltmalar, kısa çizgi ile birleştirilmiş kelimeler veya çok kelimeli ifadelerin nasıl ele alınacağı konusunda kararlar içerir. Belirteçlere ayırma yöntemi seçimi, modelin çalışacağı temel anlam birimlerini tanımladığı için sonraki NLP görevlerini önemli ölçüde etkileyebilir."
  },
  {
    "objectID": "ders8-TR.html#section-8",
    "href": "ders8-TR.html#section-8",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Durak kelime çıkarma\nDurak kelimeler, genellikle kendi başlarına önemli bir anlam taşımadıkları için metin işlemeden filtrelenen yaygın kelimelerdir (örneğin “ve”, “veya”, “ama”, “ki” gibi). Bunları çıkarmak, verideki gürültüyü azaltmaya ve analizi daha anlamlı içerik kelimelerine odaklamaya yardımcı olabilir. Ancak, neyin durak kelime oluşturduğunun tanımı, belirli uygulamaya veya alana bağlı olarak değişebilir. Örneğin, duygu analizinde genellikle durak kelime olarak kabul edilen “değil” gibi kelimeler anlamı anlamak için çok önemli olabilir. Bu nedenle, durak kelime çıkarma verimliliği artırabilir ve gürültüyü azaltabilir, ancak belirli NLP görevinin dikkatli bir şekilde değerlendirilmesini gerektirir."
  },
  {
    "objectID": "ders8-TR.html#section-9",
    "href": "ders8-TR.html#section-9",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Kök bulma (Stemming) ve Gövde bulma (Lemmatization)\nBu teknikler, kelimeleri temel veya sözlük formuna indirgemeyi amaçlar, bu da farklı kelime formlarını tek bir öğe olarak ele almaya yardımcı olur. Kök bulma, kelimelerin sonlarını kesen kaba bir sezgisel süreçtir ve genellikle kelime olmayan sonuçlar verir (örn. “koşuyor” “koş” olur, ancak “evren” “evr” olur). Öte yandan, gövde bulma, bir kelimenin sözlük formunu döndürmek için kelime dağarcığı ve morfolojik analiz kullanır (örn. “koşuyor” “koşmak” olur, “am” “olmak” olur). Kök bulma daha hızlı ve basit olsa da, gövde bulma daha doğru sonuçlar sağlar. Aralarındaki seçim, işlem hızı ile dilbilimsel doğruluk arasında denge kurarak belirli NLP görevinin gereksinimlerine bağlıdır."
  },
  {
    "objectID": "ders8-TR.html#section-10",
    "href": "ders8-TR.html#section-10",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "görsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#section-11",
    "href": "ders8-TR.html#section-11",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "görsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#section-12",
    "href": "ders8-TR.html#section-12",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Konuşma bölümü (POS) etiketleme\nPOS (Point of Speech) etiketleme, bir metindeki her kelimeyi hem tanımına hem de bağlamına dayalı olarak uygun konuşma bölümü (isim, fiil, sıfat vb.) ile etiketlemeyi içerir. Bu süreç, cümlelerin dilbilgisel yapısını anlamak için çok önemlidir ve adlandırılmış varlık tanıma veya sözdizimsel ayrıştırma gibi görevlerde önemli ölçüde yardımcı olabilir. Modern POS etiketleyiciler genellikle büyük açıklamalı derlemler üzerinde eğitilmiş makine öğrenimi modellerini kullanır ve yüksek doğruluk elde edebilir. Ancak, belirsiz kelimelerle (örn. “kayıt” bir isim veya fiil olabilir) veya kelimelerin alışılmadık şekillerde kullanıldığı alan spesifik dili ele almada hala zorluklarla karşılaşabilirler."
  },
  {
    "objectID": "ders8-TR.html#section-13",
    "href": "ders8-TR.html#section-13",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "görsel kaynağı"
  },
  {
    "objectID": "ders8-TR.html#section-14",
    "href": "ders8-TR.html#section-14",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Konuşma Bölümü\nEtiket\n\n\n\n\nİsim (Tekil)\nNN\n\n\nİsim (Çoğul)\nNNS\n\n\nFiil\nVB\n\n\nBelirleyici\nDT\n\n\nSıfat\nJJ\n\n\nZarf\nRB"
  },
  {
    "objectID": "ders8-TR.html#section-15",
    "href": "ders8-TR.html#section-15",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Sözdizimi analizi ve ayrıştırma\nBu teknik, kelimelerin birbirleriyle nasıl ilişkili olduğunu anlamak için cümlelerin dilbilgisel yapısını analiz etmeyi içerir. Genellikle cümle yapısını temsil eden bir ayrıştırma ağacı veya bağımlılık grafiği üretir. İki ana yaklaşım vardır: cümleleri iç içe geçmiş bileşenlere (öbekler) bölen bileşen ayrıştırma ve bireysel kelimeler arasındaki dilbilgisel ilişkileri tanımlayan bağımlılık ayrıştırma. Ayrıştırma, makine çevirisi veya soru cevaplama gibi cümle yapısını anlamayı gerektiren görevler için çok önemlidir. Ancak, hesaplama açısından yoğun olabilir ve belirsiz cümleler veya standart olmayan dil kullanımı ile zorluklarla karşılaşabilir.\nLütfen şu bağlantıları ziyaret edin: bağlantı1"
  },
  {
    "objectID": "ders8-TR.html#nlpde-özellik-çıkarma-ve-temsil-teknikleri",
    "href": "ders8-TR.html#nlpde-özellik-çıkarma-ve-temsil-teknikleri",
    "title": "NLP - Doğal Dil İşleme",
    "section": "NLP’de Özellik Çıkarma ve Temsil Teknikleri",
    "text": "NLP’de Özellik Çıkarma ve Temsil Teknikleri"
  },
  {
    "objectID": "ders8-TR.html#section-16",
    "href": "ders8-TR.html#section-16",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Torba içinde Kelimeler (BoW) modeli:\nKelime Çantası modeli, metni sayısal özellikler olarak temsil etmek için basit ancak etkili bir yöntemdir. Derleme içindeki tüm benzersiz kelimelerden bir kelime dağarcığı oluşturur ve her belgeyi kelime frekanslarının bir vektörü olarak temsil eder. Kelimelerin sırası dikkate alınmaz, bu yüzden “çanta” adı verilir. BoW hesaplama açısından verimli ve anlaşılması kolay olsa da, kelime sırası ve bağlam hakkındaki tüm bilgileri kaybeder. Ayrıca, kelime dağarcığı dışındaki kelimelerle mücadele eder ve büyük kelime dağarcıkları için çok yüksek boyutlu, seyrek vektörlerle sonuçlanabilir. Bu sınırlamalara rağmen, BoW birçok metin sınıflandırma görevi için faydalı olmaya devam eder ve daha gelişmiş teknikler için bir temel oluşturur."
  },
  {
    "objectID": "ders8-TR.html#section-17",
    "href": "ders8-TR.html#section-17",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Terim Frekansı-Ters Belge Frekansı (TF-IDF):\nTF-IDF, bir koleksiyon veya derlem içindeki bir belgede bir kelimenin önemini değerlendirmek için kullanılan istatistiksel bir ölçüdür. İki metriği birleştirir: Terim Frekansı (bir kelimenin bir belgede ne sıklıkta göründüğü) ve Ters Belge Frekansı (kelimenin tüm belgeler arasında ne kadar nadir olduğu). Fikir, bir belgede sık sık geçen ancak genel derlemde nadir olan kelimelerin muhtemelen o belge için daha önemli olduğudur. TF-IDF, tüm belgelerde sık sık görünen ancak az anlam taşıyan yaygın kelimelerin etkisini azaltmaya yardımcı olur. Bu yöntem, basit kelime sayımlarından daha nüanslı bir temsil sağlayarak bilgi alma ve belge sıralama gibi görevler için değerli hale getirir.\nLütfen gösterim için bu bağlantıyı ziyaret edin."
  },
  {
    "objectID": "ders8-TR.html#section-18",
    "href": "ders8-TR.html#section-18",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "N-gram modelleri:\nN-gram modelleri, BoW (Bag of Words) fikrinin bir uzantısı olarak, tek tek kelimeleri izole olarak ele almak yerine, N bitişik kelimeden oluşan dizileri dikkate alır. Örneğin, bir bigram (2-gram) modeli bitişik kelime çiftlerini ele alırken, bir trigram (3-gram) modeli üç kelimelik dizileri inceler. N-gramlar, dil modelleme veya makine çevirisi gibi görevler için çok önemli olabilen bazı yerel bağlam ve kelime sırası bilgilerini yakalar. Ancak, N arttıkça, modelin karmaşıklığı ve veri seyrekliği sorunları da artar. N-gram modelleri, BoW’un basitliği ile kelime ilişkilerini yakalama ihtiyacı arasında bir denge kurar ve bu da onları çeşitli NLP uygulamalarında kullanışlı kılar.\nGörsel açıklama için lütfen bu bağlantıları ziyaret edin: bağlantı1 bağlantı2 bağlantı3"
  },
  {
    "objectID": "ders8-TR.html#section-19",
    "href": "ders8-TR.html#section-19",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Kelime gömmeleri/embeddings (Word2Vec, GloVe, FastText):\nKelime gömmeleri, anlamsal olarak benzer kelimelerin yakın noktalara eşlendiği sürekli bir vektör uzayında kelimelerin yoğun vektör temsilleridir. Seyrek, yüksek boyutlu vektörler üreten BoW veya TF-IDF’nin aksine, kelime gömmeleri genellikle birkaç yüz boyuta sahiptir. Word2Vec gibi modeller, bu temsilleri büyük metinlerde kelime eş-oluşumlarını analiz ederek öğrenir ve anlamsal ilişkileri yakalar (örn. “kral” - “adam” + “kadın” ≈ “kraliçe”). FastText bu fikri alt kelime birimlerine genişleterek, sözlük dışı kelimeler için de gömme oluşturulmasına olanak tanır. Bu yoğun temsiller, çeşitli NLP görevlerinde performansı önemli ölçüde artırır ve ilginç kelime cebiri işlemlerine izin verir. Ancak, her kelimenin bağlamdan bağımsız olarak tek, sabit bir temsili olduğu için çok anlamlılık (bir kelimenin birden fazla anlamı olması) konusunda zorlanırlar.\nGörsel açıklamalar için lütfen bu bağlantıları ziyaret edin: bağlantı1 bağlantı2. İnteraktif görselleştirme için lütfen Embedding Projector sayfasını ziyaret edin.\nFastText için görsel açıklama"
  },
  {
    "objectID": "ders8-TR.html#section-20",
    "href": "ders8-TR.html#section-20",
    "title": "NLP - Doğal Dil İşleme",
    "section": "",
    "text": "Bağlamsal gömmeler (örn. BERT, GPT):\nBağlamsal gömmeler, bir kelimenin cümle içindeki bağlamına dayalı olarak farklı temsiller üreterek geleneksel kelime gömmelerine göre önemli bir ilerlemeyi temsil eder. BERT (Bidirectional Encoder Representations from Transformers) ve GPT (Generative Pre-trained Transformer) gibi modeller, kelimeler arasındaki karmaşık, çift yönlü ilişkileri yakalamak için derin öğrenme mimarilerini kullanarak tüm cümleleri veya belgeleri aynı anda işler. Bu modeller, büyük miktarda metin verisi üzerinde önceden eğitilir ve belirli görevler için ince ayar yapılabilir. Bağlamsal gömmeler, nüanslı anlamları yakalamada, çok anlamlılığı ele almada ve metindeki uzun mesafeli bağımlılıkları anlamada mükemmeldir. Çeşitli NLP görevlerinde yeni en son teknoloji standartları belirlemişlerdir. Ancak, hesaplama açısından yoğundurlar ve eğitim ve dağıtım için önemli kaynaklar gerektirirler, bu da sınırlı hesaplama gücüne sahip veya gerçek zamanlı gereksinimler olan uygulamalar için zorluklar oluşturur."
  },
  {
    "objectID": "rf-regression-R-tr.html",
    "href": "rf-regression-R-tr.html",
    "title": "R ile Rastgele Orman Regresyonu",
    "section": "",
    "text": "# Gerekli kütüphaneleri yükle\nlibrary(randomForest)\nlibrary(ggplot2)\n\nset.seed(123)\n\ndata &lt;- mtcars\n\n\nVeriyi eğitim ve test setlerine ayır (Train/test split)\n\ntrain_indices &lt;- sample(1:nrow(data), 0.7 * nrow(data))\ntrain_data &lt;- data[train_indices, ]\ntest_data &lt;- data[-train_indices, ]\n\n\n\nRastgele Orman modeli eğitimi ve tahmin yapılması\n\nrf_model &lt;- randomForest(mpg ~ ., data = train_data, ntree = 500)\n\npredictions &lt;- predict(rf_model, newdata = test_data)\n\n\n\nRMSE hesapla\nRMSE: Root mean square error (Kök Ortalama Kare Hatası)\n\nrmse &lt;- sqrt(mean((test_data$mpg - predictions)^2))\ncat(\"Kök Ortalama Kare Hatası:\", rmse, \"\\n\")\n\nKök Ortalama Kare Hatası: 2.00452 \n\n\n\n\nGerçek ve tahmin edilen değerler\n\nggplot(data.frame(actual = test_data$mpg, predicted = predictions), aes(x = actual, y = predicted)) +\n  geom_point(alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Gerçek Değerler\", y = \"Tahmin Edilen Değerler\", title = \"Rastgele Orman Regresyonu: Gerçek ve Tahmin Edilen\")\n\n\n\n\n\n\n\n\n\n\nÖzellik önemi\n\nimportance &lt;- importance(rf_model)\nprint(importance)\n\n     IncNodePurity\ncyl      152.64821\ndisp     224.33387\nhp       158.42647\ndrat      43.37372\nwt       178.44424\nqsec      34.05620\nvs        25.67666\nam        19.73485\ngear      10.54314\ncarb      21.62506\n\n\n\n\nÖzellik önemi - figür\n\nimportance_df &lt;- data.frame(feature = rownames(importance), importance = importance[, 1])\nggplot(importance_df, aes(x = reorder(feature, importance), y = importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(x = \"Özellikler\", y = \"Önem\", title = \"Rastgele Orman: Özellik Önemi\")\n\n\n\n\n\n\n\n\nAlternatif kaynak: http s://hackernoon.com/random-forest-regression-in-r-code-and-interpretation"
  },
  {
    "objectID": "quiz-clustering-tr.html",
    "href": "quiz-clustering-tr.html",
    "title": "Kümeleme Analizi Quiz - Türkçe Çeviri",
    "section": "",
    "text": "Kümeleme Analizi Quiz - Türkçe Çeviri\nSoru 1. Bir veri setini kümelemenin amacı nedir?\nA* Verileri birbirine yakın olan gruplara bölmek B Veri setinden en iyi verileri seçmek C Her bir verinin en yakın komşularını belirlemek D Verilerin sınıfını tahmin etmek\nSoru 2. Bir veri setini bölmek için kullanılan küme sayısı olan k’nin seçimi,\nA kişisel bir seçimdir ve kamuya açık tartışılmamalıdır B* verileri neden kümelediğinize bağlıdır C her zaman bilgisayar sisteminizin işleyebileceği kadar büyük olmalıdır D en fazla 10’dur\nSoru 3. K-means algoritması hakkında aşağıdaki ifadelerden hangisi doğrudur?\nA* K-means algoritması aykırı değerlere karşı hassastır. B Farklı başlangıç noktaları için, K-means algoritması kesinlikle aynı kümeleme sonuçlarını verecektir. C K’yı önceden tanımlamaya gerek yoktur D K-means algoritması küresel olmayan şekilli veriler için idealdir\nSoru 4. K-medyan algoritmasını düşünürsek, eğer (0, 3), (2, 1) ve (-2, 2) noktaları şu anda ilk kümeye atanan tek noktalar ise, bu küme için yeni merkez noktası nedir?\nA* (0,2) B (2,1) C (2,0) D (1,2)\nSoru 5. Iris veri seti, Iris setosa ve versicolor hakkında bilgi içermektedir. Bu iki nesne arasındaki Öklid mesafesi nedir?\nA* 2.8 B 4.6 C 22.6 D -3.6\nSoru 6. Hiyerarşik Kümelemenin iki türü nedir?\nA* Yukarıdan Aşağıya Kümeleme (Bölücü) B* Aşağıdan Yukarıya Kümeleme (Birleştirici) C Dendrogram D K-means\nSoru 7. En yaygın kullanılan benzerlik ölçüsü _____ veya onun karesidir.\nA* Öklid mesafesi B şehir blok mesafesi C Çebyşev mesafesi D Manhattan mesafesi\nSoru 8. Aşağıdakilerden hangisi K-means kümelemesi için gereklidir?\nA tanımlanmış bir mesafe metriği B küme sayısı C küme merkezleri için ilk tahmin D* tüm cevaplar doğrudur\nSoru 9. Yukarıdaki şekilde, y ekseninde y=2 için yatay bir çizgi çizerseniz, oluşacak küme sayısı ne olur?\nA* 2 B 3 C 4 D 5\nSoru 10. Aşağıdaki görevlerden hangisi için kümeleme uygun bir yaklaşım olabilir? A Bir süpermarkette çok sayıda ürünün satış verilerini kullanarak, bu ürünlerin gelecekteki satışlarını tahmin etmek. B* Kullanıcılarınız hakkında bir veritabanındaki bilgileri kullanarak, onları otomatik olarak farklı pazar segmentlerine gruplamak. C* Bir web sitesindeki kullanıcı kullanım modellerinden farklı kullanıcı gruplarını belirlemek. D Geçmiş hava durumu kayıtlarını kullanarak, yarının hava durumunun güneşli mi yoksa yağmurlu mu olacağını tahmin etmek."
  },
  {
    "objectID": "Hierarchical_Clustering_in_Data_Mining-TR.html",
    "href": "Hierarchical_Clustering_in_Data_Mining-TR.html",
    "title": "Veri Madenciliğinde Hiyerarşik Kümeleme",
    "section": "",
    "text": "İçerik “Hierarchical Clustering in Data Mining” adlı makaleden alınmıştır.\nHiyerarşik kümeleme yöntemi, verileri bir küme ağacı halinde gruplandırarak çalışır. Hiyerarşik kümeleme, her veri noktasını ayrı bir küme olarak ele alarak başlar. Daha sonra aşağıdaki adımları tekrar tekrar uygular:\n\nBirbirine en yakın 2 kümeyi belirle, ve\nEn benzer 2 kümeyi birleştir. Tüm kümeler birleşene kadar bu adımları tekrarlamak gerekir.\n\nHiyerarşik Kümelemede amaç, iç içe geçmiş kümelerin hiyerarşik bir serisini üretmektir. Dendrogram adı verilen bir diyagram (Dendrogram, birleşmelerin veya bölünmelerin sırasını gösteren ağaç benzeri bir diyagramdır) bu hiyerarşiyi grafiksel olarak temsil eder ve faktörlerin birleştirilme sırasını (aşağıdan yukarıya bakış) veya kümelerin parçalanma sırasını (yukarıdan aşağıya bakış) açıklayan ters çevrilmiş bir ağaçtır.\n\n\nHiyerarşik kümeleme, veri madenciliğinde bir veri setindeki kümelerin hiyerarşik bir temsilini oluşturan bir küme analizi yöntemidir. Yöntem, her veri noktasını ayrı bir küme olarak ele alarak başlar ve daha sonra bir durdurma kriteri karşılanana kadar en yakın kümeleri yinelemeli olarak birleştirir. Hiyerarşik kümelemenin sonucu, kümeler arasındaki hiyerarşik ilişkileri gösteren dendrogram adı verilen ağaç benzeri bir yapıdır.\n\n\n\nKonveks olmayan kümeleri ve farklı boyut ve yoğunluktaki kümeleri işleyebilme yeteneği.\nEksik verileri ve gürültülü verileri işleyebilme yeteneği.\nVerilerin hiyerarşik yapısını ortaya çıkarma yeteneği, bu da kümeler arasındaki ilişkileri anlamak için faydalı olabilir.\n\n\n\n\n\nKümeleme sürecini durdurmak ve nihai küme sayısını belirlemek için bir kritere ihtiyaç duyulması.\nYöntemin hesaplama maliyeti ve bellek gereksinimleri, özellikle büyük veri setleri için yüksek olabilir.\nSonuçlar, başlangıç koşullarına, bağlantı kriterine ve kullanılan uzaklık metriğine duyarlı olabilir.\n\nÖzetle, Hiyerarşik kümeleme, benzer veri noktalarını kümelerin hiyerarşik bir yapısını oluşturarak gruplayan bir veri madenciliği yöntemidir. - Bu yöntem farklı türdeki verileri işleyebilir ve kümeler arasındaki ilişkileri ortaya çıkarabilir. Ancak, yüksek hesaplama maliyetine sahip olabilir ve sonuçlar bazı koşullara duyarlı olabilir.\n\n\n\nTemel olarak, iki tür hiyerarşik kümeleme vardır:\n\nAglomeratif Kümeleme\nBölücü Kümeleme\n\n\n\n\nBaşlangıçta her veri noktasını bireysel bir Küme olarak düşünün ve her adımda en yakın küme çiftlerini birleştirin. (Bu bir aşağıdan yukarıya yöntemdir). İlk başta, her veri seti bireysel bir varlık veya küme olarak kabul edilir. Her iterasyonda, kümeler tek bir küme oluşana kadar farklı kümelerle birleşir.\n\n\n\nBir kümenin diğer tüm kümelerle benzerliğini hesaplayın (yakınlık matrisini hesaplayın)\nHer veri noktasını bireysel bir küme olarak düşünün\nBirbirine çok benzer veya yakın olan kümeleri birleştirin.\nHer küme için yakınlık matrisini yeniden hesaplayın\nSadece tek bir küme kalana kadar 3. ve 4. Adımları tekrarlayın.\n\nBu algoritmanın grafiksel gösterimini bir dendrogram kullanarak görelim.\n\nNot: Bu sadece gerçek algoritmanın nasıl çalıştığının bir gösterimidir, aşağıda hiçbir hesaplama yapılmamıştır, tüm kümeler arasındaki yakınlık varsayılmıştır.\n\nDiyelim ki A, B, C, D, E ve F adında altı veri noktamız var.\n\n\n\nAgglomerative Hierarchical clustering\n\n\n\nAdım-1: Her harfi tek bir küme olarak düşünün ve bir kümenin diğer tüm kümelerden uzaklığını hesaplayın.\nAdım-2: İkinci adımda benzer kümeler tek bir küme oluşturmak üzere birleştirilir. Diyelim ki küme (B) ve küme (C) birbirine çok benziyor, bu nedenle ikinci adımda onları birleştiriyoruz, benzer şekilde küme (D) ve (E) de birleşiyor ve sonunda [(A), (BC), (DE), (F)] kümelerini elde ediyoruz.\nAdım-3: Algoritmaya göre yakınlığı yeniden hesaplarız ve en yakın iki kümeyi ([(DE), (F)]) birleştirerek yeni kümeler oluştururuz: [(A), (BC), (DEF)]\nAdım-4: Aynı işlemi tekrarlayarak; DEF ve BC kümeleri benzer ve yeni bir küme oluşturmak için birleştirilir. Artık [(A), (BCDEF)] kümeleri kaldı.\nAdım-5: Son olarak, kalan iki küme tek bir küme [(ABCDEF)] oluşturmak üzere birleştirilir.\n\n\n\n\n\nBölücü Hiyerarşik kümelemenin, Aglomeratif Hiyerarşik kümelemenin tam tersi olduğunu söyleyebiliriz. Bölücü Hiyerarşik kümelemede, tüm veri noktalarını tek bir küme olarak ele alırız ve her iterasyonda benzer olmayan veri noktalarını kümelerden ayırırız. Sonunda, N küme ile kalırız.\n\n\n\nDivisive Hierarchical clustering"
  },
  {
    "objectID": "Hierarchical_Clustering_in_Data_Mining-TR.html#hiyerarşik-kümeleme-nedir",
    "href": "Hierarchical_Clustering_in_Data_Mining-TR.html#hiyerarşik-kümeleme-nedir",
    "title": "Veri Madenciliğinde Hiyerarşik Kümeleme",
    "section": "",
    "text": "Hiyerarşik kümeleme, veri madenciliğinde bir veri setindeki kümelerin hiyerarşik bir temsilini oluşturan bir küme analizi yöntemidir. Yöntem, her veri noktasını ayrı bir küme olarak ele alarak başlar ve daha sonra bir durdurma kriteri karşılanana kadar en yakın kümeleri yinelemeli olarak birleştirir. Hiyerarşik kümelemenin sonucu, kümeler arasındaki hiyerarşik ilişkileri gösteren dendrogram adı verilen ağaç benzeri bir yapıdır.\n\n\n\nKonveks olmayan kümeleri ve farklı boyut ve yoğunluktaki kümeleri işleyebilme yeteneği.\nEksik verileri ve gürültülü verileri işleyebilme yeteneği.\nVerilerin hiyerarşik yapısını ortaya çıkarma yeteneği, bu da kümeler arasındaki ilişkileri anlamak için faydalı olabilir.\n\n\n\n\n\nKümeleme sürecini durdurmak ve nihai küme sayısını belirlemek için bir kritere ihtiyaç duyulması.\nYöntemin hesaplama maliyeti ve bellek gereksinimleri, özellikle büyük veri setleri için yüksek olabilir.\nSonuçlar, başlangıç koşullarına, bağlantı kriterine ve kullanılan uzaklık metriğine duyarlı olabilir.\n\nÖzetle, Hiyerarşik kümeleme, benzer veri noktalarını kümelerin hiyerarşik bir yapısını oluşturarak gruplayan bir veri madenciliği yöntemidir. - Bu yöntem farklı türdeki verileri işleyebilir ve kümeler arasındaki ilişkileri ortaya çıkarabilir. Ancak, yüksek hesaplama maliyetine sahip olabilir ve sonuçlar bazı koşullara duyarlı olabilir.\n\n\n\nTemel olarak, iki tür hiyerarşik kümeleme vardır:\n\nAglomeratif Kümeleme\nBölücü Kümeleme\n\n\n\n\nBaşlangıçta her veri noktasını bireysel bir Küme olarak düşünün ve her adımda en yakın küme çiftlerini birleştirin. (Bu bir aşağıdan yukarıya yöntemdir). İlk başta, her veri seti bireysel bir varlık veya küme olarak kabul edilir. Her iterasyonda, kümeler tek bir küme oluşana kadar farklı kümelerle birleşir.\n\n\n\nBir kümenin diğer tüm kümelerle benzerliğini hesaplayın (yakınlık matrisini hesaplayın)\nHer veri noktasını bireysel bir küme olarak düşünün\nBirbirine çok benzer veya yakın olan kümeleri birleştirin.\nHer küme için yakınlık matrisini yeniden hesaplayın\nSadece tek bir küme kalana kadar 3. ve 4. Adımları tekrarlayın.\n\nBu algoritmanın grafiksel gösterimini bir dendrogram kullanarak görelim.\n\nNot: Bu sadece gerçek algoritmanın nasıl çalıştığının bir gösterimidir, aşağıda hiçbir hesaplama yapılmamıştır, tüm kümeler arasındaki yakınlık varsayılmıştır.\n\nDiyelim ki A, B, C, D, E ve F adında altı veri noktamız var.\n\n\n\nAgglomerative Hierarchical clustering\n\n\n\nAdım-1: Her harfi tek bir küme olarak düşünün ve bir kümenin diğer tüm kümelerden uzaklığını hesaplayın.\nAdım-2: İkinci adımda benzer kümeler tek bir küme oluşturmak üzere birleştirilir. Diyelim ki küme (B) ve küme (C) birbirine çok benziyor, bu nedenle ikinci adımda onları birleştiriyoruz, benzer şekilde küme (D) ve (E) de birleşiyor ve sonunda [(A), (BC), (DE), (F)] kümelerini elde ediyoruz.\nAdım-3: Algoritmaya göre yakınlığı yeniden hesaplarız ve en yakın iki kümeyi ([(DE), (F)]) birleştirerek yeni kümeler oluştururuz: [(A), (BC), (DEF)]\nAdım-4: Aynı işlemi tekrarlayarak; DEF ve BC kümeleri benzer ve yeni bir küme oluşturmak için birleştirilir. Artık [(A), (BCDEF)] kümeleri kaldı.\nAdım-5: Son olarak, kalan iki küme tek bir küme [(ABCDEF)] oluşturmak üzere birleştirilir.\n\n\n\n\n\nBölücü Hiyerarşik kümelemenin, Aglomeratif Hiyerarşik kümelemenin tam tersi olduğunu söyleyebiliriz. Bölücü Hiyerarşik kümelemede, tüm veri noktalarını tek bir küme olarak ele alırız ve her iterasyonda benzer olmayan veri noktalarını kümelerden ayırırız. Sonunda, N küme ile kalırız.\n\n\n\nDivisive Hierarchical clustering"
  },
  {
    "objectID": "ders7.html#contents",
    "href": "ders7.html#contents",
    "title": "Clustering Methods",
    "section": "Contents",
    "text": "Contents\n\n\n\nIntroduction to Clustering\n\nDefinition and purpose, Unsupervised learning concept, Real-world applications in scientific fields\n\nTypes of Clustering Algorithms\n\nPartitioning methods (e.g., K-means)\nHierarchical methods\nDensity-based methods (e.g., DBSCAN)\n\nK-means Clustering\n\nAlgorithm explanation\nChoosing the number of clusters (elbow method)\nAdvantages and limitations\n\n\n\n\nHierarchical Clustering\n\nAgglomerative vs. divisive approaches\nDendrogram interpretation\nLinkage methods (single, complete, average)\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN)\n\nCore concepts: core points, border points, noise\nAlgorithm explanation\nAdvantages\n\nEvaluation Metrics for Clustering\n\nSilhouette score"
  },
  {
    "objectID": "ders7.html#definition",
    "href": "ders7.html#definition",
    "title": "Clustering Methods",
    "section": "Definition",
    "text": "Definition\nClustering is a technique in machine learning and data analysis that involves grouping a set of objects or data points in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups. It’s a fundamental task in many data analysis and pattern recognition problems."
  },
  {
    "objectID": "ders7.html#purpose",
    "href": "ders7.html#purpose",
    "title": "Clustering Methods",
    "section": "Purpose",
    "text": "Purpose\nThe purpose of clustering is multifaceted:\nPattern Discovery: Clustering helps in identifying inherent patterns or structures within data that might not be immediately apparent.\nData Summarization: It can be used to compress large datasets by representing many data points with fewer cluster centroids.\nAnomaly Detection: By identifying groups of similar data points, clustering can help in spotting outliers or anomalies that don’t fit into any cluster.\nSegmentation: In fields like marketing, clustering is used to segment customers into groups with similar behaviors or characteristics.\nPreprocessing for Other Algorithms: Clustering can be used as a preprocessing step for other algorithms, reducing the complexity of the data.\nNatural Classification: In scientific fields, clustering can reveal natural groupings in data, such as in biology for grouping similar species or in astronomy for categorizing types of stars."
  },
  {
    "objectID": "ders7.html#clustering-methods",
    "href": "ders7.html#clustering-methods",
    "title": "Clustering Methods",
    "section": "Clustering Methods",
    "text": "Clustering Methods\nPartitioning methods: Divide data into non-overlapping subsets, with each data point belonging to exactly one cluster, often optimizing a specific criterion.\nHierarchical Methods: Create a tree-like structure of clusters, either by merging smaller clusters into larger ones or dividing larger clusters into smaller ones.\nDensity-based Methods: Form clusters in areas of high data point density, separated by regions of low density, allowing for discovery of arbitrarily shaped clusters."
  },
  {
    "objectID": "ders7.html#applications",
    "href": "ders7.html#applications",
    "title": "Clustering Methods",
    "section": "Applications",
    "text": "Applications\n\nBiology\n\n\nGene Expression Analysis: Grouping genes with similar expression patterns across different conditions or time points.\nSpecies Classification: Clustering organisms based on genetic or morphological features.\nProtein Structure Analysis: Identifying structural motifs in proteins.\n\n\nChemistry\n\n\nMolecular Dynamics: Analyzing conformations of molecules during simulations.\nSpectroscopy: Clustering similar spectra for compound identification.\nDrug Discovery: Grouping chemical compounds with similar properties or effects."
  },
  {
    "objectID": "ders7.html#section",
    "href": "ders7.html#section",
    "title": "Clustering Methods",
    "section": "",
    "text": "Physics\n\n\nParticle Physics: Classifying particle collision events in high-energy physics experiments.\nAstrophysics: Clustering stars or galaxies based on their properties (e.g., luminosity, temperature).\nMaterials Science: Analyzing crystal structures and defects in materials."
  },
  {
    "objectID": "ders7.html#partitioning",
    "href": "ders7.html#partitioning",
    "title": "Clustering Methods",
    "section": "Partitioning",
    "text": "Partitioning\n\nImage source"
  },
  {
    "objectID": "ders7.html#k-means-clustering",
    "href": "ders7.html#k-means-clustering",
    "title": "Clustering Methods",
    "section": "K-means clustering",
    "text": "K-means clustering\nK-means clustering is an unsupervised machine learning algorithm used to group similar data points into clusters. It works by iteratively assigning data points to k predefined clusters based on their similarity to the cluster centers (centroids). The algorithm starts by randomly initializing k centroids in the feature space. It then repeatedly performs two steps:\n\nassign each data point to the nearest centroid, and\nrecalculate the centroids as the mean of all points in each cluster.\n\nThis process continues until the centroids stabilize or a maximum number of iterations is reached. K-means aims to minimize the within-cluster sum of squares, resulting in compact and distinct clusters."
  },
  {
    "objectID": "ders7.html#k-means-clustering---interactive-demos",
    "href": "ders7.html#k-means-clustering---interactive-demos",
    "title": "Clustering Methods",
    "section": "K-means clustering - interactive demos",
    "text": "K-means clustering - interactive demos\nPlease visit the following sites:\n\nTwo-dimensional visualization of k-means clustering algorithm\nK-means Clustering - Interactive Demonstration\nVisualizing K-Means Clustering"
  },
  {
    "objectID": "ders7.html#k-means-clustering-with-r",
    "href": "ders7.html#k-means-clustering-with-r",
    "title": "Clustering Methods",
    "section": "K-means clustering with R",
    "text": "K-means clustering with R\nPlease visit K-means with R page for the code."
  },
  {
    "objectID": "ders7.html#advantages-of-k-means-clustering",
    "href": "ders7.html#advantages-of-k-means-clustering",
    "title": "Clustering Methods",
    "section": "Advantages of K-means clustering",
    "text": "Advantages of K-means clustering\n\nSimplicity: Easy to understand and implement.\nEfficiency: Generally fast and computationally efficient, especially for large datasets.\nScalability: Works well with high-dimensional data.\nFlexibility: Can be adapted to different types of data with appropriate distance metrics.\nInterpretability: Resulting clusters are often easy to interpret."
  },
  {
    "objectID": "ders7.html#disadvantages-of-k-means-clustering",
    "href": "ders7.html#disadvantages-of-k-means-clustering",
    "title": "Clustering Methods",
    "section": "Disadvantages of K-means clustering",
    "text": "Disadvantages of K-means clustering\n\nSensitivity to initial centroids: Results can vary based on the initial random placement of centroids.\nRequires predefined k: The number of clusters must be specified beforehand, which isn’t always known.\nAssumes spherical clusters: Performs poorly with non-globular cluster shapes.\nSensitivity to outliers: Outliers can significantly impact centroid calculation and cluster assignment.\nStruggles with varying cluster sizes: Tends to create clusters of similar spatial extent.\nLocal optima: May converge to a local minimum rather than the global optimum.\nInability to handle categorical data directly: Requires preprocessing for non-numerical data."
  },
  {
    "objectID": "ders7.html#hierarchical-clustering",
    "href": "ders7.html#hierarchical-clustering",
    "title": "Clustering Methods",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nHierarchical clustering is an unsupervised machine learning technique used to group similar data points into clusters. Unlike other clustering methods that require a predefined number of clusters, hierarchical clustering creates a tree-like structure of clusters called a dendrogram. This method works by either starting with each data point as its own cluster and iteratively merging the closest clusters (agglomerative approach), or by starting with all data points in one cluster and recursively dividing it (divisive approach). The process continues until a stopping criterion is met, such as a desired number of clusters or a threshold distance between clusters.\nHierarchical clustering is particularly useful for exploring the inherent structure of data at different levels of granularity and for datasets where the number of clusters is not known in advance."
  },
  {
    "objectID": "ders7.html#how-it-works",
    "href": "ders7.html#how-it-works",
    "title": "Clustering Methods",
    "section": "How it works",
    "text": "How it works\nHierarchical clustering begins by calculating the distances between all pairs of data points using a chosen distance metric (e.g., Euclidean distance). In the agglomerative approach, which is more common, each data point starts as its own cluster. The algorithm then iteratively merges the two closest clusters based on a linkage criterion (e.g., single linkage, complete linkage, or average linkage). This process repeats, forming larger clusters until all data points are in a single cluster or until the desired number of clusters is reached. The result is a hierarchical structure that can be visualized as a dendrogram, allowing users to choose the level of clustering that best suits their needs.\nPlease visit Hierarchical Clustering in Data Mining and An Introduction to Hierarchical Clustering"
  },
  {
    "objectID": "ders7.html#section-1",
    "href": "ders7.html#section-1",
    "title": "Clustering Methods",
    "section": "",
    "text": "Please visit following sites for demos\n\nClustering Demo at yFiles"
  },
  {
    "objectID": "ders7.html#advantages-of-hierarchical-clustering",
    "href": "ders7.html#advantages-of-hierarchical-clustering",
    "title": "Clustering Methods",
    "section": "Advantages of hierarchical clustering",
    "text": "Advantages of hierarchical clustering\n\nNo predefined number of clusters required: Unlike k-means, hierarchical clustering doesn’t require specifying the number of clusters in advance.\nHierarchical representation: Provides a dendrogram, offering insights into the data structure at different levels.\nFlexibility in cluster granularity: Users can choose the level of clustering that best suits their needs by cutting the dendrogram at different levels.\nApplicability to various data types: Can be applied to any data type for which a similarity or distance measure can be defined.\nInterpretability: The dendrogram provides a visually intuitive representation of the clustering process.\nHandles different cluster shapes: Can identify clusters of various shapes and sizes, not just spherical ones."
  },
  {
    "objectID": "ders7.html#disadvantages",
    "href": "ders7.html#disadvantages",
    "title": "Clustering Methods",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nComputational complexity: O(n^2) space complexity and O(n^3) time complexity in many implementations, making it less suitable for very large datasets.\nSensitivity to noise and outliers: Outliers can significantly affect the shape of the dendrogram.\nDifficulty in handling high-dimensional data: Performance and interpretability may decrease as the number of dimensions increases.\nLack of backtracking: Once a merge or split is made, it can’t be undone, which may lead to suboptimal results.\nMemory intensive: Storing the distance matrix for large datasets can be memory-intensive.\nChoice of distance metric and linkage method: Results can vary significantly based on the chosen distance metric and linkage method, requiring careful consideration.\nScalability issues: Not as scalable as some other clustering methods for very large datasets."
  },
  {
    "objectID": "ders7.html#dbscan-clustering",
    "href": "ders7.html#dbscan-clustering",
    "title": "Clustering Methods",
    "section": "DBSCAN clustering",
    "text": "DBSCAN clustering\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm that groups together points that are closely packed in space, marking points that lie alone in low-density regions as outliers.\nIt works by selecting a random unvisited point and exploring its neighborhood within a specified radius (ε). If this neighborhood contains a minimum number of points (MinPts), a cluster is formed. The algorithm then iteratively expands the cluster by exploring the neighborhoods of the newly added points. If a point doesn’t have enough neighbors to form a cluster, it’s labeled as noise. This process continues until all points have been visited, resulting in a set of dense clusters of varying shapes and sizes, along with identified noise points."
  },
  {
    "objectID": "ders7.html#interactive-demos",
    "href": "ders7.html#interactive-demos",
    "title": "Clustering Methods",
    "section": "Interactive demos",
    "text": "Interactive demos\nPlease visit DBSCAN— A visualized and detailed introduction page about core concepts\nPlease visit Visualizing DBSCAN Clustering page for interactive demo for DBSCAN algoritm"
  },
  {
    "objectID": "ders7.html#dbscan-algoritm",
    "href": "ders7.html#dbscan-algoritm",
    "title": "Clustering Methods",
    "section": "DBSCAN Algoritm",
    "text": "DBSCAN Algoritm"
  },
  {
    "objectID": "ders7.html#advantages-of-dbscan",
    "href": "ders7.html#advantages-of-dbscan",
    "title": "Clustering Methods",
    "section": "Advantages of DBSCAN",
    "text": "Advantages of DBSCAN\n\nShape flexibility: Can find arbitrarily shaped clusters, not just circular ones.\nOutlier detection: Automatically identifies noise points and outliers.\nNo pre-defined number of clusters: Unlike k-means, doesn’t require specifying the number of clusters beforehand.\nDensity-based: Effective at separating clusters of different densities.\nRobustness: Less sensitive to outliers compared to centroid-based algorithms."
  },
  {
    "objectID": "ders7.html#disadvantages-1",
    "href": "ders7.html#disadvantages-1",
    "title": "Clustering Methods",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nParameter sensitivity: Results can vary significantly based on the choice of ε and MinPts parameters.\nStruggle with varying densities: May have difficulty when clusters have widely different densities.\nHigh-dimensional data: Performance and effectiveness can decrease in high-dimensional spaces due to the “curse of dimensionality”.\nMemory requirements: Can be memory-intensive for large datasets.\nScalability: Not as efficient for very large datasets compared to some other algorithms.\nConnectivity-based: May struggle with datasets where clusters are close but separated by sparse areas."
  },
  {
    "objectID": "ders7.html#evaluating-clustering",
    "href": "ders7.html#evaluating-clustering",
    "title": "Clustering Methods",
    "section": "Evaluating clustering",
    "text": "Evaluating clustering\nDefinition: The silhouette score ranges from -1 to 1, where:\n\nA score close to 1 indicates that the data point is well-matched to its own cluster and poorly-matched to neighboring clusters.\nA score around 0 suggests that the data point is on or very close to the decision boundary between two neighboring clusters.\nA negative score indicates that the data point might have been assigned to the wrong cluster.\n\nCalculation: For each data point i, the silhouette score s(i) is calculated as:\ns(i) = (b(i) - a(i)) / max(a(i), b(i))\nWhere: a(i) is the average distance between i and all other points in the same clusteri, (i) is the average distance between i and all points in the nearest cluster that i is not a part of\nInterpretation:\n\nHigher silhouette scores indicate better-defined clusters.\nThe average silhouette score across all points can be used to compare different clustering configurations."
  },
  {
    "objectID": "ders7.html#classification-vs.-clustering",
    "href": "ders7.html#classification-vs.-clustering",
    "title": "Clustering Methods",
    "section": "Classification vs. Clustering",
    "text": "Classification vs. Clustering\nplease compare these two concepts"
  },
  {
    "objectID": "ders2.html#veri-analizi-veri-bilimi-makine-öğrenmesi",
    "href": "ders2.html#veri-analizi-veri-bilimi-makine-öğrenmesi",
    "title": "Veri tipleri ve işleme",
    "section": "Veri analizi, veri bilimi, makine öğrenmesi",
    "text": "Veri analizi, veri bilimi, makine öğrenmesi"
  },
  {
    "objectID": "ders2.html#section",
    "href": "ders2.html#section",
    "title": "Veri tipleri ve işleme",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "ders2.html#veri-tipleri",
    "href": "ders2.html#veri-tipleri",
    "title": "Veri tipleri ve işleme",
    "section": "Veri tipleri",
    "text": "Veri tipleri\n\nrecord data\ntransaction data\ndata matrix\n\nsparse data matrix\n\ngraph-based data"
  },
  {
    "objectID": "ders2.html#sıralı-veri-ordered-data",
    "href": "ders2.html#sıralı-veri-ordered-data",
    "title": "Veri tipleri ve işleme",
    "section": "Sıralı veri / Ordered data",
    "text": "Sıralı veri / Ordered data\n\n\nSequential / Temporal data\nSequence data\nTime series data\nSpatial data"
  },
  {
    "objectID": "ders2.html#section-3",
    "href": "ders2.html#section-3",
    "title": "Veri tipleri ve işleme",
    "section": "",
    "text": "Figure 2.4"
  },
  {
    "objectID": "ders2.html#veri-kalitesi",
    "href": "ders2.html#veri-kalitesi",
    "title": "Veri tipleri ve işleme",
    "section": "Veri kalitesi",
    "text": "Veri kalitesi\n Hatırlanacağı üzere, veri biliminde veri temizliği aşaması toplam sürenin %80’ini oluşturmaktadır.\n\nÖlçüm hataları / measurement error : ölçüm sırasında sayısal hata yapılması veya dahil edilmemesi gereken verinin dahil edilmesi\nGürültü / Noise and artifacts: ölçüm hatalarındaki rastgele olan bileşendir. Sinyal işleme yöntemleri ile gürültü kaldırılabilir ama bu sırada gerçek veri kaybı da gerçekleşebilir."
  },
  {
    "objectID": "ders2.html#veri-kalitesi---2",
    "href": "ders2.html#veri-kalitesi---2",
    "title": "Veri tipleri ve işleme",
    "section": "Veri kalitesi - 2",
    "text": "Veri kalitesi - 2\n\nUç değer / Outlier\nKayıp veya eksik ölçüm / Missing value\n\nÇıkarma / Eliminate\nTahmin etme, doldurma / Estimate\nGöz ardı etme / Ignore\n\nUyumsuz ölçümler / Inconsistent values : örn. Ölçüm yapılan tekniğin değişmesi veya farklı tekniklerle toplanan değerlerin uyuşmazlığı\nTekrar eden veri / Duplicate data : aynı kişi bir veritabanında birden çok kez bulunabilir ama ismi aynı iki kişi tekrar sayılmaz"
  },
  {
    "objectID": "ders2.html#veri-işleme",
    "href": "ders2.html#veri-işleme",
    "title": "Veri tipleri ve işleme",
    "section": "Veri işleme",
    "text": "Veri işleme\n\n\nAggregation\nSampling\nDimensionality reduction\nFeature subset selection\nFeature creation\nDiscretization and binarization\nVariable transformation"
  },
  {
    "objectID": "ders2.html#aggregation",
    "href": "ders2.html#aggregation",
    "title": "Veri tipleri ve işleme",
    "section": "Aggregation",
    "text": "Aggregation\n\nGroup by - sum, mean\nHistogram"
  },
  {
    "objectID": "ders2.html#aggregation-histogram",
    "href": "ders2.html#aggregation-histogram",
    "title": "Veri tipleri ve işleme",
    "section": "Aggregation / Histogram",
    "text": "Aggregation / Histogram"
  },
  {
    "objectID": "ders2.html#sampling-örnekleme",
    "href": "ders2.html#sampling-örnekleme",
    "title": "Veri tipleri ve işleme",
    "section": "Sampling / Örnekleme",
    "text": "Sampling / Örnekleme\n\nSimple random sampling\nStratified sampling"
  },
  {
    "objectID": "ders2.html#dimensionality-reduction",
    "href": "ders2.html#dimensionality-reduction",
    "title": "Veri tipleri ve işleme",
    "section": "Dimensionality reduction",
    "text": "Dimensionality reduction\n\n“The Curse of Dimensionality”: özellik/attribute sayısı arttıkça algoritmaların veya hesapların daha zor hale gelmesi. “Sparse” veri durumunda artık hesaplamaların (uzaklık, benzerlik) doğruluğunu yitirmesi\n\n\nLinear algebra techniques\n\nPCA: Principal Components Analysis\nSVD: Singular Value Decomposition"
  },
  {
    "objectID": "ders2.html#others",
    "href": "ders2.html#others",
    "title": "Veri tipleri ve işleme",
    "section": "Others",
    "text": "Others\n\nFeature Subset Selection\n\nremove Redundant features or Irrelevant features\nFeature Weighting\n\nFeature Creation\n\nFeature Extraction: e.g Photo -&gt; edges, important areas, color scheme\nMapping the Data to a New Space : e.g Fourier Transformation\n\nDiscretization and Binarization\n\nCategory -&gt; Binary values\nContinous data -&gt; bins, categories, intervals\n\nVariable Transformation"
  },
  {
    "objectID": "ders2.html#fourier-transformation",
    "href": "ders2.html#fourier-transformation",
    "title": "Veri tipleri ve işleme",
    "section": "Fourier Transformation",
    "text": "Fourier Transformation"
  },
  {
    "objectID": "ders2.html#category-to-binary",
    "href": "ders2.html#category-to-binary",
    "title": "Veri tipleri ve işleme",
    "section": "Category to binary",
    "text": "Category to binary"
  },
  {
    "objectID": "ders2.html#variable-transformation",
    "href": "ders2.html#variable-transformation",
    "title": "Veri tipleri ve işleme",
    "section": "Variable Transformation",
    "text": "Variable Transformation\n\n\nSimple functions: \\(x^k\\) , log(x), \\(e^x\\) , sqrt(x), \\(1/x\\), sin(x), or |x|\nNormalization or Standardization\n\nfor each value subtract mean and divide by standard deviation\ne.g. age and income data, find similarity between individuals\nmean = 0, sd = 1"
  },
  {
    "objectID": "id3-decision-tree.html",
    "href": "id3-decision-tree.html",
    "title": "Decision Tree Algoritm",
    "section": "",
    "text": "Taken from https://medium.datadriveninvestor.com/decision-tree-algorithm-with-hands-on-example-e6c2afb40d38\nEntropy\nIn machine learning, entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information.\n\nInformation Gain\nInformation gain can be defined as the amount of information gained about a random variable or signal from observing another random variable.It can be considered as the difference between the entropy of parent node and weighted average entropy of child nodes.\n\nGini Impurity\nGini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n\nGini impurity is lower bounded by 0, with 0 occurring if the data set contains only one class.\n\nThere are many algorithms there to build a decision tree. They are\n\nCART (Classification and Regression Trees) — This makes use of Gini impurity as the metric.\nID3 (Iterative Dichotomiser 3) — This uses entropy and information gain as metric.\n\nIn this article, I will go through ID3. Once you got it it is easy to implement the same using CART.\n\nClassification using the ID3 algorithm\nConsider whether a dataset based on which we will determine whether to play football or not.\n\nHere There are for independent variables to determine the dependent variable. The independent variables are Outlook, Temperature, Humidity, and Wind. The dependent variable is whether to play football or not.\nAs the first step, we have to find the parent node for our decision tree. For that follow the steps:\n*Find the entropy of the class variable.*\nE(S) = -[(9/14)log(9/14) + (5/14)log(5/14)] = 0.94\nnote: Here typically we will take log to base 2.Here total there are 14 yes/no. Out of which 9 yes and 5 no.Based on it we calculated probability above.\nFrom the above data for outlook we can arrive at the following table easily\n\n*Now we have to calculate average weighted entropy*. ie, we have found the total of weights of each feature multiplied by probabilities.\nE(S, outlook) = (5/14)E(3,2) + (4/14)E(4,0) + (5/14)*E(2,3) = (5/14)(-(3/5)log(3/5)-(2/5)log(2/5))+ (4/14)(0) + (5/14)((2/5)log(2/5)-(3/5)log(3/5)) = 0.693\n*The next step is to find the information gain*. It is the difference between parent entropy and average weighted entropy we found above.\nIG(S, outlook) = 0.94 - 0.693 = 0.247\nSimilarly find Information gain for Temperature, Humidity, and Windy.\nIG(S, Temperature) = 0.940 - 0.911 = 0.029\nIG(S, Humidity) = 0.940 - 0.788 = 0.152\nIG(S, Windy) = 0.940 - 0.8932 = 0.048\n*Now select the feature having the largest entropy gain*. Here it is Outlook. So it forms the first node(root node) of our decision tree.\nNow our data look as follows\n\n\n\nSince overcast contains only examples of class ‘Yes’ we can set it as yes. That means If outlook is overcast football will be played. Now our decision tree looks as follows.\n\nThe next step is to find the next node in our decision tree. Now we will find one under sunny. We have to determine which of the following Temperature, Humidity or Wind has higher information gain.\n\nCalculate parent entropy E(sunny)\nE(sunny) = (-(3/5)log(3/5)-(2/5)log(2/5)) = 0.971.\nNow Calculate the information gain of Temperature. IG(sunny, Temperature)\n\nE(sunny, Temperature) = (2/5)E(0,2) + (2/5)E(1,1) + (1/5)*E(1,0)=2/5=0.4\nNow calculate information gain.\nIG(sunny, Temperature) = 0.971–0.4 =0.571\nSimilarly we get\nIG(sunny, Humidity) = 0.971\nIG(sunny, Windy) = 0.020\nHere IG(sunny, Humidity) is the largest value. So Humidity is the node that comes under sunny.\n\nFor humidity from the above table, we can say that play will occur if humidity is normal and will not occur if it is high. Similarly, find the nodes under rainy.\n*Note: A branch with entropy more than 0 needs further splitting.*\nFinally, our decision tree will look as below:\n\n\n\nClassification using CART algorithm\nClassification using CART is similar to it. But instead of entropy, we use Gini impurity.\nSo as the first step we will find the root node of our decision tree. For that Calculate the Gini index of the class variable\nGini(S) = 1 - [(9/14)² + (5/14)²] = 0.4591\nAs the next step, we will calculate the Gini gain. For that first, we will find the average weighted Gini impurity of Outlook, Temperature, Humidity, and Windy.\nFirst, consider case of Outlook\n\nGini(S, outlook) = (5/14)gini(3,2) + (4/14)gini(4,0)+ (5/14)gini(2,3) = (5/14)(1 - (3/5)² - (2/5)²) + (4/14)*0 + (5/14)(1 - (2/5)² - (3/5)²)= 0.171+0+0.171 = 0.342\nGini gain (S, outlook) = 0.459 - 0.342 = 0.117\nGini gain(S, Temperature) = 0.459 - 0.4405 = 0.0185\nGini gain(S, Humidity) = 0.459 - 0.3674 = 0.0916\nGini gain(S, windy) = 0.459 - 0.4286 = 0.0304\nChoose one that has a higher Gini gain. Gini gain is higher for outlook. So we can choose it as our root node."
  },
  {
    "objectID": "ders1-TR.html",
    "href": "ders1-TR.html",
    "title": "GPT modelleri",
    "section": "",
    "text": "GPT1, GPT2 ve GPT3, ChatGPT’den birkaç yıl önce piyasaya sürüldü ve cümleleri tamamlama yeteneğine sahipti. Daha fazla bilgi için “ChatGPT Nasıl Çalışır?” sitesini ziyaret edin. ChatGPT’nin piyasaya sürülmesinden sonra, büyük dil modelleri “konuşma yeteneği” kazandı.\nLütfen OpenAI Playground tamamlama sitesini ziyaret edin ve model olarak “gpt-3.5-turbo”yu seçin. Tamamlanmamış bir cümle yazın ve ardından Gönder düğmesine basın.\n\nModelin cümlenizi tamamladığını göreceksiniz. Sağ menüdeki “Olasılıkları göster” seçeneğini açarsanız, GPT’nin nasıl çalıştığına dair bir fikir edinebilirsiniz.\n\nCümleyi tamamlayıp kelimelerin üzerine geldiğinizde, modelin belirli olasılıklarla seçebileceği birçok kelime olduğunu göreceksiniz. Bu örnekte, “my” kelimesi Today is ifadesinden sonra 4. en olası kelimedir ve “birthday” kelimesi Today is my ifadesinden sonra en olası kelimedir.\n\n\nGPT modellerinin temel ayarlarından biri sıcaklık değeridir. Sıcaklığı azaltırsanız, model en olası kelimeleri seçecektir. Sıcaklık yüksekse, model düşük olasılıklı kelimeleri seçecektir, bu da yaratıcı ve çeşitli çıktılar sağlayacaktır.\nSıcaklığı sıfıra düşürürseniz, çıktı her zaman aynı olacaktır."
  },
  {
    "objectID": "ders1-TR.html#chatgpt-bizim-için-kod-yazıyor",
    "href": "ders1-TR.html#chatgpt-bizim-için-kod-yazıyor",
    "title": "GPT modelleri",
    "section": "ChatGPT bizim için kod yazıyor",
    "text": "ChatGPT bizim için kod yazıyor\nHadi ChatGPT’den Python’da hızlı bir asal sayı fonksiyonu isteyelim\n\n\nAz önce ne olduğunu gördünüz mü! Bize özel hazırlanmış bir kod sağladı ve kodun parçalarını da açıkladı. Kodun dersimiz sırasında öğrendiğimiz püf noktaları içerdiğini fark ettiniz mi: * erken durma: bir koşul karşılanırsa, hemen True veya False döndür * N’ye kadar kontrol etmek yerine, \\(\\sqrt{N}\\)’e kadar olan sayıları kontrol etmeliyiz (Python’da n  0.5**)\nKonuşmaya devam edebilirsiniz. Hadi memoization isteyelim.\n\n\nHadi kodu alalım ve burada test edelim:\ndef is_prime(n, memo={}):\n    if n in memo:\n        return memo[n]\n    if n in (2, 3):\n        result = True\n    elif n == 1 or n % 2 == 0:\n        result = False\n    else:\n        result = True\n        for i in range(3, int(n ** 0.5) + 1, 2):\n            if n % i == 0:\n                result = False\n                break\n    memo[n] = result\n    return result\n# is_prime(100000000003)\n\n\n\n\n\n\nUyarı\n\n\n\nBu notu yazdığım sırada (Ocak 2023) chatGPT’nin tam olarak doğru olmayan sonuçlar verdiği biliniyordu!Bu nedenle, chatGPT’den aldığınız kodu veya bilgiyi kontrol etmeden veya doğrulamadan olduğu gibi kullanmamaya dikkat ediniz."
  },
  {
    "objectID": "ders1-TR.html#chatgpt-kodu-düzeltebilir-veya-değiştirebilir",
    "href": "ders1-TR.html#chatgpt-kodu-düzeltebilir-veya-değiştirebilir",
    "title": "GPT modelleri",
    "section": "ChatGPT kodu düzeltebilir veya değiştirebilir",
    "text": "ChatGPT kodu düzeltebilir veya değiştirebilir\nElinizde bir kod parçası var ve istediğiniz gibi çalışmıyor mu? O zaman ChatGPT’den kodu düzeltmesini isteyebilirsiniz.\nHatalı bir kod verelim ve chatGPT’den hata ayıklamasını isteyelim:\n\n\nDemek ki ChatGPT kodu düzeltebiliyor! Bu arada, o örneği hazırlamak için ChatGPT’yi kullandığımı biliyor muydunuz? ;)"
  },
  {
    "objectID": "ders1-TR.html#bu-programlamanın-sonu-mu",
    "href": "ders1-TR.html#bu-programlamanın-sonu-mu",
    "title": "GPT modelleri",
    "section": "Bu programlamanın sonu mu?",
    "text": "Bu programlamanın sonu mu?\n“Bir dönem boyunca Python öğrendik ama şimdi ChatGPT var, hiç kod öğrenmeye gerek yok!” diye düşünebilirsiniz. Benzer endişeleri araştıran çevrimiçi içerikler bulabilirsiniz. Ancak daha makul yaklaşım, “AI araçları geliştiricilerin hızını ve verimliliğini artıracak ve ayrıca yeni dillere giriş engellerini azaltacak” şeklindedir.\nAşağıdaki bölüm Github Copilot’un sayfasından alınmıştır:"
  },
  {
    "objectID": "ders1-TR.html#chatgptyi-gösterdiğiniz-için-teşekkürler-artık-ödevlerimi-kolayca-hazırlayabilirim",
    "href": "ders1-TR.html#chatgptyi-gösterdiğiniz-için-teşekkürler-artık-ödevlerimi-kolayca-hazırlayabilirim",
    "title": "GPT modelleri",
    "section": "ChatGPT’yi gösterdiğiniz için teşekkürler, artık ödevlerimi kolayca hazırlayabilirim!",
    "text": "ChatGPT’yi gösterdiğiniz için teşekkürler, artık ödevlerimi kolayca hazırlayabilirim!\nChatGPT’nin yeteneklerini görmek, yanlış kullanım hakkında bazı ipuçları verebilir. Öncelikle, bir ödev sizin bir kavramı öğrenmeniz içindir (gitar çalma benzetmesini hatırlayın?) bu nedenle AI destekli intihal yapmak hala bir intihaldir ve sizin açınızdan bir kayıptır. (lütfen intihalin olumsuz etkileri için 00-academic-integrity.ipynb dosyasını da kontrol edin)\nİkincisi, OpenAI ve diğerleri ChatGPT çıktısına kolayca fark edilebilecek filigranlar eklemek için araçlar ve yöntemler üzerinde çalışıyor.\nBu yüzden lütfen bu araçtan ilham alın, öğrenmenizi geliştirmek için kullanın ama intihal için kullanmayın."
  },
  {
    "objectID": "ders1-TR.html#kapalı-ve-açık-modeller",
    "href": "ders1-TR.html#kapalı-ve-açık-modeller",
    "title": "GPT modelleri",
    "section": "Kapalı ve Açık modeller",
    "text": "Kapalı ve Açık modeller\nŞu anda, her biri eğitilmek için yüz milyonlarca dolar harcanan ve büyük şirketler tarafından geliştirilen birkaç kapalı kaynaklı büyük dil modeli var: * GPT3.5 (ChatGPT) ve GPT4 OpenAI tarafından * PaLM ve Bard Google tarafından * Claude Anthropics tarafından"
  },
  {
    "objectID": "ders1-TR.html#claude-3.5-sonnet-by-antropic",
    "href": "ders1-TR.html#claude-3.5-sonnet-by-antropic",
    "title": "GPT modelleri",
    "section": "Claude 3.5 Sonnet by Antropic",
    "text": "Claude 3.5 Sonnet by Antropic\nClaude 3.5, Claude modelinin en son sürümüdür. Antropic, Claude’u üç boyutta piyasaya sürer: Haiku (en küçük), Sonnet (orta) ve Opus (en büyük). Şu anda, Claude 3.5 Sonnest en iyi LLM’dir. Genellikle Claude yaratıcı yazma konusunda iyidir, ancak son sürümüyle kodlamada da mükemmelleşmiştir. Ayrıca, Claude sağ panelde kodun önizlemesine (web tabanlı) izin verir.\nAşağıda, Claude 3.5 Sonnet’e “Lütfen HTML, CSS ve JS ile yılan oyunu yaz” komutu verildiğinde oluşturulan bir örnek yer alıyor. Oyunu gerçekten oynayabilirsiniz!\n\nClaude ayrıca belgeleri (örneğin PDF) yüklemeye ve belge hakkında sorular sormaya izin verir.\nClaude’u (bazı sınırlamalarla) ücretsiz bir hesap kaydettikten sonra https://claude.ai/chat adresinde kullanabilirsiniz."
  },
  {
    "objectID": "ders1-TR.html#gemini-by-google",
    "href": "ders1-TR.html#gemini-by-google",
    "title": "GPT modelleri",
    "section": "Gemini by Google",
    "text": "Gemini by Google\nGemini aslında çevrimiçi arama kapasitesine sahip bir LLM’dir. Ayrıca, (Google Drive aracılığıyla) belgeler indirebilir veya sorular sormak için YouTube videolarına yönlendirebilirsiniz. Son olarak, Gemini 1 milyon belirteçlik en uzun bağlam penceresine sahiptir (bir kerede yaklaşık 10 kitap sığacaktır)."
  },
  {
    "objectID": "ders1-TR.html#modelleri-lokal-olarak-çalıştırma",
    "href": "ders1-TR.html#modelleri-lokal-olarak-çalıştırma",
    "title": "GPT modelleri",
    "section": "Modelleri lokal olarak çalıştırma",
    "text": "Modelleri lokal olarak çalıştırma\nGeliştiriciler model ağırlıklarını paylaştığı için, modelleri indirip yerel olarak çalıştırmak mümkün. Çeşitli boyutlarda modeller var. 7B (7 milyar) modeller yaklaşık 4Gb bellek gerektiriyor, bu yüzden onları dizüstü bilgisayarınızda çalıştırabilirsiniz.\nModelleri, Python için PyTorch ve bazı diğer kütüphaneleri kurarak ve ardından biraz Python kodu yazarak çalıştırabilirsiniz. Ya da Ollama’yı kurabilir ve ardından onunla uyumlu herhangi bir modeli çalıştırabilirsiniz.\nBir modeli yerel olarak çalıştırmanın avantajları: 1. Daha Az Sansür 2. Daha İyi Veri Gizliliği 3. Çevrimdışı Kullanım 4. Maliyet Tasarrufu 5. Daha İyi Özelleştirme\nBir modeli yerel olarak çalıştırmanın dezavantajları: 1. Kaynak Yoğun 2. Daha Yavaş Yanıtlar ve Daha Düşük Performans 3. Karmaşık Kurulum\n\nJupyter notebook veya Google Colab kullanarak lokal olarak model çalıştırma\nAslında LLM modellerini Jupyter notebook’larında çalıştırabilirsiniz. Ancak bu süreç, birçok Python paketini kurmanızı, model ağırlıklarını indirmenizi ve ardından modele soru sormak ve yanıtı bir sözlükte yakalamak için biraz Python kodu yazmanızı gerektirecektir. “Sohbet benzeri” deneyim için bazı çözümler olsa da, deneyim daha çok “tek seferlik soru ve cevap” gibidir.\nLütfen Google’ın bir Jupyter notebook ortamında modeli çalıştırmak için GPU (veya TPU) sağladığı Google Colab’daki süreci anlatan YouTube videosunu kontrol edin.\n\n\nOllama ile lokal olarak model çalıştırma\n\nÖzelleştirilmiş modeller hakkında bir fikir edinmek için lütfen modeller listesi sayfasını ziyaret edin. Ollama ile, bu modellerden herhangi birini indirebilir ve kullanabilirsiniz.\nTerminalde, mevcut modelleri listeleyelim\n$ ollama list\n\nNAME                ID              SIZE    MODIFIED\ndeepseek-coder:6.7b 72be2442d736    3.8 GB  5 weeks ago     \nneural-chat:latest  73940af9fe02    4.1 GB  5 weeks ago     \norca2:7b            ea98cc422de3    3.8 GB  5 weeks ago     \nphi:latest          c651b7a89d73    1.6 GB  14 hours ago    \nsolar:latest        059fdabbe6e6    6.1 GB  2 hours ago     \nstablelm-zephyr:3b  7c596e78b1fc    1.6 GB  3 weeks ago\nMicrosoft tarafından geliştirilen Phi-2’yi çalıştıralım. İşte Phi-2 hakkında bilgi:\n\n2,7 milyar parametreli bir dil modeli olan Phi-2, olağanüstü akıl yürütme ve dil anlama yetenekleri sergiliyor ve 13 milyar parametreden daha az olan temel dil modelleri arasında en son teknoloji performansı gösteriyor. Karmaşık kıyaslamalarda Phi-2, model ölçeklendirme ve eğitim verisi kürasyon konusundaki yeni yenilikler sayesinde 25 kat daha büyük modellere eşdeğer veya daha iyi performans gösteriyor.\n\n$ ollama run phi:latest\n\n&gt;&gt;&gt; Gökyüzü neden mavi?\n\nGökyüzü, Rayleigh saçılması adı verilen bir fenomen nedeniyle mavi görünür. Güneş ışığı Dünya'nın atmosferine girdiğinde, oksijen ve azot atomları gibi havadaki moleküller ve küçük parçacıklarla çarpışır. Bu çarpışmalar, ışığın daha kısa dalga boylarının (mavi) daha uzun dalga boylarından (kırmızı, turuncu, sarı, yeşil ve mor) daha fazla saçılmasına neden olur.\nSonuç olarak, gözlerimiz saçılan mavi ışığı baskın olarak algılar ve bu nedenle gökyüzü bize mavi görünür.\nGördüğünüz gibi, soruları cevaplayabilen, kodlama konusunda yardımcı olabilen küçük bir model yerel olarak çalıştırılabilir.\n\n\nKullanıcı arayüzü ile lokal olarak model çalıştırma\n\nOllama Web UI\n\n\n\nLLM Studio\nLM Studio’yu kurabilir ve ardından kullanıcı arayüzü ile yerel modellerle etkileşime geçebilirsiniz"
  },
  {
    "objectID": "ders1-TR.html#github-copilot",
    "href": "ders1-TR.html#github-copilot",
    "title": "GPT modelleri",
    "section": "Github CoPilot",
    "text": "Github CoPilot"
  },
  {
    "objectID": "ders1-TR.html#ücretsiz-github-copilot-alternatifi",
    "href": "ders1-TR.html#ücretsiz-github-copilot-alternatifi",
    "title": "GPT modelleri",
    "section": "Ücretsiz Github CoPilot alternatifi",
    "text": "Ücretsiz Github CoPilot alternatifi\nCody, Sourcegraph’in AI kodlama asistanıdır ve sizi 10x geliştirici yapacak başka hiçbir asistanın sahip olmadığı birkaç özelliğe sahiptir. Buradan inceleyebilirsiniz: https://sourcegraph.com/cody"
  },
  {
    "objectID": "ders1-TR.html#açık-kaynak-lokal-alternatifler",
    "href": "ders1-TR.html#açık-kaynak-lokal-alternatifler",
    "title": "GPT modelleri",
    "section": "Açık kaynak lokal alternatifler",
    "text": "Açık kaynak lokal alternatifler\nDaha fazla bilgi için lütfen https://tabby.tabbyml.com/ adresini ziyaret edin"
  },
  {
    "objectID": "ders1-TR.html#görüntü-üretimi",
    "href": "ders1-TR.html#görüntü-üretimi",
    "title": "GPT modelleri",
    "section": "Görüntü Üretimi",
    "text": "Görüntü Üretimi\n\nOpenAI’dan DALL-E 3\nİstek: “arka planda istanbul şehri silüeti olan bir dna sarmalı”\nDüzeltilmiş istek: “Bilim ve mimarinin buluştuğu detaylı bir illüstrasyon hayal edin. Ön planda, karanlık bir arka plan üzerinde büyütülmüş ve parlayan bir DNA sarmalı var. Bu sarmal yapı tam olarak betimlenmiş, tanıdık çift sarmal şekli muhteşem bir şekilde kıvrılıyor. Bu bilimsel görüntüyle iç içe geçmiş şekilde, arka planda İstanbul şehrinin bir silüet temsili var. Şehir silueti, kubbeli camileri ve yükselen minareleriyle muhteşem tarihi mimarisini sergiliyor. Şehir manzarası, akşamüstü gökyüzüne karşı duruyor, batan güneş binaların arkasındaki ufukta turuncu bir parıltı yaratıyor. Bu kombinasyon, bilim ve şehir güzelliğinin büyüleyici bir yan yana gelişiyle sonuçlanıyor.”\nSonuç:\n\n\n\nMidjourney v6\nMidjourney, Aralık 2023’te v6’yı duyurdu. Fotogerçekçi görüntüler üretebiliyor. Normalde, ücretsiz görüntü üretmek için Discord kanallarını kullanmanız gerekir. Ayrıca, getimg.ai kullanarak da görüntüler üretebilirsiniz. Aşağıdaki isteklerle görüntüler üretmeyi denedim:\n\nÇok sayıda öğrencinin olduğu moleküler biyoloji laboratuvarı.\nBeyaz kumlu güzel bir deniz kenarı ve kıyıda tropikal ağaçlar. Deniz yeşilimsi mavi görünüyor.\n\nSonuçlar harika:\n\n\n\nDezenformasyon\nDeepfake veya yapay zeka tarafından üretilmiş sahte fotoğraflara dikkat edin\nSahte Trump Fotoğrafı\n\nSahte Papa Fotoğrafı"
  },
  {
    "objectID": "ders1-TR.html#video-üretimi",
    "href": "ders1-TR.html#video-üretimi",
    "title": "GPT modelleri",
    "section": "Video Üretimi",
    "text": "Video Üretimi\n\nİstekten videoya\n\nRunwayML Gen-2\nPika\nStable Diffusion’dan Stable Video Diffusion\nGENMO\n\n\n\nGörüntüden videoya\n\nGENMO"
  },
  {
    "objectID": "ders1-TR.html#müzik-veya-ses-üretimi",
    "href": "ders1-TR.html#müzik-veya-ses-üretimi",
    "title": "GPT modelleri",
    "section": "Müzik veya Ses Üretimi",
    "text": "Müzik veya Ses Üretimi\n\nElevenlabs yapay zeka ses üreticimizle metni çevrimiçi olarak ücretsiz konuşmaya dönüştürebilir"
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering-TR.html",
    "href": "An_Introduction_to_Hierarchical_Clustering-TR.html",
    "title": "Hiyerarşik Kümelemeye Giriş",
    "section": "",
    "text": "DataCamp’teki “An Introduction to Hierarchical Clusteringin Python” adlı makaleden alınmıştır.\n\n\nHiyerarşik kümeleme yaklaşımı, daha önce tanımlanmış kümelere dayalı olarak ardışık kümelerin belirlenmesine dayanır. Bu teknik, verileri dendrogramlar olarak adlandırılan bir kümeler ağacında gruplamayı amaçlar ve altta yatan kümeler arasındaki hiyerarşik ilişkiyi grafiksel olarak temsil eder.\n\n\n\nHiyerarşik kümeleme güçlü bir algoritmadır, ancak tek algoritma değildir ve her kümeleme türü kendi avantaj ve dezavantajlarıyla birlikte gelir.\nK-means ve model tabanlı kümeleme gibi diğer kümeleme türleriyle nasıl karşılaştırıldığını anlamaya çalışalım. Çok daha fazla teknik var, ancak bunlar ikisi, hiyerarşik kümelemeye ek olarak, yaygın olarak kullanılır ve diğerlerini daha iyi anlamak için bir çerçeve sağlar.\nMakine öğreniminde kümeleme hakkında daha fazla bilgiyi, beş temel kümeleme algoritmasını kapsayan ayrı makalemizde bulabilirsiniz.\n\n\nHiyerarşik kümelemenin aksine, K-means kümeleme, orijinal veri noktalarını kullanıcının önceden belirlediği “K” gruba veya kümeye ayırmayı amaçlar.\nGenel fikir, tüm öznitelikler (değişkenler veya özellikler) üzerinden tüm noktaların merkezlerden karesel Öklid mesafesini minimize eden kümeler aramak ve bu bireyleri yinelemeli bir şekilde birleştirmektir.\n“Scikit-learn ile Python’da K-means Kümeleme” öğreticimiz, ilginç bir vaka çalışmasıyla K-means kümelemenin iç işleyişini anlamanıza yardımcı olacaktır.\n\n\n\nHiyerarşik kümelemeye kıyasla hesaplama açısından verimlidir ve büyük veri setlerini analiz etmek için kullanılabilir.\nK-means anlaşılması ve uygulanması daha kolaydır.\n\n\n\n\n\nHiyerarşik kümelemeden daha az esnektir çünkü kullanıcıyı küme sayısını önceden belirtmeye zorlar, bu da bazı durumlarda açık olmayabilir.\nSonuç kararlı değildir ve aynı veri seti için bir yinelemeden diğerine değişir.\nAykırı değerlere karşı daha duyarlıdır çünkü verilerdeki aykırı değerlerin kullanımı kümenin ortalamasını etkiler.\nHem k-means hem de hiyerarşik kümeleme, kategorik verileri doğrudan işleyemez ve sürekli olmayan veya çok büyük varyansa sahip verilerle iyi çalışmayabilir.\n\nSınırlamalarına rağmen, k-means kümeleme hala kullanım kolaylığı ve hesaplama verimliliği nedeniyle popüler bir yöntemdir. Sıklıkla diğer kümeleme tekniklerinin performansını karşılaştırmak için bir referans noktası olarak kullanılır.\n\n\n\n\nHem K-means hem de hiyerarşik kümeleme teknikleri, veri setindeki tüm noktalar arasındaki mesafeleri temsil etmek için bir uzaklık matrisi kullanır. Öte yandan, model tabanlı kümeleme, verilerdeki kümeleri tanımlamak için istatistiksel teknikler uygular. Genel süreç aşağıdaki gibidir:\n\nKullanılacak istatistiksel modele karar verin ve küme sayısını seçin.\nModeli verilere uygulayın.\nModel parametrelerine göre kümeleri belirleyin.\n\n\n\n\nModel tabanlı kümeleme, farklı küme türlerini tanımlamak için farklı modellerin kullanılmasına izin verdiği için hiyerarşik kümelemeden daha esnektir.\nKarmaşık şekillere veya yapılara sahip veriler üzerinde daha iyi çalışır.\n\n\n\n\n\nÖzellikle büyük veriler için hesaplama açısından hiyerarşik kümelemeden daha pahalıdır.\nModel seçimi nihai sonucu etkileyebileceğinden, istatistiksel modelleme tekniklerinin daha iyi anlaşılmasını gerektirir.\nK-means kümelemeye benzer şekilde, küme sayısının önceden belirtilmesini gerektirir.\n\n\n\n\n\n\nHiyerarşik kümeleme, günlük hayatımızda biyoloji, görüntü işleme, pazarlama, ekonomi ve sosyal ağ analizi dahil (ancak bunlarla sınırlıolmamak üzere) çeşitli uygulamalara sahiptir.\n\n\nDNA dizilerinin kümelenmesi, biyoinformatiğin en büyük zorluklarından biridir.\nBiyologlar, organizmaları taksonomik gruplara sınıflandırmak için organizmalar arasındaki genetik ilişkileri incelemek üzere hiyerarşikkümelemeyi kullanabilirler. Bu, altta yatan ilişkilerin hızlı analizi ve görselleştirilmesi için faydalıdır.\n\n\n\nGörüntü işlemede, renk, yoğunluk veya diğer özellikler açısından bir görüntünün benzer bölgelerini veya piksellerini gruplandırmak içinhiyerarşik kümeleme gerçekleştirilebilir. Bu, görüntü segmentasyonu, görüntü sınıflandırması ve nesne tanıma gibi diğer görevler için faydalı olabilir.\n\n\n\nPazarlama uzmanları, daha iyi pazarlama stratejileri ve ürün önerileri için müşterilerin satın alma alışkanlıklarına göre farklı müşteri türleri arasında bir hiyerarşi çizmek için hiyerarşik kümelemeyi kullanabilirler. Örneğin, perakendede farklı ürünler, düşük, orta veya yüksek harcama yapan müşterilere göre önerilebilir.\n\n\n\nSosyal ağlar, verimli bir şekilde kullanıldığında değerli bilgilerin harika bir kaynağıdır. Hiyerarşik kümeleme, grupları veya toplulukları belirlemek ve bunların birbirleriyle ve ağın bütünüyle olan ilişkilerini anlamak için kullanılabilir.\n\n\n\n\nBu bölümde üç ana kavrama bakacağız. Hiyerarşik algoritmanın adımları, iki tür hiyerarşik kümelemenin (aglomeratif ve bölücü) vurgulanması ve son olarak doğru mesafe ölçüsünü seçmek için bazı teknikler.\n\n\nHiyerarşik kümeleme algoritması, kümeler oluşturmak için mesafe ölçülerini kullanır. Bu oluşturma süreci aşağıdaki ana adımları içerir:\n\n\n\nHiyerarşik Kümeleme Algoritması Oluşturma\n\n\nEksik verileri kaldırarak ve verileri mümkün olduğunca temiz hale getiren ek görevler uygulayarak verileri ön işlemden geçirin. Bu adım, çoğu makine öğrenimi görevi için daha genel bir adımdır.\n\nÖklid mesafesi, Manhattan mesafesi veya kosinüs benzerliği gibi belirli bir mesafe metriği kullanarak her veri noktası çifti arasındaki mesafeyi içeren mesafe matrisini hesaplayın. Ancak varsayılan mesafe metriği Öklid mesafesidir.\nMesafe olarak en yakın olan iki kümeyi birleştirin.\nMesafe matrisini yeni kümelere göre güncelleyin.\nTüm kümeler tek bir küme oluşturmak üzere birleştirilene kadar 1, 2 ve 3. adımları tekrarlayın.\n\n\n\n\nAglomeratif ve bölücü kümelemeyi birbirinin aynası olarak düşünebiliriz. Her birinin nasıl çalıştığına, hiyerarşik kümeleme örneğine vegrafiksel görselleştirmeye daha yakından bakalım.\n\n\nBu ilk senaryo, yukarıda açıklanan yaklaşıma karşılık gelir. Her gözlemi tekil bir küme (yalnızca bir veri noktası içeren küme) olarak düşünerek başlar. Daha sonra yalnızca bir küme elde edilene kadar kümeleri yinelemeli olarak birleştirir. Bu süreç aşağıdan yukarıya yaklaşım olarak da bilinir.\nAşağıdaki örnekte gösterildiği gibi:\n\nHer hayvanı kendine özgü bir küme olarak düşünerek başlıyoruz.\nArdından, bu benzersiz hayvanlardan benzerliklerine göre üç farklı küme oluşturuyoruz:\n\nKuşlar: Kartal ve Tavus Kuşu\nMemeliler: Aslan ve Ayı\nÜçten fazla bacaklı hayvanlar: Örümcek ve Akrep\n\nEn benzer iki kümeyi (Kuşlar ve Memeliler) birleştirerek Omurgalıkümesini oluşturmak için birleştirme işlemini tekrarlıyoruz.\nBu adımdan sonra, kalan iki küme, Omurgalı ve Üçten fazla bacaklı, tek bir Hayvanlar kümesi oluşturmak üzere birleştirilir.\n\n\n\n\nAglomeratif Kümeleme Yaklaşımının Dendrogramı\n\n\nAglomeratif Kümeleme Yaklaşımının Dendrogramı\n\n\n\nÖte yandan, bölücü kümeleme yukarıdan aşağıya bir yaklaşımdır çünkütüm veri noktalarını tek bir küme olarak düşünerek başlar. Daha sonra tüm veri noktaları benzersiz olana kadar onları ayırır.\nBu bölücü yaklaşım grafiğinden:\n\nTüm hayvan veri setinin tek bir blok olarak düşünüldüğünü fark ediyoruz.\nArdından, bu bloğu iki kümeye ayırıyoruz: Omurgalı ve 3’ten fazlabacaklı.\nBölme işlemi, benzersiz hayvanlara ulaşana kadar önceden oluşturulan kümelere yinelemeli olarak uygulanır.\n\n\n\n\nBölücü Kümeleme Yaklaşımının Dendrogramı\n\n\nBölücü Kümeleme Yaklaşımının Dendrogramı\n\n\n\n\nMesafe ölçüsü seçiminiz, kümelemede kritik bir adımdır ve çözmeye çalıştığınız probleme bağlıdır. Aşağıdaki senaryoyu düşünerek, öğrencileri şu gibi herhangi bir yaklaşıma göre kümeleyebiliriz:\n\nMenşe ülkesi\nCinsiyet, erkek veya kadın\nÖnceki akademik geçmiş\n\nBunların hepsi geçerli kümelerdir ancak anlamları farklıdır.\nÖklid mesafesi çoğu kümeleme yazılımında en yaygın kullanılan mesafe olmasına rağmen, Manhattan mesafesi, Canberra mesafesi, Pearson veya Spearman korelasyonu ve Minkowski mesafesi gibi diğer mesafe ölçüleri de mevcuttur.\n\n\n\nDaha önce bahsedilen mesafeler öğelerle ilgilidir. Bu bölümde, onları birleştirmeden önce en yakın küme çiftini ölçmenin üç standart yolunu (kapsamlı değil) ele alacağız: (1) Tek bağlantı, (2) Tam bağlantı ve (3) Ortalama bağlantı.\n\n\nC1 ve C2 kümelerindeki öğeler arasındaki tüm ikili mesafelerden, tek bağlantı kümeler arasındaki mesafeyi minimum mesafe olarak alır.\nMesafe (C1, C2) = Min { d(i, j), burada öğe i C1 içinde ve öğe jC2 içinde}\nİki kümeden gelen tüm öğe çiftleri arasından, yeşil renkle vurgulananlar minimum mesafeye sahiptir.\n\n\n\nTek bağlantı gösterimi\n\n\nTek bağlantı gösterimi\n\n\n\nC1 ve C2 kümelerindeki öğeler arasındaki tüm ikili mesafelerden, tam bağlantı kümeler arasındaki mesafeyi maksimum mesafe olarak alır.\nMesafe (C1, C2) = Max { d(i, j), burada öğe i C1 içinde ve öğe jC2 içinde}\nİki kümeden gelen tüm öğe çiftleri arasından, yeşil renkle vurgulananlar maksimum mesafeye sahiptir.\n\n\n\nTam bağlantı gösterimi\n\n\nTam bağlantı gösterimi\n\n\n\nOrtalama bağlantı kümelemesinde, verilen iki küme C1 ve C2 arasındaki mesafe, iki kümedeki tüm öğe çiftleri arasındaki ortalama mesafelere karşılık gelir.\nMesafe (C1, C2) = Toplam{ d(i, j) } / Toplam mesafe sayısı\n\n\n\nOrtalama bağlantı gösterimi\n\n\nOrtalama bağlantı gösterimi\nArdından ortalama bağlantı kümelemesi şu şekilde gerçekleştirilir:\nd(a,j) + d(a,h) + d(a,n) + d(d,j) + d(d,h) + d(d,n)\n————————————————————–, burada Toplam mesafe sayısı = 6\nToplam mesafe sayısı"
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümelemenin-tanımı",
    "href": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümelemenin-tanımı",
    "title": "Hiyerarşik Kümelemeye Giriş",
    "section": "",
    "text": "Hiyerarşik kümeleme yaklaşımı, daha önce tanımlanmış kümelere dayalı olarak ardışık kümelerin belirlenmesine dayanır. Bu teknik, verileri dendrogramlar olarak adlandırılan bir kümeler ağacında gruplamayı amaçlar ve altta yatan kümeler arasındaki hiyerarşik ilişkiyi grafiksel olarak temsil eder."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümelemenin-diğer-kümeleme-teknikleriyle-karşılaştırılması",
    "href": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümelemenin-diğer-kümeleme-teknikleriyle-karşılaştırılması",
    "title": "Hiyerarşik Kümelemeye Giriş",
    "section": "",
    "text": "Hiyerarşik kümeleme güçlü bir algoritmadır, ancak tek algoritma değildir ve her kümeleme türü kendi avantaj ve dezavantajlarıyla birlikte gelir.\nK-means ve model tabanlı kümeleme gibi diğer kümeleme türleriyle nasıl karşılaştırıldığını anlamaya çalışalım. Çok daha fazla teknik var, ancak bunlar ikisi, hiyerarşik kümelemeye ek olarak, yaygın olarak kullanılır ve diğerlerini daha iyi anlamak için bir çerçeve sağlar.\nMakine öğreniminde kümeleme hakkında daha fazla bilgiyi, beş temel kümeleme algoritmasını kapsayan ayrı makalemizde bulabilirsiniz.\n\n\nHiyerarşik kümelemenin aksine, K-means kümeleme, orijinal veri noktalarını kullanıcının önceden belirlediği “K” gruba veya kümeye ayırmayı amaçlar.\nGenel fikir, tüm öznitelikler (değişkenler veya özellikler) üzerinden tüm noktaların merkezlerden karesel Öklid mesafesini minimize eden kümeler aramak ve bu bireyleri yinelemeli bir şekilde birleştirmektir.\n“Scikit-learn ile Python’da K-means Kümeleme” öğreticimiz, ilginç bir vaka çalışmasıyla K-means kümelemenin iç işleyişini anlamanıza yardımcı olacaktır.\n\n\n\nHiyerarşik kümelemeye kıyasla hesaplama açısından verimlidir ve büyük veri setlerini analiz etmek için kullanılabilir.\nK-means anlaşılması ve uygulanması daha kolaydır.\n\n\n\n\n\nHiyerarşik kümelemeden daha az esnektir çünkü kullanıcıyı küme sayısını önceden belirtmeye zorlar, bu da bazı durumlarda açık olmayabilir.\nSonuç kararlı değildir ve aynı veri seti için bir yinelemeden diğerine değişir.\nAykırı değerlere karşı daha duyarlıdır çünkü verilerdeki aykırı değerlerin kullanımı kümenin ortalamasını etkiler.\nHem k-means hem de hiyerarşik kümeleme, kategorik verileri doğrudan işleyemez ve sürekli olmayan veya çok büyük varyansa sahip verilerle iyi çalışmayabilir.\n\nSınırlamalarına rağmen, k-means kümeleme hala kullanım kolaylığı ve hesaplama verimliliği nedeniyle popüler bir yöntemdir. Sıklıkla diğer kümeleme tekniklerinin performansını karşılaştırmak için bir referans noktası olarak kullanılır.\n\n\n\n\nHem K-means hem de hiyerarşik kümeleme teknikleri, veri setindeki tüm noktalar arasındaki mesafeleri temsil etmek için bir uzaklık matrisi kullanır. Öte yandan, model tabanlı kümeleme, verilerdeki kümeleri tanımlamak için istatistiksel teknikler uygular. Genel süreç aşağıdaki gibidir:\n\nKullanılacak istatistiksel modele karar verin ve küme sayısını seçin.\nModeli verilere uygulayın.\nModel parametrelerine göre kümeleri belirleyin.\n\n\n\n\nModel tabanlı kümeleme, farklı küme türlerini tanımlamak için farklı modellerin kullanılmasına izin verdiği için hiyerarşik kümelemeden daha esnektir.\nKarmaşık şekillere veya yapılara sahip veriler üzerinde daha iyi çalışır.\n\n\n\n\n\nÖzellikle büyük veriler için hesaplama açısından hiyerarşik kümelemeden daha pahalıdır.\nModel seçimi nihai sonucu etkileyebileceğinden, istatistiksel modelleme tekniklerinin daha iyi anlaşılmasını gerektirir.\nK-means kümelemeye benzer şekilde, küme sayısının önceden belirtilmesini gerektirir."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümelemenin-uygulamaları",
    "href": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümelemenin-uygulamaları",
    "title": "Hiyerarşik Kümelemeye Giriş",
    "section": "",
    "text": "Hiyerarşik kümeleme, günlük hayatımızda biyoloji, görüntü işleme, pazarlama, ekonomi ve sosyal ağ analizi dahil (ancak bunlarla sınırlıolmamak üzere) çeşitli uygulamalara sahiptir.\n\n\nDNA dizilerinin kümelenmesi, biyoinformatiğin en büyük zorluklarından biridir.\nBiyologlar, organizmaları taksonomik gruplara sınıflandırmak için organizmalar arasındaki genetik ilişkileri incelemek üzere hiyerarşikkümelemeyi kullanabilirler. Bu, altta yatan ilişkilerin hızlı analizi ve görselleştirilmesi için faydalıdır.\n\n\n\nGörüntü işlemede, renk, yoğunluk veya diğer özellikler açısından bir görüntünün benzer bölgelerini veya piksellerini gruplandırmak içinhiyerarşik kümeleme gerçekleştirilebilir. Bu, görüntü segmentasyonu, görüntü sınıflandırması ve nesne tanıma gibi diğer görevler için faydalı olabilir.\n\n\n\nPazarlama uzmanları, daha iyi pazarlama stratejileri ve ürün önerileri için müşterilerin satın alma alışkanlıklarına göre farklı müşteri türleri arasında bir hiyerarşi çizmek için hiyerarşik kümelemeyi kullanabilirler. Örneğin, perakendede farklı ürünler, düşük, orta veya yüksek harcama yapan müşterilere göre önerilebilir.\n\n\n\nSosyal ağlar, verimli bir şekilde kullanıldığında değerli bilgilerin harika bir kaynağıdır. Hiyerarşik kümeleme, grupları veya toplulukları belirlemek ve bunların birbirleriyle ve ağın bütünüyle olan ilişkilerini anlamak için kullanılabilir."
  },
  {
    "objectID": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümeleme-algoritması",
    "href": "An_Introduction_to_Hierarchical_Clustering-TR.html#hiyerarşik-kümeleme-algoritması",
    "title": "Hiyerarşik Kümelemeye Giriş",
    "section": "",
    "text": "Bu bölümde üç ana kavrama bakacağız. Hiyerarşik algoritmanın adımları, iki tür hiyerarşik kümelemenin (aglomeratif ve bölücü) vurgulanması ve son olarak doğru mesafe ölçüsünü seçmek için bazı teknikler.\n\n\nHiyerarşik kümeleme algoritması, kümeler oluşturmak için mesafe ölçülerini kullanır. Bu oluşturma süreci aşağıdaki ana adımları içerir:\n\n\n\nHiyerarşik Kümeleme Algoritması Oluşturma\n\n\nEksik verileri kaldırarak ve verileri mümkün olduğunca temiz hale getiren ek görevler uygulayarak verileri ön işlemden geçirin. Bu adım, çoğu makine öğrenimi görevi için daha genel bir adımdır.\n\nÖklid mesafesi, Manhattan mesafesi veya kosinüs benzerliği gibi belirli bir mesafe metriği kullanarak her veri noktası çifti arasındaki mesafeyi içeren mesafe matrisini hesaplayın. Ancak varsayılan mesafe metriği Öklid mesafesidir.\nMesafe olarak en yakın olan iki kümeyi birleştirin.\nMesafe matrisini yeni kümelere göre güncelleyin.\nTüm kümeler tek bir küme oluşturmak üzere birleştirilene kadar 1, 2 ve 3. adımları tekrarlayın.\n\n\n\n\nAglomeratif ve bölücü kümelemeyi birbirinin aynası olarak düşünebiliriz. Her birinin nasıl çalıştığına, hiyerarşik kümeleme örneğine vegrafiksel görselleştirmeye daha yakından bakalım.\n\n\nBu ilk senaryo, yukarıda açıklanan yaklaşıma karşılık gelir. Her gözlemi tekil bir küme (yalnızca bir veri noktası içeren küme) olarak düşünerek başlar. Daha sonra yalnızca bir küme elde edilene kadar kümeleri yinelemeli olarak birleştirir. Bu süreç aşağıdan yukarıya yaklaşım olarak da bilinir.\nAşağıdaki örnekte gösterildiği gibi:\n\nHer hayvanı kendine özgü bir küme olarak düşünerek başlıyoruz.\nArdından, bu benzersiz hayvanlardan benzerliklerine göre üç farklı küme oluşturuyoruz:\n\nKuşlar: Kartal ve Tavus Kuşu\nMemeliler: Aslan ve Ayı\nÜçten fazla bacaklı hayvanlar: Örümcek ve Akrep\n\nEn benzer iki kümeyi (Kuşlar ve Memeliler) birleştirerek Omurgalıkümesini oluşturmak için birleştirme işlemini tekrarlıyoruz.\nBu adımdan sonra, kalan iki küme, Omurgalı ve Üçten fazla bacaklı, tek bir Hayvanlar kümesi oluşturmak üzere birleştirilir.\n\n\n\n\nAglomeratif Kümeleme Yaklaşımının Dendrogramı\n\n\nAglomeratif Kümeleme Yaklaşımının Dendrogramı\n\n\n\nÖte yandan, bölücü kümeleme yukarıdan aşağıya bir yaklaşımdır çünkütüm veri noktalarını tek bir küme olarak düşünerek başlar. Daha sonra tüm veri noktaları benzersiz olana kadar onları ayırır.\nBu bölücü yaklaşım grafiğinden:\n\nTüm hayvan veri setinin tek bir blok olarak düşünüldüğünü fark ediyoruz.\nArdından, bu bloğu iki kümeye ayırıyoruz: Omurgalı ve 3’ten fazlabacaklı.\nBölme işlemi, benzersiz hayvanlara ulaşana kadar önceden oluşturulan kümelere yinelemeli olarak uygulanır.\n\n\n\n\nBölücü Kümeleme Yaklaşımının Dendrogramı\n\n\nBölücü Kümeleme Yaklaşımının Dendrogramı\n\n\n\n\nMesafe ölçüsü seçiminiz, kümelemede kritik bir adımdır ve çözmeye çalıştığınız probleme bağlıdır. Aşağıdaki senaryoyu düşünerek, öğrencileri şu gibi herhangi bir yaklaşıma göre kümeleyebiliriz:\n\nMenşe ülkesi\nCinsiyet, erkek veya kadın\nÖnceki akademik geçmiş\n\nBunların hepsi geçerli kümelerdir ancak anlamları farklıdır.\nÖklid mesafesi çoğu kümeleme yazılımında en yaygın kullanılan mesafe olmasına rağmen, Manhattan mesafesi, Canberra mesafesi, Pearson veya Spearman korelasyonu ve Minkowski mesafesi gibi diğer mesafe ölçüleri de mevcuttur.\n\n\n\nDaha önce bahsedilen mesafeler öğelerle ilgilidir. Bu bölümde, onları birleştirmeden önce en yakın küme çiftini ölçmenin üç standart yolunu (kapsamlı değil) ele alacağız: (1) Tek bağlantı, (2) Tam bağlantı ve (3) Ortalama bağlantı.\n\n\nC1 ve C2 kümelerindeki öğeler arasındaki tüm ikili mesafelerden, tek bağlantı kümeler arasındaki mesafeyi minimum mesafe olarak alır.\nMesafe (C1, C2) = Min { d(i, j), burada öğe i C1 içinde ve öğe jC2 içinde}\nİki kümeden gelen tüm öğe çiftleri arasından, yeşil renkle vurgulananlar minimum mesafeye sahiptir.\n\n\n\nTek bağlantı gösterimi\n\n\nTek bağlantı gösterimi\n\n\n\nC1 ve C2 kümelerindeki öğeler arasındaki tüm ikili mesafelerden, tam bağlantı kümeler arasındaki mesafeyi maksimum mesafe olarak alır.\nMesafe (C1, C2) = Max { d(i, j), burada öğe i C1 içinde ve öğe jC2 içinde}\nİki kümeden gelen tüm öğe çiftleri arasından, yeşil renkle vurgulananlar maksimum mesafeye sahiptir.\n\n\n\nTam bağlantı gösterimi\n\n\nTam bağlantı gösterimi\n\n\n\nOrtalama bağlantı kümelemesinde, verilen iki küme C1 ve C2 arasındaki mesafe, iki kümedeki tüm öğe çiftleri arasındaki ortalama mesafelere karşılık gelir.\nMesafe (C1, C2) = Toplam{ d(i, j) } / Toplam mesafe sayısı\n\n\n\nOrtalama bağlantı gösterimi\n\n\nOrtalama bağlantı gösterimi\nArdından ortalama bağlantı kümelemesi şu şekilde gerçekleştirilir:\nd(a,j) + d(a,h) + d(a,n) + d(d,j) + d(d,h) + d(d,n)\n————————————————————–, burada Toplam mesafe sayısı = 6\nToplam mesafe sayısı"
  },
  {
    "objectID": "r-linear-regression-tr.html",
    "href": "r-linear-regression-tr.html",
    "title": "R ile Doğrusal Regresyon",
    "section": "",
    "text": "Burada, 1974 Motor Trend US dergisinden alınan mtcars {datasets} adlı bir verimiz var ve 32 otomobil (1973-74 modelleri) için yakıt tüketimi ve otomobil tasarımı ve performansının 10 yönünü içermektedir. Ağırlık (wt) ve Motor Hacmi (disp) parametreleri arasında matematiksel bir model bulacağız. Hadi en uygun modeli bulalım. Lütfen grafiği çizin, denklemin derecesini belirleyin (birinci, ikinci veya üçüncü) ve eğriyi çizin.\n\nmtcars\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n [ reached 'max' / getOption(\"max.print\") -- omitted 23 rows ]\n\n\n\nplot(mtcars)\n\n\n\n\n\n\n\n\n\nplot(mtcars$disp, mtcars$wt)\n\n\n\n\n\n\n\n\n\nfirst_wd &lt;- lm(wt ~ disp, data=mtcars)\nsummary(first_wd)\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\n\nsecond_wd &lt;- lm(wt ~ disp + I(disp^2), data=mtcars)\nsummary(second_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88654 -0.29136 -0.00961  0.32389  0.67010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.631e+00  3.622e-01   4.502 0.000101 ***\ndisp        6.694e-03  3.325e-03   2.013 0.053448 .  \nI(disp^2)   6.206e-07  6.380e-06   0.097 0.923178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4652 on 29 degrees of freedom\nMultiple R-squared:  0.7886,    Adjusted R-squared:  0.774 \nF-statistic: 54.08 on 2 and 29 DF,  p-value: 1.639e-10\n\n\n\nthird_wd &lt;- lm(wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\nsummary(third_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63764 -0.22785  0.00307  0.24614  0.58333 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.111e+00  5.651e-01  -1.966   0.0593 .  \ndisp         4.945e-02  8.198e-03   6.032 1.68e-06 ***\nI(disp^2)   -1.807e-04  3.361e-05  -5.378 9.88e-06 ***\nI(disp^3)    2.243e-07  4.119e-08   5.446 8.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3299 on 28 degrees of freedom\nMultiple R-squared:  0.8973,    Adjusted R-squared:  0.8863 \nF-statistic: 81.57 on 3 and 28 DF,  p-value: 5.959e-14\n\n\n\nplot(mtcars$disp,mtcars$wt)\ncurve(2.243e-07*x^3 -1.807e-04*x^2 +4.945e-02*x -1.111e+00,add=TRUE)\n\n\n\n\n\n\n\n\n\nbest_model &lt;- lm(wt ~ poly(disp,15), data=mtcars)\nsummary(best_model)\n\n\nCall:\nlm(formula = wt ~ poly(disp, 15), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43800 -0.11202 -0.00364  0.16319  0.37783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.2172500  0.0509709  63.119  &lt; 2e-16 ***\npoly(disp, 15)1   4.8375552  0.2883350  16.778 1.41e-11 ***\npoly(disp, 15)2   0.0452475  0.2883350   0.157   0.8773    \npoly(disp, 15)3   1.7965723  0.2883350   6.231 1.20e-05 ***\npoly(disp, 15)4   0.4463962  0.2883350   1.548   0.1411    \npoly(disp, 15)5  -0.6866393  0.2883350  -2.381   0.0300 *  \npoly(disp, 15)6  -0.3384161  0.2883350  -1.174   0.2577    \npoly(disp, 15)7  -0.6134397  0.2883350  -2.128   0.0493 *  \npoly(disp, 15)8   0.0528245  0.2883350   0.183   0.8569    \npoly(disp, 15)9   0.0814084  0.2883350   0.282   0.7813    \npoly(disp, 15)10 -0.0654665  0.2883350  -0.227   0.8233    \npoly(disp, 15)11  0.2853853  0.2883350   0.990   0.3370    \npoly(disp, 15)12 -0.0007108  0.2883350  -0.002   0.9981    \npoly(disp, 15)13  0.4495723  0.2883350   1.559   0.1385    \npoly(disp, 15)14 -0.0699728  0.2883350  -0.243   0.8113    \npoly(disp, 15)15 -0.5031480  0.2883350  -1.745   0.1002    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2883 on 16 degrees of freedom\nMultiple R-squared:  0.9552,    Adjusted R-squared:  0.9132 \nF-statistic: 22.73 on 15 and 16 DF,  p-value: 5.551e-08\n\n\n\npred &lt;- predict(best_model, data.frame(disp=50:500))\nplot(mtcars$disp, mtcars$wt)\npoints(50:500, pred, type=\"l\")\n\n\n\n\n\n\n\n\n\npredict(third_wd, data.frame(disp=420))\n\n       1 \n4.397246 \n\n\n\npredict(best_model, data.frame(disp=420))\n\n       1 \n7.329183 \n\n\n\npredict(best_model, data.frame(disp=c(20,420, 500)))\n\n           1            2            3 \n 3865.329736     7.329183 -1513.538298 \n\n\n\nYani, “best_model” model aslında hiç de en iyi değil.."
  },
  {
    "objectID": "r-linear-regression-tr.html#polinom-modeli",
    "href": "r-linear-regression-tr.html#polinom-modeli",
    "title": "R ile Doğrusal Regresyon",
    "section": "",
    "text": "Burada, 1974 Motor Trend US dergisinden alınan mtcars {datasets} adlı bir verimiz var ve 32 otomobil (1973-74 modelleri) için yakıt tüketimi ve otomobil tasarımı ve performansının 10 yönünü içermektedir. Ağırlık (wt) ve Motor Hacmi (disp) parametreleri arasında matematiksel bir model bulacağız. Hadi en uygun modeli bulalım. Lütfen grafiği çizin, denklemin derecesini belirleyin (birinci, ikinci veya üçüncü) ve eğriyi çizin.\n\nmtcars\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n [ reached 'max' / getOption(\"max.print\") -- omitted 23 rows ]\n\n\n\nplot(mtcars)\n\n\n\n\n\n\n\n\n\nplot(mtcars$disp, mtcars$wt)\n\n\n\n\n\n\n\n\n\nfirst_wd &lt;- lm(wt ~ disp, data=mtcars)\nsummary(first_wd)\n\n\nCall:\nlm(formula = wt ~ disp, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.89044 -0.29775 -0.00684  0.33428  0.66525 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.5998146  0.1729964   9.248 2.74e-10 ***\ndisp        0.0070103  0.0006629  10.576 1.22e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4574 on 30 degrees of freedom\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7815 \nF-statistic: 111.8 on 1 and 30 DF,  p-value: 1.222e-11\n\n\n\nsecond_wd &lt;- lm(wt ~ disp + I(disp^2), data=mtcars)\nsummary(second_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.88654 -0.29136 -0.00961  0.32389  0.67010 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.631e+00  3.622e-01   4.502 0.000101 ***\ndisp        6.694e-03  3.325e-03   2.013 0.053448 .  \nI(disp^2)   6.206e-07  6.380e-06   0.097 0.923178    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4652 on 29 degrees of freedom\nMultiple R-squared:  0.7886,    Adjusted R-squared:  0.774 \nF-statistic: 54.08 on 2 and 29 DF,  p-value: 1.639e-10\n\n\n\nthird_wd &lt;- lm(wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\nsummary(third_wd)\n\n\nCall:\nlm(formula = wt ~ disp + I(disp^2) + I(disp^3), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.63764 -0.22785  0.00307  0.24614  0.58333 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.111e+00  5.651e-01  -1.966   0.0593 .  \ndisp         4.945e-02  8.198e-03   6.032 1.68e-06 ***\nI(disp^2)   -1.807e-04  3.361e-05  -5.378 9.88e-06 ***\nI(disp^3)    2.243e-07  4.119e-08   5.446 8.21e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3299 on 28 degrees of freedom\nMultiple R-squared:  0.8973,    Adjusted R-squared:  0.8863 \nF-statistic: 81.57 on 3 and 28 DF,  p-value: 5.959e-14\n\n\n\nplot(mtcars$disp,mtcars$wt)\ncurve(2.243e-07*x^3 -1.807e-04*x^2 +4.945e-02*x -1.111e+00,add=TRUE)\n\n\n\n\n\n\n\n\n\nbest_model &lt;- lm(wt ~ poly(disp,15), data=mtcars)\nsummary(best_model)\n\n\nCall:\nlm(formula = wt ~ poly(disp, 15), data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.43800 -0.11202 -0.00364  0.16319  0.37783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       3.2172500  0.0509709  63.119  &lt; 2e-16 ***\npoly(disp, 15)1   4.8375552  0.2883350  16.778 1.41e-11 ***\npoly(disp, 15)2   0.0452475  0.2883350   0.157   0.8773    \npoly(disp, 15)3   1.7965723  0.2883350   6.231 1.20e-05 ***\npoly(disp, 15)4   0.4463962  0.2883350   1.548   0.1411    \npoly(disp, 15)5  -0.6866393  0.2883350  -2.381   0.0300 *  \npoly(disp, 15)6  -0.3384161  0.2883350  -1.174   0.2577    \npoly(disp, 15)7  -0.6134397  0.2883350  -2.128   0.0493 *  \npoly(disp, 15)8   0.0528245  0.2883350   0.183   0.8569    \npoly(disp, 15)9   0.0814084  0.2883350   0.282   0.7813    \npoly(disp, 15)10 -0.0654665  0.2883350  -0.227   0.8233    \npoly(disp, 15)11  0.2853853  0.2883350   0.990   0.3370    \npoly(disp, 15)12 -0.0007108  0.2883350  -0.002   0.9981    \npoly(disp, 15)13  0.4495723  0.2883350   1.559   0.1385    \npoly(disp, 15)14 -0.0699728  0.2883350  -0.243   0.8113    \npoly(disp, 15)15 -0.5031480  0.2883350  -1.745   0.1002    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2883 on 16 degrees of freedom\nMultiple R-squared:  0.9552,    Adjusted R-squared:  0.9132 \nF-statistic: 22.73 on 15 and 16 DF,  p-value: 5.551e-08\n\n\n\npred &lt;- predict(best_model, data.frame(disp=50:500))\nplot(mtcars$disp, mtcars$wt)\npoints(50:500, pred, type=\"l\")\n\n\n\n\n\n\n\n\n\npredict(third_wd, data.frame(disp=420))\n\n       1 \n4.397246 \n\n\n\npredict(best_model, data.frame(disp=420))\n\n       1 \n7.329183 \n\n\n\npredict(best_model, data.frame(disp=c(20,420, 500)))\n\n           1            2            3 \n 3865.329736     7.329183 -1513.538298 \n\n\n\nYani, “best_model” model aslında hiç de en iyi değil.."
  },
  {
    "objectID": "r-linear-regression-tr.html#eğitimtest-bölünmesi-traintest-split",
    "href": "r-linear-regression-tr.html#eğitimtest-bölünmesi-traintest-split",
    "title": "R ile Doğrusal Regresyon",
    "section": "Eğitim/Test Bölünmesi (Train/Test split)",
    "text": "Eğitim/Test Bölünmesi (Train/Test split)\nAşırı öğrenme (overfitting)](https://www.investopedia.com/terms/o/overfitting.asp) konusunu anlayalım.\nVeriyi eğitim ve test olarak bölelim. Modeli “eğitim” verisiyle eğitelim. Sonra “test” verisiyle tahmin yapalım.\n\nsample(1:32, 5, replace=FALSE)\n\n[1]  9  6 20 17  5\n\n\n\nset.seed(2)\nidx &lt;- sample(1:32, 5, replace=FALSE)\nidx\n\n[1] 21 15  6 30  8\n\n\n\ntest &lt;- mtcars[idx,]\n\ntest\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona      21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nValiant            18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 240D          24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n\n\n\ntrain &lt;- mtcars[-idx,]\ntrain\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 230          22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280          19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C         17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n [ reached 'max' / getOption(\"max.print\") -- omitted 18 rows ]\n\n\n\nthird_deg &lt;- lm(wt ~ poly(disp,3), data = train)\nsummary(third_deg)$r.squared\n\n[1] 0.8879368\n\n\n\npredict(third_deg, test)\n\n     Toyota Corona Cadillac Fleetwood            Valiant       Ferrari Dino \n          2.633774           5.824101           3.418857           2.970499 \n         Merc 240D \n          2.989188 \n\n\n\ntest$third_pred &lt;- predict(third_deg, test)\ntest\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona      21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nValiant            18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nFerrari Dino       19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 240D          24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n                   third_pred\nToyota Corona        2.633774\nCadillac Fleetwood   5.824101\nValiant              3.418857\nFerrari Dino         2.970499\nMerc 240D            2.989188\n\n\n\nfifteen_deg &lt;- lm(wt ~ poly(disp,15), data=train)\nsummary(fifteen_deg)$r.squared\n\n[1] 0.9550231\n\n\n\npredict(fifteen_deg, test)\n\n     Toyota Corona Cadillac Fleetwood            Valiant       Ferrari Dino \n          2.644378        -844.610091           2.784157           2.761685 \n         Merc 240D \n          2.745391 \n\n\n\ntest$fifteen &lt;- predict(fifteen_deg, test)\n\ntest[c(\"disp\",\"wt\",\"third_pred\",\"fifteen\")]\n\n                    disp    wt third_pred     fifteen\nToyota Corona      120.1 2.465   2.633774    2.644378\nCadillac Fleetwood 472.0 5.250   5.824101 -844.610091\nValiant            225.0 3.460   3.418857    2.784157\nFerrari Dino       145.0 2.770   2.970499    2.761685\nMerc 240D          146.7 3.190   2.989188    2.745391\n\n\nGördüğünüz gibi, “on beşinci derece” model veriyi ezberlemiş (yani aşırı öğrenme/overfittng göstermiş) ve kötü tahminler yapıyor."
  },
  {
    "objectID": "ders5.html#contents",
    "href": "ders5.html#contents",
    "title": "Classification Methods",
    "section": "Contents",
    "text": "Contents\n\n\n\nIntroduction to supervised learning\nDefinition and applications of classification\nPreparing the data\n\nFeature selection and preprocessing ✅\n\nMethods\n\nDecision Trees\nRandom Forest\nSupport Vector Machines (SVM)\nLogistic Regression\nK-nearest neighbor\nNaive Bayes\nArtificial Neural Networks ✅\n\n\n\n\nEnsemble methods\nEvaluation ✅\n\nConfusion Matrix ✅\nAccuracy, precision, recall, F1-score ✅\nROC curves ✅\n\nOverfitting and underfitting ✅\nCross-validation"
  },
  {
    "objectID": "ders5.html#section",
    "href": "ders5.html#section",
    "title": "Classification Methods",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders5.html#section-1",
    "href": "ders5.html#section-1",
    "title": "Classification Methods",
    "section": "",
    "text": "Source"
  },
  {
    "objectID": "ders5.html#definition-of-classification",
    "href": "ders5.html#definition-of-classification",
    "title": "Classification Methods",
    "section": "Definition of classification",
    "text": "Definition of classification\n\nA type of supervised learning\nGoal: Categorize input data into predefined classes or categories\nThe model learns to draw decision boundaries between classes\nOutput is a discrete class label (unlike regression, which predicts continuous values)"
  },
  {
    "objectID": "ders5.html#applications",
    "href": "ders5.html#applications",
    "title": "Classification Methods",
    "section": "Applications",
    "text": "Applications\n\n\n\nText Classification\n\nSpam detection in emails\nSentiment analysis of product reviews\nNews article categorization\n\nImage Classification\n\nMedical imaging for disease detection\nFacial recognition systems\nPlant or animal species identification\n\nFinancial Applications\n\nCredit scoring (approve/deny loan applications)\nFraud detection in transactions\n\n\n\n\nHealthcare\n\nDisease diagnosis based on symptoms and test results\nPredicting patient readmission risk\n\nEnvironmental Science\n\nClimate pattern classification\nSpecies habitat prediction\n\nLiterature and Linguistics\n\nAuthorship attribution\nGenre classification of texts\nLanguage identification"
  },
  {
    "objectID": "ders5.html#your-turn",
    "href": "ders5.html#your-turn",
    "title": "Classification Methods",
    "section": "Your Turn",
    "text": "Your Turn\nIn the zoom chat window please write down your department and an example of classification task related to your domain"
  },
  {
    "objectID": "ders5.html#your-turn-1",
    "href": "ders5.html#your-turn-1",
    "title": "Classification Methods",
    "section": "Your Turn",
    "text": "Your Turn\nPick one example and discuss about the data\nVisit Kaggle and find related dataset"
  },
  {
    "objectID": "ders5.html#decision-trees",
    "href": "ders5.html#decision-trees",
    "title": "Classification Methods",
    "section": "Decision Trees",
    "text": "Decision Trees\nDecision Trees are a classification method that uses a tree-like model of decisions and their possible consequences. The algorithm learns a series of if-then-else decision rules that split the data based on feature values, creating a structure that resembles a flowchart. Each internal node represents a “test” on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label or decision.\nbranch, test, leaf"
  },
  {
    "objectID": "ders5.html#example",
    "href": "ders5.html#example",
    "title": "Classification Methods",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\n\n\n\nHours studied\nPrevious Score\nAttended Review\nPass?\n\n\n\n\n3\n60\nNo\n?\n\n\n4\n75\nNo\n?\n\n\n7\n80\nYes\n?"
  },
  {
    "objectID": "ders5.html#key-metrics-for-decision-tree-construction",
    "href": "ders5.html#key-metrics-for-decision-tree-construction",
    "title": "Classification Methods",
    "section": "Key Metrics for Decision Tree Construction",
    "text": "Key Metrics for Decision Tree Construction\nQuestions: Which feature is the first branch? At what value we create a branch (5 hours, 70 points, etc.)\n\n\nEntropy\nInformation Gain\nGini Impurity"
  },
  {
    "objectID": "ders5.html#entropy",
    "href": "ders5.html#entropy",
    "title": "Classification Methods",
    "section": "Entropy",
    "text": "Entropy\n\nEntropy is a measure of impurity or uncertainty in a set of examples. In the context of decision trees, it quantifies the disorder in the class labels of a dataset.\n\nFormula: \\(H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)\\)\nWhere \\(S\\) is the dataset, \\(c\\) is the number of classes, and \\(p_i\\) is the proportion of examples belonging to class \\(i\\).\n\nRanges from 0 (completely pure, all examples belong to one class) to \\(\\log_2(c)\\) (completely impure, equal distribution across all classes).\nUsed to calculate information gain."
  },
  {
    "objectID": "ders5.html#information-gain",
    "href": "ders5.html#information-gain",
    "title": "Classification Methods",
    "section": "Information Gain",
    "text": "Information Gain\n\nInformation gain measures the reduction in entropy achieved by splitting the data on a particular feature. It helps determine which feature to split on at each node of the decision tree.\n\nFormula: \\(IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)\\)\nWhere \\(S\\) is the dataset, \\(A\\) is the feature being considered for splitting, \\(Values(A)\\) are the possible values of feature \\(A\\), and \\(S_v\\) is the subset of \\(S\\) where feature \\(A\\) has value \\(v\\).\n\nHigher information gain indicates a more useful feature for classification.\nThe feature with the highest information gain is typically chosen for splitting at each node."
  },
  {
    "objectID": "ders5.html#gini-impurity",
    "href": "ders5.html#gini-impurity",
    "title": "Classification Methods",
    "section": "Gini Impurity",
    "text": "Gini Impurity\n\nGini impurity is an alternative to entropy for measuring the impurity of a set of examples. It represents the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the distribution of labels in the subset.\n\nFormula: \\(Gini(S) = 1 - \\sum_{i=1}^{c} (p_i)^2\\)\nWhere \\(S\\) is the dataset, \\(c\\) is the number of classes, and \\(p_i\\) is the proportion of examples belonging to class \\(i\\).\n\nRanges from 0 (completely pure) to \\(1 - \\frac{1}{c}\\) (completely impure).\nOften used in algorithms like CART (Classification and Regression Trees).\n\nThe choice between using entropy (with information gain) or Gini impurity often depends on the specific implementation of the decision tree algorithm. In practice, they often yield similar results."
  },
  {
    "objectID": "ders5.html#algoritms-for-decision-tree-construction",
    "href": "ders5.html#algoritms-for-decision-tree-construction",
    "title": "Classification Methods",
    "section": "Algoritms for decision tree construction",
    "text": "Algoritms for decision tree construction\n\nID3\nCART\n\nPlease visit this link for details about algoritms"
  },
  {
    "objectID": "ders5.html#an-example-with-r",
    "href": "ders5.html#an-example-with-r",
    "title": "Classification Methods",
    "section": "An example with R",
    "text": "An example with R\nhttps://www.dataspoof.info/post/decision-tree-classification-in-r/\nhttps://forum.posit.co/t/decision-tree-in-r/5561/5"
  },
  {
    "objectID": "ders5.html#section-2",
    "href": "ders5.html#section-2",
    "title": "Classification Methods",
    "section": "",
    "text": "Advantages of Decision Trees:\n\nInterpretability: Easy to understand and explain, even for non-experts. The decision-making process can be visually represented.\nNo or little data preprocessing required: Can handle both numerical and categorical data without the need for normalization or scaling.\nComputationally efficient: Generally fast to train and make predictions, especially with small to medium-sized datasets.\n\nDisadvantages:\n\nOverfitting: Prone to overfitting, especially with deep trees, leading to poor generalization on new data.\nInstability: Small changes in the data can result in a completely different tree being generated.\nDifficulty with high-dimensional data: Can become computationally expensive and prone to overfitting with many features."
  },
  {
    "objectID": "ders5.html#quiz-time",
    "href": "ders5.html#quiz-time",
    "title": "Classification Methods",
    "section": "Quiz time",
    "text": "Quiz time"
  },
  {
    "objectID": "ders5.html#random-forest",
    "href": "ders5.html#random-forest",
    "title": "Classification Methods",
    "section": "Random Forest",
    "text": "Random Forest\nRandom forest is a commonly-used machine learning algorithm, trademarked by Leo Breiman and Adele Cutler, that combines the output of multiple decision trees to reach a single result.\nRandom forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees."
  },
  {
    "objectID": "ders5.html#kaggle-example",
    "href": "ders5.html#kaggle-example",
    "title": "Classification Methods",
    "section": "Kaggle example",
    "text": "Kaggle example\nPlease visit: https://www.kaggle.com/code/lara311/diabetes-prediction-using-machine-learning"
  },
  {
    "objectID": "ders5.html#support-vector-machines",
    "href": "ders5.html#support-vector-machines",
    "title": "Classification Methods",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nThe Basic Idea\nImagine you’re trying to separate different types of objects, like apples and oranges, based on their characteristics, such as color, shape, and size. You want to find a way to draw a line (or a hyperplane in higher dimensions) that separates the two types of objects as accurately as possible."
  },
  {
    "objectID": "ders5.html#the-svm-method",
    "href": "ders5.html#the-svm-method",
    "title": "Classification Methods",
    "section": "The SVM Method",
    "text": "The SVM Method\nA Support Vector Machine is a type of supervised learning algorithm that aims to find the best hyperplane that separates the data into different classes. Here’s how it works:\n\nData Preparation: Collect a dataset of objects (e.g., apples and oranges) with their corresponding characteristics (features) and labels (e.g., “apple” or “orange”).\nPlotting the Data: Plot the data points in a feature space, where each axis represents a feature (e.g., color, shape, size).\nFinding the Hyperplane: The goal is to find a hyperplane that separates the data points into different classes. A hyperplane is a line (in 2D) or a plane (in 3D) that divides the feature space into two regions.\nMaximizing the Margin: The SVM algorithm tries to find the hyperplane that maximizes the margin between the two classes. The margin is the distance between the hyperplane and the nearest data points (called support vectors) on either side of the hyperplane.\nSupport Vectors: The support vectors are the data points that lie closest to the hyperplane and have the most influence on its position. They are the “support” that helps define the hyperplane."
  },
  {
    "objectID": "ders5.html#support-vector-machines-1",
    "href": "ders5.html#support-vector-machines-1",
    "title": "Classification Methods",
    "section": "Support Vector Machines",
    "text": "Support Vector Machines\nKey Concepts\n\nHyperplane: A line (in 2D) or a plane (in 3D) that separates the data into different classes.\nMargin: The distance between the hyperplane and the nearest data points (support vectors) on either side of the hyperplane.\nSupport Vectors: The data points that lie closest to the hyperplane and have the most influence on its position."
  },
  {
    "objectID": "ders5.html#why-svms-are-useful",
    "href": "ders5.html#why-svms-are-useful",
    "title": "Classification Methods",
    "section": "Why SVMs are Useful",
    "text": "Why SVMs are Useful\nSVMs are powerful because they:\n\nCan handle high-dimensional data\nAre robust to noise and outliers\nCan be used for both classification and regression tasks\nProvide a clear geometric interpretation of the decision boundary"
  },
  {
    "objectID": "ders5.html#section-3",
    "href": "ders5.html#section-3",
    "title": "Classification Methods",
    "section": "",
    "text": "H1 does not separate the classes. H2 does, but only with a small margin. H3 separates them with the maximal margin. Source"
  },
  {
    "objectID": "ders5.html#section-4",
    "href": "ders5.html#section-4",
    "title": "Classification Methods",
    "section": "",
    "text": "Maximum-margin hyperplane and margins for an SVM trained with samples from two classes. Samples on the margin are called the support vectors. Source"
  },
  {
    "objectID": "ders5.html#section-5",
    "href": "ders5.html#section-5",
    "title": "Classification Methods",
    "section": "",
    "text": "Please visit SVM demo site for an online interactive demo for SVM"
  },
  {
    "objectID": "ders5.html#logistic-regression",
    "href": "ders5.html#logistic-regression",
    "title": "Classification Methods",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe Basic Idea\nLogistic regression is a supervised machine learning algorithm that accomplishes binary classification tasks by predicting the probability of an outcome, event, or observation. The model delivers a binary outcome limited to two possible outcomes: yes/no, 0/1, or true/false."
  },
  {
    "objectID": "ders5.html#the-logistic-regression-method",
    "href": "ders5.html#the-logistic-regression-method",
    "title": "Classification Methods",
    "section": "The Logistic Regression Method",
    "text": "The Logistic Regression Method\nLogistic Regression is a type of supervised learning algorithm that models the probability of an event occurring (e.g., passing an exam) based on a set of input variables (e.g., scores). Here’s how it works:\n\nData Preparation: Collect a dataset of input variables (e.g., scores) and output variables (e.g., pass/fail).\nLogistic Function: The logistic function, also known as the sigmoid function, is used to model the probability of the event occurring. It maps the input variables to a probability between 0 and 1.\nLog-Odds: The logistic function is based on the log-odds of the event occurring, which is the logarithm of the ratio of the probability of the event occurring to the probability of the event not occurring.\nCoefficients: The algorithm learns the coefficients (weights) for each input variable, which determine the importance of each variable in predicting the output.\nDecision Boundary: The algorithm uses the coefficients and the logistic function to create a decision boundary, which separates the input space into two regions: one for each class (e.g., pass and fail).\nPrediction: For a new input, the algorithm calculates the probability of the event occurring using the logistic function and the learned coefficients. If the probability is above a certain threshold (e.g., 0.5), the algorithm predicts the event will occur (e.g., the student will pass)."
  },
  {
    "objectID": "ders5.html#key-concepts",
    "href": "ders5.html#key-concepts",
    "title": "Classification Methods",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nLogistic Function: A mathematical function that maps input variables to a probability between 0 and 1.\nLog-Odds: The logarithm of the ratio of the probability of the event occurring to the probability of the event not occurring.\nCoefficients: The weights learned by the algorithm for each input variable, which determine their importance in predicting the output.\nDecision Boundary: The boundary that separates the input space into two regions, one for each class."
  },
  {
    "objectID": "ders5.html#why-logistic-regression-is-useful",
    "href": "ders5.html#why-logistic-regression-is-useful",
    "title": "Classification Methods",
    "section": "Why Logistic Regression is Useful",
    "text": "Why Logistic Regression is Useful\nLogistic Regression is a popular algorithm because it:\n\nIs easy to implement and interpret\nCan handle multiple input variables\nProvides a probability estimate for each prediction\nIs widely used in many fields, such as medicine, finance, and marketing"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "FEF3001 Yapay Zekaya Giriş Ders Notları",
    "section": "",
    "text": "YTÜ Fen Fakültesinde 2024 Yaz döneminde verilen FEF3001 kodlu dersin içerikleri sayfasıdır.\n\n\n\n\n\nFinal sınavı ne zaman? Cevap: 7 Ağustos Çarşamba 13:00, Derslikler Fen Fakültesi sayfasında duyurulmuştur\nFinal konuları hangileridir? Cevap: Ders5 Sınıflandırma ve sonrası konular\n\n\n\n\n\nVize ne zaman? Cevap: 18 Temmuz 2024 perşembe saat 12:00\nVize yüz yüze mi? Cevap: Evet\nSınavlar hangi derslikte yapılacak? Cevap: Sınav derslikleri Fen Fakültesi sayfasında duyurulmuştur. Lütfen grubunuza (İng veya Türkçe) ve bölümünüze uygun derslikte sınava giriniz.\nVize konuları nelerdir? Cevap: Yapay sinir ağı konusunun sonuna kadar\nYüklenen sunumlar dışında bir kaynaktan soru sorulacak mıdır? Cevap: Temel olarak sunumlar kullanılacaktır fakat derste anlatılanlardan da sorumluyuz, ders video kaydı ve derste aldığınız notlarınızı da çalışınız\nVize test şeklinde mi ve kaç soru sorulur? Cevap: Test şeklinde 20-30 soru planlanmaktadır\nÇok fazla formül var, vizede formüller verilebilir mi? Cevap: Bütün formüller verilmeyecektir, az kullanılan ve karmaşık olan formüller verilebilir\nHaftaya ders yapılacak mı? Cevap: Sınav olduğu gün sadece FEF3001 dersi yapılmayacaktır, Cuma günü ders işlenecektir.\nYorum soruları sorulacak mı? Cevap: Genelde bilgi ölçen sorular terich edilecektir"
  },
  {
    "objectID": "about.html#sıkça-sorulan-sorular",
    "href": "about.html#sıkça-sorulan-sorular",
    "title": "FEF3001 Yapay Zekaya Giriş Ders Notları",
    "section": "",
    "text": "Final sınavı ne zaman? Cevap: 7 Ağustos Çarşamba 13:00, Derslikler Fen Fakültesi sayfasında duyurulmuştur\nFinal konuları hangileridir? Cevap: Ders5 Sınıflandırma ve sonrası konular\n\n\n\n\n\nVize ne zaman? Cevap: 18 Temmuz 2024 perşembe saat 12:00\nVize yüz yüze mi? Cevap: Evet\nSınavlar hangi derslikte yapılacak? Cevap: Sınav derslikleri Fen Fakültesi sayfasında duyurulmuştur. Lütfen grubunuza (İng veya Türkçe) ve bölümünüze uygun derslikte sınava giriniz.\nVize konuları nelerdir? Cevap: Yapay sinir ağı konusunun sonuna kadar\nYüklenen sunumlar dışında bir kaynaktan soru sorulacak mıdır? Cevap: Temel olarak sunumlar kullanılacaktır fakat derste anlatılanlardan da sorumluyuz, ders video kaydı ve derste aldığınız notlarınızı da çalışınız\nVize test şeklinde mi ve kaç soru sorulur? Cevap: Test şeklinde 20-30 soru planlanmaktadır\nÇok fazla formül var, vizede formüller verilebilir mi? Cevap: Bütün formüller verilmeyecektir, az kullanılan ve karmaşık olan formüller verilebilir\nHaftaya ders yapılacak mı? Cevap: Sınav olduğu gün sadece FEF3001 dersi yapılmayacaktır, Cuma günü ders işlenecektir.\nYorum soruları sorulacak mı? Cevap: Genelde bilgi ölçen sorular terich edilecektir"
  },
  {
    "objectID": "ders6.html#contents",
    "href": "ders6.html#contents",
    "title": "Regression Methods",
    "section": "Contents",
    "text": "Contents\n\nDefinition of regression\nTypes of regression (linear, polynomial, and ensemble methods)\n\nLinear Regression: Simple Linear Regression, Ordinary Least Squares method, Evaluating model performance (R-squared, RMSE)\nPolynomial Regression: When to use polynomial regression, overfitting\nRegression with Random Forest: Random Forest algorithm for regression, Advantages (handling non-linearity, feature importance)\nOther types of regression: Logistic regression (for binary outcomes), Ridge and Lasso regression (regularization techniques)\n\nExamples on Excel\nCode examples in R and Python"
  },
  {
    "objectID": "ders6.html#regression",
    "href": "ders6.html#regression",
    "title": "Regression Methods",
    "section": "Regression",
    "text": "Regression\nRegression is a statistical method used in data analysis and machine learning to model and analyze the relationship between a dependent variable (often called the target or outcome variable) and one or more independent variables (also known as predictors or features). The primary goal of regression is to estimate how changes in the independent variables are associated with changes in the dependent variable, allowing for prediction and inference.\nKey points about regression:\n\nIt helps identify and quantify relationships between variables.\nIt can be used for both prediction and understanding the impact of variables.\nThere are various types of regression, including linear, polynomial, and more complex methods like Random Forest or Neural networks for regression.\nRegression models can handle both continuous and categorical variables.\nIt’s widely used in many fields, including science, economics, and social sciences."
  },
  {
    "objectID": "ders6.html#linear-regression",
    "href": "ders6.html#linear-regression",
    "title": "Regression Methods",
    "section": "Linear Regression",
    "text": "Linear Regression"
  },
  {
    "objectID": "ders6.html#linear-regression-1",
    "href": "ders6.html#linear-regression-1",
    "title": "Regression Methods",
    "section": "Linear Regression",
    "text": "Linear Regression"
  },
  {
    "objectID": "ders6.html#linear-regression-2",
    "href": "ders6.html#linear-regression-2",
    "title": "Regression Methods",
    "section": "Linear Regression",
    "text": "Linear Regression\ntry finding the best line: https://www.geogebra.org/m/xC6zq7Zv"
  },
  {
    "objectID": "ders6.html#ols-method",
    "href": "ders6.html#ols-method",
    "title": "Regression Methods",
    "section": "OLS method",
    "text": "OLS method\nOrdinary Least Squares (OLS) is a statistical method used to estimate the parameters of a linear regression model. It’s the most common technique for fitting a line to a set of data points in linear regression.\nOLS is a method that minimizes the sum of the squared differences between the observed dependent variable values and the predicted values by the linear function of the independent variable(s). In other words, it finds the line (or hyperplane in multiple dimensions) that best fits the data by minimizing the sum of the squared residuals.\nPlease visit OLS Method at Wikipedia for technical details."
  },
  {
    "objectID": "ders6.html#multivariate-linear-regression",
    "href": "ders6.html#multivariate-linear-regression",
    "title": "Regression Methods",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nMultivariate Linear Regression, also known as Multiple Linear Regression, is a statistical method used to model the relationship between multiple independent variables and a single dependent variable. It extends the concept of simple linear regression to include more than one predictor variable.\nThe general form of the model is:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\varepsilon\n\\]\nWhere:\ny is the dependent variable, x₁, x₂, …, xₙ are the independent variables, β₀ is the y-intercept (the value of y when all x’s are zero), β₁, β₂, …, βₙ are the coefficients associated with each independent variable, n is the number of independent variables, ε is the error term"
  },
  {
    "objectID": "ders6.html#polynomial-regression",
    "href": "ders6.html#polynomial-regression",
    "title": "Regression Methods",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\nPolynomial Regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial. It’s used when the relationship between variables is not linear but can be approximated by a polynomial function.\n\\[\ny = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\ldots + \\beta_nx^n + \\varepsilon\n\\]\nWhere:\ny is the dependent variable, x is the independent variable, β₀, β₁, β₂, …, βₙ are the coefficients to be estimated, n is the degree of the polynomial and ε is the error term"
  },
  {
    "objectID": "ders6.html#polynomial-regression-1",
    "href": "ders6.html#polynomial-regression-1",
    "title": "Regression Methods",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression"
  },
  {
    "objectID": "ders6.html#linear-regression-in-excel",
    "href": "ders6.html#linear-regression-in-excel",
    "title": "Regression Methods",
    "section": "Linear Regression in Excel",
    "text": "Linear Regression in Excel\nUse MTCARS dataset Excel file and;\n\ndraw Trendline between wt (weight) and mpg (miles per galon) columns.\ndraw Trendline between disp (displacement) and wt (weight) columns.\n\nPlease visit MyExcelOnline in order see how to draw a Trendline in Excel"
  },
  {
    "objectID": "ders6.html#linear-regression-in-r",
    "href": "ders6.html#linear-regression-in-r",
    "title": "Regression Methods",
    "section": "Linear Regression in R",
    "text": "Linear Regression in R\nPlease refer to Linear Regression with R page for simple linear and polynomial regression."
  },
  {
    "objectID": "ders6.html#regression-with-random-forest",
    "href": "ders6.html#regression-with-random-forest",
    "title": "Regression Methods",
    "section": "Regression with Random Forest",
    "text": "Regression with Random Forest\nPlease refer to random forest regression with R document."
  },
  {
    "objectID": "Hierarchical_Clustering_in_Data_Mining.html",
    "href": "Hierarchical_Clustering_in_Data_Mining.html",
    "title": "Hierarchical Clustering in Data Mining",
    "section": "",
    "text": "Contents are taken from the Article “Hierarchical Clustering in Data Mining”\nA *Hierarchical clustering* method works via grouping data into a tree of clusters. Hierarchical clustering begins by treating every data point as a separate cluster. Then, it repeatedly executes the subsequent steps:\nIn Hierarchical Clustering, the aim is to produce a hierarchical series of nested clusters. A diagram called *Dendrogram* (A Dendrogram is a tree-like diagram that statistics the sequences of merges or splits) graphically represents this hierarchy and is an inverted tree that describes the order in which factors are merged (bottom-up view) or clusters are broken up (top-down view)."
  },
  {
    "objectID": "Hierarchical_Clustering_in_Data_Mining.html#what-is-hierarchical-clustering",
    "href": "Hierarchical_Clustering_in_Data_Mining.html#what-is-hierarchical-clustering",
    "title": "Hierarchical Clustering in Data Mining",
    "section": "What is Hierarchical Clustering?",
    "text": "What is Hierarchical Clustering?\nHierarchical clustering is a method of cluster analysis in data mining that creates a hierarchical representation of the clusters in a dataset. The method starts by treating each data point as a separate cluster and then iteratively combines the closest clusters until a stopping criterion is reached. The result of hierarchical clustering is a tree-like structure, called a dendrogram, which illustrates the hierarchical relationships among the clusters.\n\n*Hierarchical clustering has several advantages over other clustering methods*\n\nThe ability to handle non-convex clusters and clusters of different sizes and densities.\nThe ability to handle missing data and noisy data.\nThe ability to reveal the hierarchical structure of the data, which can be useful for understanding the relationships among the clusters.\n\n\n\n*Drawbacks of Hierarchical Clustering*\n\nThe need for a criterion to stop the clustering process and determine the final number of clusters.\nThe computational cost and memory requirements of the method can be high, especially for large datasets.\nThe results can be sensitive to the initial conditions, linkage criterion, and distance metric used. In summary, Hierarchical clustering is a method of data mining that groups similar data points into clusters by creating a hierarchical structure of the clusters.\nThis method can handle different types of data and reveal the relationships among the clusters. However, it can have high computational cost and results can be sensitive to some conditions.\n\n\n\nTypes of Hierarchical Clustering\nBasically, there are two types of hierarchical Clustering:\n\nAgglomerative Clustering\nDivisive clustering\n\n\n\n1. Agglomerative Clustering\nInitially consider every data point as an *individual* Cluster and at every step, merge the nearest pairs of the cluster. (It is a bottom-up method). At first, every dataset is considered an individual entity or cluster. At every iteration, the clusters merge with different clusters until one cluster is formed.\n\nThe algorithm for Agglomerative Hierarchical Clustering is:\n\nCalculate the similarity of one cluster with all the other clusters (calculate proximity matrix)\nConsider every data point as an individual cluster\nMerge the clusters which are highly similar or close to each other.\nRecalculate the proximity matrix for each cluster\nRepeat Steps 3 and 4 until only a single cluster remains.\n\nLet’s see the graphical representation of this algorithm using a dendrogram.\n\n*Note:* This is just a demonstration of how the actual algorithm works no calculation has been performed below all the proximity among the clusters is assumed.\n\nLet’s say we have six data points *A, B, C, D, E, and F*.\n\n\n\nAgglomerative Hierarchical clustering\n\n\nAgglomerative Hierarchical clustering\n\n*Step-1:* Consider each alphabet as a single cluster and calculate the distance of one cluster from all the other clusters.\n*Step-2:* In the second step comparable clusters are merged together to form a single cluster. Let’s say cluster (B) and cluster (C) are very similar to each other therefore we merge them in the second step similarly to cluster (D) and (E) and at last, we get the clusters [(A), (BC), (DE), (F)]\n*Step-3:* We recalculate the proximity according to the algorithm and merge the two nearest clusters([(DE), (F)]) together to form new clusters as [(A), (BC), (DEF)]\n*Step-4:* Repeating the same process; The clusters DEF and BC are comparable and merged together to form a new cluster. We’re now left with clusters [(A), (BCDEF)].\n*Step-5:* At last, the two remaining clusters are merged together to form a single cluster [(ABCDEF)].\n\n\n\n\n2. Divisive Hierarchical clustering\nWe can say that Divisive Hierarchical clustering is precisely the *opposite* of Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we take into account all of the data points as a single cluster and in every iteration, we separate the data points from the clusters which aren’t comparable. In the end, we are left with N clusters.\n\n\n\nDivisive Hierarchical clustering\n\n\nDivisive Hierarchical clustering"
  }
]